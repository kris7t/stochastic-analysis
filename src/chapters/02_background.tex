\chapter{Background}
\label{chap:background}

\section{Petri nets}

Petri nets are a widely used graphical and mathematical modeling tool
for systems which are concurrent, asynchronous, distributed, parallel
or nondeterministic.

\begin{dfn}
  A \emph{Petri net} is a 5-tuple $\PN = (P, T, F, W, M_0)$, where
  \begin{asparaitem}
  \item $P = \{p_0, p_1, \ldots, p_{n - 1}\}$ is a finite set of places;
  \item $T = \{t_0, t_1, \ldots, t_{m - 1}\}$ is a finite set of transitions;
  \item $F \subseteq (P \times T) \cup (P \times T)$ is a set of
    arcs, also called the flow relation;
  \item $W: F \to \NNpos$ is an arc weight function;
  \item $M_0: P \to \NN$ is the initial marking;
  \item $P \cap T = \emptyset$ and $P \cup T \neq
    \emptyset$~\citep{murata1989petri}.
  \end{asparaitem}
\end{dfn}

Arcs from $P$ to $T$ are called \emph{input arcs}. The input places of
a transition $t$ are denoted by $\inarc{t} = \{ p : (p, t) \in F
\}$.
In contrast, arcs of the form $(t, p)$ are called \emph{output arcs}
and the output places of $t$ are denoted by
$\outarc{t} = \{p : (t, p) \in F \}$.

A \emph{marking} $M: P \to \NN$ assigns a number of \emph{tokens} to each
place. The transition $t$ is \emph{enabled} in the marking $M_1$
\paren{written as $M_1\,\tranto{t}\,$} when $M(p) \ge W(p, t)$ for all $p \in
\inarc{t}$.

Petri nets are graphically represented as edge weighted directed
bipartite graphs. Places are drawn as circles, while transitions are
drawn as rules or rectangles. Edge weights of $1$ are ususally omitted
from presentation. Dots on places correspond to tokens in the current
marking.

If $M_1\,\tranto{t}$ the transition $t$ can be \emph{fired} to get a
new marking $M_2$ \paren{written as $M_1 \tranto{t} M_2$} by
decreasing the token counts for each place $p \in \inarc{t}$ by
$W(p, t)$ and increasing the token counts for each place
$p \in \outarc{t}$ by $W(t, p)$. Note that in general, $\inarc{t}$ and
$\outarc{t}$ need not be disjoint. Thus, the firing rule can be
written as
\begin{equation}
  \label{eq:background:petri:fire}
  M_2(p) = M_1(p) - W(p, t) + W(t, p) \text,
\end{equation}
where we take $W(x, y) = 0$ if $(x, y) \notin F$ for brevity.

A marking $M'$ is \emph{reachable} from the marking $M$ (written as
$M \reachto M'$) if there exists a sequence of markings and transitions
for some finite $k$ such that
\begin{equation}
  M_1 \tranto{t_{i_1}} M_2 \tranto{t_{i_2}} M_3 \tranto{t_{i_3}}
  \cdots \tranto{t_{i_{k - 1}}} M_{k - 1} \tranto{t_{i_k}} M_k \text,
\end{equation}
where $M_1 = M$ and $M_k = M'$. A marking $M$ is in the reachable
\emph{state space} of the net if $M_0 \reachto M$. The set of all
markings reachable from $M_0$ is denoted by
\begin{equation}
  \RS = \{ M : M_0 \reachto M \} \text.
\end{equation}

\begin{dfn}
  The Petri net $\PN$ is $k$-\emph{bounded} if $M(p) \le k$ for all $M
  \in \RS$ and $p \in P$. $\PN$ is \emph{bounded} if it is $k$-bounded
  for some (finite) $k$.
\end{dfn}

The reachable state space $\RS$ is finite precisely when the Peti net
is bounded.

\begin{figure}
  \centering
  \newcommand*{\examplenet}[3]{
    \begin{tikzpicture}
      \matrix [column sep=1cm,ampersand replacement=\&] {
        \node [petri net place] (h2) {#1}; \& \& \\
        \& \node [petri net transition] (t) {} ; \& \node [petri net place] (h2o) {#3}; \\
        \node [petri net place] (o2) {#2}; \& \& \\
      };
      \draw [-{Latex}] (h2) edge node [above] {$2$} (t) (o2) edge (t)
      (t) edge node [above] {$2$} (h2o);
      \node [below=0cm of h2] {$p_{\text{H$_2$}}$};
      \node [below=0cm of o2] {$p_{\text{O$_2$}}$};
      \node [below=0cm of h2o] {$p_{\text{H$_2$O}}$};
      \node [below=0cm of t] {$t$};
    \end{tikzpicture}}

  $\vcenter{\hbox{\examplenet{\token\token}{\token\token}{}}}
  \quad\vcenter{\hbox{$\tranto{t}$}}\quad
  \vcenter{\hbox{\examplenet{}{\token}{\token\token}}}$
  \caption{A Petri net model of the reaction of hydrogen and oxygen.}
  \label{fig:background:petri:h2o}
\end{figure}

\begin{example}
  The Petri net in~\cref{fig:background:petri:h2o} models the chemical
  reaction
  \begin{equation}
    2 \, \mathrm{H}_2 + \mathrm{O}_2 \rightarrow 2 \,
    \mathrm{H}_2\mathrm{O} \text.
  \end{equation}
  In the initial marking \paren{left} there are two hydrogen and two
  oxygen molecules, represented by token on the places $p_{\text{H$_{2}$}}$
  and $p_{\text{O$_{2}$}}$, therefore the transition $t$ is
  enabled. Firing $t$ yields the marking on the right where the two
  tokens on $p_{\text{H$_{2}$O}}$ are the reaction products. Now $t$ is no
  longer enabled.
\end{example}

\begin{figure}
  \centering
  \begin{tikzpicture}
    \runningExamplePetriNet
  \end{tikzpicture}
  \caption{The \emph{SharedResource} Petri net model.}
  \label{fig:background:petri:sharedresource}
\end{figure}

\begin{runningExample}
  In~\cref{fig:background:petri:sharedresource} we introduce the
  \emph{SharedResource} model which will serve as a running example
  throughout this report.

  The model consists of a single shared resource $S$ and two
  consumers. Each consumer can be in one of the
  $C_i$ \paren{calculating locally}, $W_i$ \paren{waiting for
    resource} and $S_i$ \paren{shared working} states. The transitions
  $r_i$ \paren{request resource}, $a_i$ \paren{acquire resource} and
  $d_i$ \paren{done} correspond to behaviors of the consumers. The net
  is \mbox{$1$-bounded}, therefore it has finite $\RS$.

  The Petri net model allows the verification of safety properties,
  e.g.~we can show that there is mutual exclusion
  --~$M(S_1) + M(S_2) \le 1$ for all reachable markings~-- or that
  deadlocks cannot occur. In contrast, we cannot compute dependability
  or performability measures \paren{e.g.~the utilization of the shared
    resource or number of calculations completed per unit time}
  because the model does not describe the temporal behavior of the
  system.
\end{runningExample}

\subsection{Petri nets extended with inhibitor arcs}

One of the most frequently used extensions of Petri nets is the
addition of inhibitor arcs, which modifies the rule for transition
enablement. This modification gives Petri nets expressive power
equivalent to Turing
machines~\citep{DBLP:conf/apn/Chrzastowski-Wachtel99}.

\begin{dfn}
  A \emph{Petri net with inhibitor arcs} is a 3-tuple $\PNI = (\PN,
  I, W_I)$, where
  \begin{asparaitem}
  \item $\PN = (P, T, F, W, M_0)$ is a Petri net;
  \item $I \subseteq P \times T$ is the set of inhibitor arcs;
  \item $W_I: I \to \NNpos$ is the inhibitor arc weight function.
  \end{asparaitem}
\end{dfn}

Let $\inharc{t} = \{ p : (p, t) \in I\}$ denote the set of inhibitor
places of the transition $t$. The enablement rule for Petri nets with
inhibitor arcs can be formalized as
\begin{equation}
  \label{eq:background:petri:inh-fire}
  M\,\tranto{t} \Longleftrightarrow \text{\strut$M(p) \ge W(p, t)$ for all
    $p \in \inarc{t}$ and $M(p) < W_I(p, t)$ for all $p \in
    \inharc{t}$.}
\end{equation}
The firing rule \eqref{eq:background:petri:fire} remains unchanged.

\section{Continous-time Markov chains}

Continous-time Markov chains are mathematical tools for describing the
behavior of systems in countinous time where the random behavior of
the system only depends on its current state.

\begin{dfn}
  A \emph{Continous-time Markov Chain} \paren{\CTMC}
  $X(t) \in S, t \ge 0$ over a finite or countable infinite state
  space $S = \{0, 1, \ldots, n - 1\}$ is a continous-time random
  process with the \emph{Markovian} or memoryless property
  \begin{multline}\allowdisplaybreaks[0]
    \Pr(X(t_k) = x_k \mid X(t_{k - 1}) = x_{k - 1}, X(t_{k -
      2}) = x_{k - 2}, \ldots, X(t_{0}) = x_{0}) \\
    = \Pr(X(t_k) = x_k \mid X(t_{k - 1}) = x_{k - 1}) \text,
  \end{multline}
  where $t_0 \le t_1 \le \cdots \le t_k$. A \CTMC\ is said to be
  \emph{time-homogenous} if it also satisfies
  \begin{equation}
    \Pr(X(t_k) = x_k \mid X(t_{k - 1}) = x_{k - 1}) = \Pr(X(t_k - t_{k -
      1}) = x_k \mid X(0) = x_{k - 1}) \text.
  \end{equation}
\end{dfn}

In this report we will restrict our attention to time-homogenous \CTMC
s over finite state spaces. The state probabilities of these
stochastic processes at time $t$ form a finite-dimensional vector
$\vec{\uppi}(t) \in \RR$,
\begin{equation}
  \pi(t)[x] = \Pr(X(t) = x)
\end{equation}
that satisfies the differential equation
\begin{equation}
  \label{eq:background:ctmc:diffeq}
  \frac{\dd \vec{\uppi}(t)}{\dd t} = \vec{\uppi}(t) \, Q
\end{equation}
for some square matrix $Q$. The matrix $Q$ is called the
\emph{infinitesimal generator matrix} of the \CTMC\ and can be
interpreted as follows:
\begin{itemize}
\item The diagonal elements $q[x, x] < 0$ describe the holding times
  of the \CTMC. If $X(t) = x$, the \emph{holding time}
  $h_x = \inf \{ h > 0 : X(t + h) \ne x \}$ spent in state $x$ is
  exponentially distributed with rate $\lambda_x = -q[x, x]$. If $q[x,
  x] = 0$, the no transitions are possible form state $x$ and it is
  said to be \emph{absorbing}.
\item The off-diagonal elements $q[x, y] \ge 0$ describe the state
  transitions. In state~$x$ the \CTMC\ will jump to state~$y$ at the
  next state transition with probability $-q[x, y] / q[x, x]$.
  Equivalently, there is expontentially distributed countdown in the
  state $x$ for each $y : q[x, y] > 0$ with \emph{transition rate}
  $\lambda_{xy} = q[x, y]$. The first countdown to finish will trigger
  a state change to the corresponding state $y$. Thus, the \CTMC\ is a
  Kripke structure with exponentially distributed timed transitions.
\item Elements in each row of $Q$ sum to $0$, hence it satisfies
  $Q \vec{1}^\T = \vec{0}^\T$.
\end{itemize}

For more algebraic properties of infinitesimal generator matrices, we
refer to \citet{plemmons1979nonnegative,stewart1994introduction}.

A state $y$ is said to be \emph{reachable} from the state $x$
($x \reachto y$) if there exists a sequence of states
\begin{equation}
  x = z_1, z_2, z_3, \ldots, z_{k - 1}, z_k = y
\end{equation}
such that $q[z_i, z_{i + 1}] > 0$ for all $i = 1, 2, \ldots, k -
1$.
If $y$ is reachable from $x$ for all $x, y \in S$ $y$, the Markov chain
is said to be \emph{irreducible}. Equivalenty, $Q$ is the
infinitesimal generator matrix of an irreducible \CTMC\ if there is no
permutation matrix $M$ such that
\begin{equation}
  M^{-1} Q M =
  \begin{pmatrix}
    Q_1 & 0 \\
    0 & Q_2
  \end{pmatrix}
\end{equation}
for some square matrices $Q_1$, $Q_2$.

The \emph{steady-state probability distribution}
$\vec{\uppi} = \lim_{t \to \infty} \vec{\uppi}(t)$ exists and is
independent from the \emph{initial distribution}
$\vec{\uppi}(0) = \vec{\uppi}_0$ if and only if the finite \CTMC\ is
irreducible. The steady-state distribution is a stationary solution
of~\cref{eq:background:ctmc:diffeq}, therefore it satisfies the linear
equation
\begin{equation}
  \label{eq:background:ctmc:steadystate}
  \frac{\dd \vec{\uppi}}{\dd t} = \vec{\uppi} \, Q = \vec{0}
  \text.
\end{equation}

\begin{figure}
  \begin{minipage}{.49\linewidth}
    \centering
    \begin{tikzpicture}
      \matrix [column sep=1.5cm, every node/.style={inner
        sep=0pt,minimum size=.85cm, draw,circle,tdk highlight}] {
        \node (s0) {0}; & \node (s1) {1}; & \node (s2) {2}; \\
      }; \draw [every edge/.append style={-{Latex},bend left}, every
      node/.append style={above}] (s0) edge node {$\lambda_1$} (s1)
      (s1) edge node {$\lambda_2$} (s2) (s2) edge node {$\mu_2$} (s1)
      (s1) edge node {$\mu_1$} (s0) (s2) edge [bend left=45] node
      {$\mu_3$} (s0);
    \end{tikzpicture}
  \end{minipage}
  \begin{minipage}{.49\linewidth}
    \centering
    $\begin{blockarray}{rccc}
      & 0 & 1 & 2 \\
      \begin{block}{r(ccc)}
        0 & -\lambda_1 & \lambda_1 & 0 \\
        Q = 1 & \mu_1 & -\lambda_2 - \mu_1 & \lambda_2 \\
        2 & \mu_3 & \mu_2 & -\mu_2 - \mu_3 \\
      \end{block}
    \end{blockarray}$
    \vspace{0.5cm}
  \end{minipage}
  \caption{Example \CTMC\ with 3 states and its generator matrix.}
  \label{fig:background:ctmc:repair}
\end{figure}

\begin{example}
  \Cref{fig:background:ctmc:repair} shows a \CTMC\ with $3$
  states. The transitions from state $0$ to $1$ and from $1$ to $2$
  are associated with exponentially distributed countdowns with rates
  $\lambda_1$ and $\lambda_2$ respectively, while transitions in the
  reverse direction have rates $\mu_1$ and $\mu_2$. The transition
  form state $2$ to $0$ is also possible with rate $\mu_3$.
  
  The rows \paren{corresponding to source states} and
  columns \paren{destination states} of the infinitesimal generator
  matrix $Q$ are labeled with the state numbers. The diagonal element
  $q[1, 1]$ is $-\lambda_2 - \mu_1$, hence the holding time in state
  $1$ is exponentially distributed with rate $\lambda_2 + \mu_1$. The
  transition to $0$ is taken with probability
  $-q[1, 0] / q[1, 1] = \mu_1 / (\lambda_2 + \mu_1)$, while the
  transition to $2$ is taken with probability
  $\lambda_2 / (\lambda_2 + \mu_1)$.

  The \CTMC\ is irreducible, because every state is reachable from
  every other state. Therefore, there is a unique steady-state
  distribution $\vec{\uppi}$ independent from the initial distribution
  $\vec{\uppi}_0$.
\end{example}

\subsection{Markov reward models}

Continous-time Markov chains may be employed in the estimation of
performance measures of models by defining \emph{rewards} that
associate \emph{reward rates} with the states of a \CTMC. The
momentary reward rate random variable $R(t)$ can describe performance
measures defined at a single point of time, such as resource
utilization or probability of failure, while the \emph{accumulated
  reward} random variable $Y(t)$ may correspond to performance
measures associated intervals of time, such as total downtime.

\begin{dfn}
  A \emph{Continous-time Markov Reward Process} over a finite state
  space $S = \{0, 2, \ldots, n - 1\}$ is a pair $(X(t), \vec{r})$,
  where $X(t)$ is a \CTMC\ over $S$ and $\vec{r} \in \RR^n$ is a
  \emph{reward rate vector}.
\end{dfn}

The element $r[x]$ of reward vector is a momentary reward rates in
state $x$, therefore the reward rate random variable can be written as
$R(t) = r[X(t)]$. Accumulated rewards reward until time $t$ is
calculated by integration as
\begin{equation}
  Y(t) = \int_{0}^t R(\tau) \, \dd \tau \text.
\end{equation}

\textbf{TODO Valamit írni és hivatkozni arról, hogy az $R(t)$ és
  $Y(t)$ eloszlásait nehéz meghatározni, mi csak a várható értékekkel
  foglalkozunk + behivatkozni valamit a várható értékhez.}

Given the initial probability distribution vector
$\vec{\uppi}(0) = \vec{\uppi}_0$ the expected value of the reward rate
at time $t$ can be calculated as
\begin{equation}
  \label{eq:background:ctmc:reward}
  \Ex R(t) = \sum_{i = 0}^{n - 1} \pi(t)[i] r[i] = \vec{\uppi}(t)
  \, \vec{r}^\T \text,
\end{equation}
which requires the solution of the initial value problem
\begin{equation}
  \frac{\dd \vec{\uppi}(t)}{\dd t} = \vec{\uppi}(t) \, Q, \quad
  \vec{\uppi}(0) = \vec{\uppi}_0
\end{equation}
to from the inner product $ \Ex[R(t)] = \vec{\uppi}(t) \,
\vec{r}^\T$.
To obtain the expected steady-state reward rate (if it exists) the
linear equation~\eqref{eq:background:ctmc:steadystate} should be solved
instead for the steady-state probability vector $\vec{\uppi}$.

The expected value of the accumulated reward is
\begin{align}
  \Ex Y(t) &= \Ex\mleft[ \int_0^t R(\tau) \,\dd\tau \mright] =
             \int_0^t \Ex[R(\tau)] \,\dd\tau \\
           &= \int_0^t \sum_{i = 0}^{n - 1} \pi(\tau)[i] r[i]
             \,\dd\tau = \sum_{i = 0}^{n - 1} \int_0^t \pi(\tau)[i]
             \,\dd\tau \, r[i] \\
           &= \int_0^t \vec{\uppi}(t) \,\dd\tau \, \vec{r}^\T =
             \vec{L}(t) \, \vec{r}^\T \text,
\end{align}
where $\vec{L}(t) = \int_0^t \vec{\uppi}(t) \,\dd\tau$ is the accumulated
probability vector, which is the solution of the initial value problem
\begin{equation}
  \frac{\dd \vec{L}(t)}{\dd t} = \vec{\uppi}(t), \quad \frac{\dd
    \vec{\uppi}(t)}{\dd t} = \vec{\uppi}(t) \, Q, \quad
  \vec{L}(0) = \vec{0}, \quad \vec{\uppi}(0) = \vec{\uppi}_0.
\end{equation}

\begin{example}
  Let $c_0$, $c_1$ and $c_2$ denote operating costs per unit time
  associated with the states of the \CTMC\ in
  \cref{fig:background:ctmc:repair}. Consider the Markov reward
  process $(X(t), \vec{r})$ with reward rate vector
  \begin{equation}
    \vec{r} = \begin{pmatrix} c_0 & c_1 & c_2 \end{pmatrix} \text.
  \end{equation}
  The random variable $R(t)$ describes the momentary operating cost,
  while $Y(t)$ is the total operating expenditure until time $t$. The
  steady-state expectation of $R$ is the average maintenance cost per
  unit time of the long-running system.
\end{example}

\subsection{Sensitivty}

Consider a reward process $(X(t), \vec{r})$ where both the
infinitesimal generator matrix $Q(\vec{\uptheta})$ and the reward rate
vector $\vec{r}(\vec{\uptheta})$ may depend on some \emph{parameters}
$\vec{\uptheta} \in \RR^m$. The \emph{sensitivity} analysis of the
rewards $R(t)$ may reveal performance or reliability bottlenecks of
the modeled system and aid designers in achieving desired performance
measures.

\begin{dfn}
  The \emph{sensitivity} of the expected reward rate~$\Ex R(t)$ to the
  parameter~$\theta[i]$ is the partial derivative
  \begin{equation}
    \frac{\partial \Ex R(t)}{\partial \theta[i]} \text.
  \end{equation}
\end{dfn}

The model reacts to the change of parameters with high absolute
sensitivity more prominently, therefore they can be promising avenues
of system optimization.

To calculate the sensivity of $\Ex R(t)$, the partial derivative of
both sides of \cref{eq:background:ctmc:reward} is taken, yielding
\begin{equation}
  \frac{\partial \Ex R(t)}{\partial \theta[i]} = \frac{\partial
    \vec{\uppi}(t)}{\partial \theta[i]} \vec{r}^\T + \vec{\uppi}(t)
  \mleft(\frac{\partial \vec{r}}{\partial \theta[i]}\mright)^\T =
  \vec{s}_i(t) \, \vec{r}^\T + \vec{\uppi}(t)
  \mleft(\frac{\partial \vec{r}}{\partial \theta[i]}\mright)^\T \text,
\end{equation}
where $\vec{s}_i$ is the sensitivity of $\vec{\uppi}$ to the parameter
$\theta[i]$.

In transient analysis, the sensitivity vector $\vec{s}_i$ is the
solution of the initial value problem
\begin{equation}
  \frac{\dd \vec{s}_i(t)}{\dd t} = \vec{s}_i(t) Q + \vec{\pi}(t) V_i,
  \quad \frac{\dd \vec{\uppi}(t)}{\dd t} = \vec{\uppi}_i(t) Q,
  \quad \vec{s}_i(0) = \vec{0}, \quad \vec{\uppi}(0) = \vec{\uppi}_0
  \text,
\end{equation}
where $V_i = \partial Q(\vec{\uptheta}) / \partial \theta[i]$ is the
partial derivative of the generator matrix. A similar initial value
problem can be derived for the sensitivity of
$\vec{L}(t)$~\citep{DBLP:conf/sigmetrics/BlakeRT88}.

To obtain the The sensitivity $\vec{s}_i$ of the steady-state
probability vector $\vec{\uppi}$, the system of linear equations
\begin{equation}
  \vec{s}_i Q = -\vec{\uppi} V_i, \quad \vec{s}_i \vec{1}^T = 0
\end{equation}
is solved.

\textbf{TODO Sensitivity masik definicioja, amikor a numerikus
  stabilitast vizsgaljuk\ldots}

\subsection{Time to first failure}

\section{Stochastic Petri nets}

While reward processes based continous-time Markov chains allow the
study of dependability or reliability measurements, the explicit
specification of stochastic processes and rewards is often
cumbersome. More expressive formalisms include queing networks,
stochastic process algebras, stochastic Automata
Networks~(\textls{SAN}) and Stochastic Petri Nets
(\textls{SPN}). \textbf{TODO Kanonikus hivatkozasokat keresni}

Stochastic Petri Nets extend Petri nets by assigning random
exponentially distributed random delays to
transitions~\citep{DBLP:conf/apn/Marsan88}. After the delay associated
with an enabled transition is elapsed the transition fires
\emph{atomically} are transitions delays are reset.

\begin{dfn}
  A Stochastic Petri Net is a pair $\SPN = (\PN, \Lambda)$, where
  $\PN$ is a Petri net $(P, T, F, W, M_0)$ and $\Lambda: P \to \RRpos$
  is a transition rate function.
\end{dfn}

\needspace{5ex}

A finite \CTMC\ can be associated with a bounded stochastic Petri net
as follows:
\begin{enumerate}
\item The reachable state space of the Petri net is explored. We
  associate a consecutive natural numbers with the states such that
  the state space is
  \begin{equation}
    \RS = \{ M_0, M_1, M_2, \ldots, M_{n - 1} \} \text,
  \end{equation}
  where $M_0$ is the initial marking. From now on, we will use
  markings $M_x \in \RS$ and natural numbers $x \in \{0, 1, \ldots, n
  - 1\}$ to refer to states of the model interchangably.
\item We define a \CTMC\ $X(t)$ over the finite state space
  \begin{equation}
    S = \{ 0, 1, 2, \ldots, n - 1 \} \text.
  \end{equation}
  The initial distribution vector will be set to
  \begin{equation}
    \vec{\uppi}(0) = \vec{\uppi}_0 = \begin{pmatrix}
      1 & 0 & 0 & \cdots & 0
    \end{pmatrix}
  \end{equation}
  in the analysis steps ($\pi_0[x] = \delta_{0x}$).
\item The generator matrix $Q \in \RR^{n \times n}$ encodes the
  possible state transitions of the Petri net and the associated
  transition rate $\Lambda(\,\cdot\,)$ as
  \begin{gather}
    q_O[x, y] = \sum_{\mathclap{\substack{t \in T \\ M_{x} \tranto{t} M_y}}}
    \Lambda(t) \quad \text{if $x \ne y$} \text, \\
    q_O[x, x] = 0 \text, \\
    Q = Q_O - \diag \{ Q_O \vec{1}^\T \} \text,
  \end{gather}
  where the summation is done over all transition from the marking
  $M_x$ to $M_y$, while $Q_O$ and $Q_D = -\diag \{ Q_O \vec{1}^\T \}$
  are the off-diagonal and diagonal parts of $Q$, respectively.
\end{enumerate}

\begin{figure}
  \centering
  \begin{tikzpicture}
    \runningExamplePetriNet
    \node [below=2pt of a1] {$1.0$};
    \node [below=2pt of d1] {$0.5$};
    \node [below=2pt of r1] {$\frac{1}{\theta[0]} = 1.6$};
    \node [above=2pt of a2] {$1.0$};
    \node [above=2pt of d2] {$1.1$};
    \node [above=2pt of r2] {$\frac{1}{\theta[1]} = 0.8$};
  \end{tikzpicture}
  \caption{Example stochastic Petri net for the \emph{SharedResource}
    model.}
  \label{fig:background:spn:sharedresource}
\end{figure}

\begin{table}
  \centering
  $\mathllap{\RS = }\left\{\;\text{\small%
    \begin{tabular}{@{}r*{7}{c}l@{}}
      \toprule
      & $S$ & $C_1$ & $W_1$ & $S_1$ & $C_2$ & $W_2$ & $S_2$ & \\
      \midrule
      $M_0$ & 1 & 1 & 0 & 0 & 1 & 0 & 0 & initial \\
      $M_1$ & 1 & 0 & 1 & 0 & 1 & 0 & 0 & client 1 waiting \\
      $M_2$ & 1 & 1 & 0 & 0 & 0 & 1 & 0 & client 2 waiting \\
      $M_3$ & 1 & 0 & 1 & 0 & 0 & 1 & 0 & 1 waiting, 2 waiting \\
      $M_4$ & 0 & 0 & 0 & 1 & 1 & 0 & 0 & client 1 shared working \\
      $M_5$ & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 1 shared working, 2 waiting \\
      $M_6$ & 0 & 1 & 0 & 0 & 0 & 0 & 1 & client 2 shared working \\
      $M_7$ & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 waiting, 2 shared working \\
      \bottomrule
    \end{tabular}}%
  \;\right\}$
  \caption{Reachable state space of the \emph{SharedResource} model.}
  \label{tab:background:spn:sharedresource-rs}
\end{table}

\begin{figure}
  \centering
  \begin{tikzpicture}
    \newcommand*{\state}[2][]{\node [#1] (m#2) {$M_{#2}$};}
    \newcommand*{\tran}[4][above]{\draw [-{Latex},#1]
      (m#2) edge node [#1] {$#4$} (m#3);}

    \matrix [column sep=1.2cm, row sep=1.2cm,
    every node/.append style={tdk highlight, draw, circle,
      inner sep=0pt, minimum size=0.8cm}] {
      \state[thick]{0} & \state{1} & \state{4} \\
      \state{2} & \state{3} & \state{5} \\
      \state{6} & \state{7} \\
    };

    \tran[font=\footnotesize,above]{0}{1}{\frac{1}{\theta[0]}}
    \tran{1}{4}{1}
    \tran[bend right=60,above,pos=0.25,yshift=2pt]{4}{0}{0.5}
    \tran[font=\footnotesize,left]{0}{2}{\frac{1}{\theta[1]}}
    \tran[font=\footnotesize,left]{1}{3}{\frac{1}{\theta[1]}}
    \tran[font=\footnotesize,left]{4}{5}{\frac{1}{\theta[1]}}
    \tran[font=\footnotesize,above]{2}{3}{\frac{1}{\theta[0]}}
    \tran{3}{5}{1}
    \tran[bend right=60,above,pos=0.25,yshift=2pt]{5}{2}{0.5}
    \tran[left]{2}{6}{1}
    \tran[left]{3}{7}{1}
    \tran[font=\footnotesize,above]{6}{7}{\frac{1}{\theta[0]}}
    \tran[bend left=60,left,pos=0.25]{6}{0}{1.1}
    \tran[bend left=60,left,pos=0.25]{7}{1}{1.1}
  \end{tikzpicture}
  \caption{The \CTMC\ associated with the \emph{SharedResource}
    \textls{SPN} model.}
  \label{fig:background:spn:sharedresouce-ctmc}
\end{figure}

\begin{runningExample}
  \Cref{fig:background:spn:sharedresource} shows \textls{SPN}
  \emph{SharedResouce} model, which is the Petri net
  from~\vref{fig:background:petri:sharedresource} extended with
  exponential transition rates.

  The transitions $a_1$, $d_1$, $a_2$ and $d_2$ have rates $1.0$,
  $0.5$, $1.0$ and $1.1$, respectively. The parameter vector
  $\vec{\uptheta} = (0.625 \;\; 1.25) \in \RR^2$ is introduced such
  that the transitions $r_1$ and $r_2$ have rates $1 / \theta[0]$ and
  $1 / \theta[1]$.

  The reachable state space
  (\cref{tab:background:spn:sharedresource-rs}) contains $8$ markings
  which are mapped to the integers $S = \{0, 1, \ldots, 7\}$. The
  state space graph along with the transition rates of the \CTMC\ is
  shown in \cref{fig:background:spn:sharedresouce-ctmc}. The generator
  matrix is
  \begin{equation}
    Q =
    \begin{blockarray}{l*{8}{c}}
      & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
      \begin{block}{l(*{8}{c})}
        0 & \ast & \frac{1}{\theta[0]} & \frac{1}{\theta[1]} & 0 & 0 & 0 & 0 & 0 \\
        1 & 0 & \ast & 0 & \frac{1}{\theta[1]} & 1 & 0 & 0 & 0 \\
        2 & 0 & 0 & \ast &  \frac{1}{\theta[0]} & 0 & 0 & 1 & 0 \\
        3 & 0 & 0 & 0 & \ast & 0 & 1 & 0 & 1 \\
        4 & 0.5 & 0 & 0 & 0 & \ast & \frac{1}{\theta[1]} & 0 & 0 \\
        5 & 0 & 0 & 0.5 & 0 & 0 & \ast & 0 & 0 \\
        6 & 1.1 & 0 & 0 & 0 & 0 & 0 & \ast & \frac{1}{\theta[0]} \\
        7 & 0 & 1.1 & 0 & 0 & 0 & 0 & 0 & \ast \\
      \end{block}
    \end{blockarray}\;\;\text,
  \end{equation}
  where in each row the diagonal element is the negative of the sum of
  the other elemens so that $Q \vec{1}^\T = \vec{0}^\T$. The \CTMC\ is
  irreducible, therefore it has a well-defined steady-state distribution.
\end{runningExample}

\subsection{Stochastic reward nets}

\begin{dfn}
  A \emph{Stochastic Reward Net} is a triple
  $\SRN = (\SPN, \rateReward, \impulseReward)$, where $\SPN$ is a
  stochastic Petri net, $\rateReward: \NN^P \to \RR$ is a \emph{rate
    reward function} and $\impulseReward: T \times \NN^P \to \RR$ is
  an \emph{impulse reward} function.
\end{dfn}

The rate reward $\rateReward(M)$ is the reward gained per unit time in
marking $M$, while $\impulseReward(t, M)$ is the reward gained when
the transition $t$ fires in marking $M$. The instantaneous reward rate
and accumulated reward at time $t$ is denoted by $R(t)$ and $Y(t)$,
respectively.

If $\impulseReward(t, M) \equiv 0$, the \textls{SRN} is
equivalent to the Markov reward process $(X(t), \vec{r})$, where
$X(t)$ is the \CTMC\ associated with the stochastic Petri net and
\begin{equation}
  \vec{r} \in \RR^n, \quad r[x] = \rateReward(M_x) \text.
\end{equation}

If there are impulse rewards, exact calculation of the expected reward
rate $\Ex R(t)$ and expected accumulated reward $\Ex Y(t)$ can be
performed on reward process $(X, \vec{r})$,
\begin{equation}
  r[x] = \rateReward(M_x) + \sum_{\mathclap{t \in T, M_x
      \tranto{t}}} \Lambda(t) \, \impulseReward(t, M_x) \text,
\end{equation}
where the summation is taken over all enabled transitions. In general,
the distributions of $R(t)$ and $Y(t)$ cannot be derived by this
method.

\begin{runningExample}
  The \textls{SRN} model
  \begin{equation}
    \rateReward_1(M) = M(P_{S_1}) + M(P_{S_2}), \quad
    \impulseReward_1(t, M) = 0
  \end{equation}
  describes the utilization of the shared resouce in the
  \emph{SharedResouce} \textls{SPN}
  (\vref{fig:background:spn:sharedresource}). $R_1(t) = 1$ if the
  resource is allocated, hence $\Ex R_1(t)$ is the probability that
  the resource is in use at time $t$, while $Y(t)$ is the total usage
  time until $t$.

  Another reward structure
  \begin{equation}
    \rateReward_2(M) = 0, \quad \impulseReward_2(t, M) = \begin{cases}
      1 & \text{if $t \in \{t_{r_1}, t_{r_2}\}$,} \\
      0 & \text{otherwise}
    \end{cases}
  \end{equation}
  describes the calculations --~represented by places $C_1$ and
  $C_2$~-- performed successfully. The exprected steady-state reward
  rate $\lim_{t \to \infty} \Ex R(t)$ equals the number of
  calculations per unit time in a long-running system, while $Y(t)$ is
  the number of calculations performed until time $t$.

  The reward vectors of the Markov reward models associated with the
  two \textls{SRNs} above are
  \begin{align}
    \begin{blockarray}{l@{\mkern 15mu}*{8}{c}@{\mkern 10mu}r}
      & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & \\
      \begin{block}{l@{\mkern 15mu}(*{8}{c})@{\mkern 10mu}r}
        \vec{r}_1 = & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & \text,\\
      \end{block}
      \begin{block}{l@{\mkern 15mu}(*{8}{c})@{\mkern 10mu}r}
        \vec{r}_2 = & \frac{1}{\theta[0]} + \frac{1}{\theta[1]}
        & \frac{1}{\theta[1]} & \frac{1}{\theta[0]}
        & 0 & \frac{1}{\theta[1]} & 0 & \frac{1}{\theta[0]} & 0 & \text.\\
      \end{block}
    \end{blockarray}
  \end{align}
\end{runningExample}

\section{Kronecker algebra}

\begin{dfn}
  The \emph{Kronecker product} of matrices
  $A \in \RR^{n_1 \times m_1}$ and $B \in \RR^{n_2 \times m_2}$ is the
  matrix $C = A \krtimes B \in \RR^{n_1 n_2 \times m_1 m_3}$, where
  \begin{equation}
    c[i_1 n_1 + i_2, j_1 m_1 + j_2] = a[i_1, j_1] b[i_2, j_2] \text.
  \end{equation}
\end{dfn}

Some properties of the Kroncker product are
\begin{enumerate}
\item Associativity:
  \begin{equation}
    A \krtimes (B \krtimes C) = (A \krtimes B) \krtimes C \text,
  \end{equation}
\item Distributivity over matrix addition:
  \begin{equation}
    (A + B) \krtimes (C + D) = A \krtimes C + B \krtimes C + A
    \krtimes D + B \krtimes D \text,
  \end{equation}
\item Compatibility with ordinary matrix multiplication:
  \begin{gather}
    (AB) \krtimes (CD) = (A \krtimes C) (B \krtimes D) \text,
    \shortintertext{in particular,}
    A \krtimes B = (A \krtimes I_2) (I_1 \krtimes B)
  \end{gather}
  for appropriately-sized identity matrices $I_1$ and $I_2$.
\end{enumerate}