\chapter{Algorithms for stochastic analysis}
\label{chap:algorithms}

Steady state, transient, accumulated and sensitivity analysis problems
pose several numerical challanges, especially when the state space of
the \CTMC\ and the vectors and matrices involved in the computation are
extemely large.

In steady-state and sensitivty analysis, linear equations of the form
$\vec{x} A = \vec{b}$ are solved, such as%
~\vref{eq:background:ctmc:steadystate,eq:background:ctmc:sensitvity:s}. The
steady-state probability vector is the solution of the linear system
\begin{equation}
  \frac{\dd \vec{\uppi}}{\dd t} = \vec{\uppi} \, Q = \vec{0},
  \quad \vec{\uppi} \vec{1}^\T = 1 \text,
  \tag{\ref{eq:background:ctmc:steadystate}~revisited}
\end{equation}
where the infinitesimal generator $Q$ is a rank-deficient
matrix. Therefore, steady-state solution methods must handle various
generator matrix decompositions and homogenous linear equation with
rank deficient matrices. Convergence and compuation times of linear
equations solvers depend on the numerical properties of the $Q$
matrices, thus different solvers may be preferred for different
models.

In transient analysis, initial value problems with first-order linear
differetial equations such as%
~\vref{eq:background:ctmc:diffeq,eq:background:ctmc:L-ivp} are
considered. The decomposed generator matrix $Q$ must be also handled
efficiently. Another difficulty is caused by the \emph{stiffness} of
differential equations arising from some models, which may
significantly increase computation times.

To facilitate configurable stochastic analysis, we developeds several
linear equation solvers and transient analysis methods. Where it is
reasonable, the implementation is independent of the form of the
generator matrix $Q$. We achieved genericity by defining an interface
between the algorithms and the data structures with operations including
\begin{asparaitem}
\item multiplication of a matrix with a vector from left or right,
\item scalar product of vectors with other vectors and columns of
  matrices,
\item specialized operations like accessing the diagonal or
  off-diagonal parts of a matrix and replacing columns of matrices.
\end{asparaitem}

The implementation of these low-level operations is also decoupled
from the data structure. This strategy enables further configurability
by replacing the operations at runtime, for example, switching between
sequential and parallel execution for different parts of the analysis
workflow.

While high level configurability allows the modeler to select analysis
algorithms appropriate for the model and performance measures under
study, low leven configurability of the operations enables additional
customization of algorithm execution for the structure of the model as
well as the hardware in use. Benchmark results for the workflow are
discussed in \vref{sec:evaluation:results}.

In this chapter, we describe the algorithms implemented in our
stochastic analysis framework. The pseudocode of the algorithms is
annotated with the low level operations performed on the configurable
data structure by the high level algorithms.

\section{Linear equation solvers}
\label{sec:algorithms:solvers}

\subsection{Explicit solution by \textls[30]{LU} decomposition}
\label{ssec:algorithms:lu}

\textls{LU} decomposition is a direct method for solving linear
equations with forward and backward substitution, i.e.~it does not
require iteration to reach a given precision.

The decomposition computes the lower triangular matrix $L$ and upper
triangular matrix $U$ such that
\begin{equation}
  A = LU \text.
\end{equation}
To solve the equation
\begin{equation}
  \vec{x} A = \vec{x} LU = \vec{b}
\end{equation}
forward substitution is applied first to find $\vec{z}$ in
\begin{equation}
  \vec{z} U = \vec{b}\text,
\end{equation}
then $\vec{x}$ is computed by back substitution from
\begin{equation}
  \vec{x} L = \vec{b}\text.
\end{equation}

\begin{algorithm}
  \KwIn{the matrix $A \in \RR^{n \times n}$ operated on in-place}
  \KwOut{$L, U \in \RR^{n \times n}$ such that $A = LU$, $u[i, i] = 1$
    for all $i = 0, 1, \ldots, n - 1$}
  \For{$i \gets 0$ \KwTo $n - 1$}{
    \lFor{$j \gets 0$ \KwTo $i$}{%
      $a[i,j] \gets a[i,j] - \sum_{k = 0}^{j - 1} a[i, k] a[k, j]$ 
    }
    \lFor{$j \gets i + 1$ \KwTo $n - 1$}{%
      $a[i, j] \gets \bigl( a[i, j] - \sum_{k = 0}^{i - 1} a[i, k]
      a[i, j] \bigr) \big/ a[i, i]$ 
    }
  }
  Let $A_L$, $A_D$ and $A_U$ refer to the strictly lower triangular,
  diagonal and strictly upper triangular parts of $A$, respectively.\;
  $L \gets A_L + A_D$\;
  $U \gets A_U + I$\;
  \KwRet{$L, U$}\;
  \caption{Crout's \textls{LU} decomposition without pivoting.}
  \label{alg:algorithms:lu:crout}
\end{algorithm}

We used Crout's \textls{LU} decomposition%
~\citep[Section~2.3.1]{press2007numerical}, presented in
\vref{alg:algorithms:lu:crout}), which ensures
\begin{equation}
  u[i, i] = 1 \text{ for all $i = 0, 1, \ldots, n - 1$,}
\end{equation}
i.e.~the diagonal of the $U$ matrix is uniformly $1$. The matrix is
filled in during the decomposition even if it was initially sparse,
therefore it should first be copied to a dense array storage for
efficiency reasons. This considerably limits the size of Markov chains that
can be analysed by direct solution due to memory requirements. Our
data structure allows access to upper and lower diagonal parts to
matrices and linear combinations, therefore no additional storage is
needed other than $A$ itself.

\begin{algorithm}
  \KwIn{$U, L \in \RR^{n \times n}$, right vector $\vec{b} \in \RR^n$}
  \KwOut{solution of $\vec{x}LU = \vec{b}$}
  \KwAllocate{$\vec{x}, \vec{z} \in \mathbb{R}^n$}\;
  \lIf(\tcp*[f]{Skip forward substitution for homogenous equations})
  {$\vec{b} = \vec{0}$}{$\vec{z} \gets \vec{0}$}
  \KwSty{else} \lFor{$j \gets 0$ \KwTo $n - 1$}{%
    $z[j] \gets b[j] \cdot \sum_{i = 0}^{j - 1} u[i, j]$
  }
  \If{$l[n - 1, n - 1] \approx 0$}{
    \lIf(\tcp*[f]{Set the free parameter to $1$})
    {$z[n - 1] \approx 0$}{$x[n - 1] \gets 0$}
    \lElse{%
      \label{ln:algorithms:lu:substitution:not-in-image}
      \KwError{inconsistent linear equation system}
    }
  }
  \lElse{$x[n - 1] \gets z[n - 1] / l[n - 1, n - 1]$}
  \For{$j \gets n - 2$ \KwDownto $0$}{
    \lIf{$l[j, j] \approx 0$}{%
      \label{ln:algorithms:lu:substitution:rank-too-low}
      \KwError{more than one free parameter}
    }
    $x[j] \gets \bigl( z[i] - \sum_{i = j + 1}^{n - 1} x[i] l[i, j]
    \bigr) \big/ l[j, j]$\;
  }
  \KwRet{$\vec{x}$}\;
  \caption{Forward and back substitution.}
  \label{alg:algorithms:lu:substitution}
\end{algorithm}

The forward and back substitution process is shown in
\vref{alg:algorithms:lu:substitution}. If multiple equations are
solver with the same matrix, its \textls{LU} decomposition may be
cached.

\subsubsection{Matrices of less than full rank}

If the matrix $Q$ is of rank $n - 1$, the element $l[n - 1, n - 1]$ in
Crout's \textls{LU} decomposition will be $0$. In this case,
$x[n - 1]$ is a free parameter and will be set to $1$ to yield a
nonzero solution vector when $z[n - 1] = 0$. If $z[n - 1] \ne 0$, the
equation $\vec{x} L = \vec{z}$ does not have a solution and the error
condition in line~\ref{ln:algorithms:lu:substitution:not-in-image} is
triggered. A matrix of rank less than $n - 1$ triggers the error
condition in line~\ref{ln:algorithms:lu:substitution:rank-too-low}.

In practice, the algorithm can be used to solve homogenous equations
in Markovian analysis, because the infinitesimal generator matrix $Q$
of an irreducible \CTMC\ is always of rank $n - 1$. The solution
vector $\vec{x}$ is not a probability vector in general, so it must be
normalized as $\vec{\pi} = \vec{x} / \vec{x} \vec{1}^\T$ to get a
stationary probability distribution vector.

\subsection{Iterative methods}

\begin{algorithm}
  \KwIn{matrix $A \in \RR^{n \times n}$, right vector $\vec{b} \in
    \RR^n$, initial guess $\vec{x} \in \RR^n$, tolerance $\tau > 0$}
  \KwOut{approximate solution of $\vec{x}A = \vec{b}$ and its residual
    norm}
  \KwAllocate{$\vec{x}' \in \mathbb{R}^n$}
  \tcp*{Previous iterate for convergence test}
  \Repeat{$\|\vec{x}' - \vec{x}\| \le \tau$}{
    \nllabel{ln:algorithms:iterative:basic:loopbeg}
    $\vec{x}' \gets \vec{x}$\tcp*{Save the previous vector}
    $\vec{x} \gets f(\vec{x}')$\;
  }
  \nllabel{ln:algorithms:iterative:basic:loopend}
  \KwRet{$\vec{x}$ and $\| \vec{x}Q - \vec{b} \|$}\;
  \caption{Basic iterative scheme for solving linear equations.}
  \label{alg:algorithms:iterative:basic}
\end{algorithm}

Iterative methods express the solution of the linear equation $\vec{x}
A = \vec{b}$ as a recurrence
\begin{equation}
  \vec{x}_{k} = f(\vec{x}_{k - 1}) \text,
\end{equation}
where $\vec{x}_0$ is an initial guess vector. The iteration converges
to a solution vector when $\lim_{k \to \infty} \vec{x}_k = \vec{x}$
exists and $\vec{x}$ equals the true solution vector $\vec{x}^*$. The
iteration is illustrated in \vref{alg:algorithms:iterative:basic}.

The process is assumed to have converged if subsequent iterates are
sufficiently close, i.e.~the stopping criterion at the $k$th iteration
is
\begin{equation}
  \label{eq:algorithms:iterative:convergence}
  \| \vec{x}_{k} - \vec{x}_{k - 1} \| \le \tau
\end{equation}
for some prescribed tolerance $\tau$. In our implementation, we
selected the $L^1$-norm
\begin{equation}
  \| \vec{x}_{k} - \vec{x}_{k - 1} \| = \sum_{i} \bigl\lvert x_{k}[i] - x_{k -
    1}[i] \bigr\rvert
\end{equation}
as the vector norm used for detecting convergence.

Premature termination may be avoided if iterates spaced $m > 1$
iterations apart are used for convergence test
($\| \vec{x}_k - \vec{x}_{k - m} \| \le \tau$), but only at the
expense of additional memory required for storing $m$ previous
iterates. In order to handle large Markov chains with reasonable
memory consumption, we only used the convergence test with a single
previous iterate.

Correctness of the solution can be checked by observing the norm of
the residual $\vec{x}_{k} A - \vec{b}$, since the error vector
$\vec{x}_k - \vec{x}^*$ is generally not available. Because the
additional matrix multiplication may make the latter check costly, it
is performed only after detecting convergence by
\vref{eq:algorithms:iterative:convergence}. Unfortunately, the
residual norm may not be representative of the error norm if the
problem is ill-conditioned.

For a detailed discussion stopping criterions and iterate
normalization in steady-state \CTMC\ analysis, we refer
to~\citep[Section~10.3.5]{stewart2009probability}.

\subsubsection{Power iteration}
\label{ssec:algorithms:power}

Power iteration~\citep[Section~10.3.1]{stewart2009probability} is the
one of the simplest iterative methods for Markovian analysis. Its
iteration function has the form
\begin{equation}
  \vec{x}_k = f(\vec{x}_{k - 1}) = \vec{x}_{k - 1} + \frac{1}{\alpha} \mleft(
  \vec{x}_{k - 1} A - \vec{b} \mright) \text.
\end{equation}

The iteration converges if the diagonal elements $a[i,i]$ of $A$ are
strictly negative, the off-diagonal elements $a[i,j]$ are nonnegative
and $\alpha \ge \max_{i} \lvert a[i,i] \rvert$. The matrix $A$
satisfies these properties if it is an inifinitesimal generator matrix
of an irreducible \CTMC. The fastest convergence is achieved when
$\alpha = \min_{i} \lvert a[i,i] \rvert$.

\begin{algorithm}
  $\alpha^{-1} \gets 1 / \max_{i} \lvert a[i,i] \rvert$\;
  \Repeat{$\epsilon \le \tau$}{
    $\vec{x}' \gets \vec{x} A$
    \tcp*{Vector-matrix product}
    $\vec{x}' \gets \vec{x}' + (-1) \cdot \vec{x}$
    \tcp*{In-place scaled vector addition}
    $\epsilon \gets \alpha^{-1} \| \vec{x}' \|$
    \tcp*{Vector norm calculation}
    $\vec{x} \gets \vec{x} + \alpha^{-1} \vec{x}'$
    \tcp*{In-place scaled vector addition}
  }
  \caption{Power iteration.}
  \label{alg:algorithms:iterative:power}
\end{algorithm}

Power iteration can be realized by replacing lines%
~\ref{ln:algorithms:iterative:basic:loopbeg}%
--\ref{ln:algorithms:iterative:basic:loopend} in
\vref{alg:algorithms:iterative:basic} with the loop in
\vref{alg:algorithms:iterative:power}.

This realization uses memory efficiently, because it only requires the
allocation of a single vector $\vec{x}'$ in addition to the initial
guess $\vec{x}$.

\begin{obs}
  \label{obs:algorithms:iterative:power-keepnorm}
  If $\vec{b} = 0$ and $A$ is an inifitesimal generator matrix, then
  \begin{align}
    \vec{x}_k \vec{1}^\T &= \mleft[ \vec{x}_{k - 1} + \frac{1}{\alpha}
                           \mleft( \vec{x}_{k - 1} A - \vec{b}
                           \mright) \mright] \vec{1}^\T \\
                         &= \vec{x}_{k - 1} \vec{1}^\T +
                           \frac{1}{\alpha} \vec{x}_{k - 1} A
                           \vec{1}^T - \vec{b} \vec{1}^\T \\
                         &= \vec{x}_{k - 1} \vec{1}^\T +
                           \frac{1}{\alpha} \vec{x}_{k - 1} \vec{0}^\T
                           - \vec{0} \vec{1}^\T = \vec{x}_{k - 1} \vec{1}^\T.
  \end{align}
  This means the sum of the elements of the result vector $\vec{x}$
  and the initial guess vector $\vec{x}_0$ are equal, because the
  iteration leaves the sum unchanged.
\end{obs}

To solve an equation of the form
\begin{equation}
  \label{eq:algorithms:iterative:homogenous-prob}
  \vec{x} Q = \vec{0},\quad \vec{x} \vec{1}^\T = 1
\end{equation}
where $Q$ is an infinitesimal generator matrix, the initial guess
$\vec{x}_0$ is selected such that $\vec{x}_0 \vec{1}^\T = 1$. If the
\CTMC\ described by $Q$ is irreducible, we may select
\begin{equation}
  \label{eq:algorithms:iterative:power-init}
  x_0[i] \equiv \frac{1}{n} \text,
\end{equation}
where $n$ is the dimensionality of $\vec{x}$. After the initial guess
is selected, the equation $\vec{x} \vec{1}^\T$ may be ignored to solve
$\vec{x} Q = \vec{0}$ with the power method. This process yields the
solution of the original problem%
~\eqref{eq:algorithms:iterative:homogenous-prob}.

\subsubsection{Jacobi and Gauss--Seidel iteration}
\label{ssec:algorithms:jgs}

Jordan and Gauss--Seidel iterative methods%
~\citep[Section~10.3.2--3]{stewart2009probability} repeatedly solve a
system of simultaneous equations of a specific form.

In Jordan iteration, the system
{\small\begin{equation} \left.
      \begin{array}{r@{${} = {}$}*{2}{r@{\kern1pt}l@{${} + {}$}}c@{${} + {}$}r@{\kern1pt}l}
        b[0] & x_{k}[0] & a[0,0] & x_{k - 1}[1] & a[1,0] & \cdots & x_{k - 1}[n - 1] & a[n - 1,0] \text,\\
        b[1] & x_{k - 1}[0] & a[0,1] & x_{k}[1] & a[1,1] & \cdots & x_{k - 1}[n - 1] & a[n - 1,1] \text,\\
        \multicolumn{8}{c}{$\vdots$} \\
        b[n - 1] & x_{k - 1}[0] & a[0,n - 1] & x_{k - 1}[1] & a[1,n - 1] & \cdots & x_{k}[n - 1] & a[n - 1,n - 1] \text,\\
      \end{array}
    \right\}
  \end{equation}}%
is solved for $\vec{x}_{k}$ at each iteration, i.e.~there is a
single unknown in each row and the rest of the variables are taken
from the previous iterate. In vector form, the iteration can be
expressed as
\begin{equation}
  \vec{x}_{k} = A_D^{-1} (\vec{b} - A_O \vec{x}_{k - 1}) \text,
\end{equation}
where $A_D$ and $A_O$ are the diagonal (all off-diagonal elements are
zero) and off-diagonal (all diagonal elements are zero) parts of $A =
A_D + A_O$.

In Gauss--Seidel iteration, the linear system
{\small\begin{equation}
    \left.
      \begin{array}{r@{${} = {}$}*{2}{r@{\kern1pt}l@{${} + {}$}}c@{${} + {}$}r@{\kern1pt}l}
        b[0] & x_{k}[0] & a[0,0] & x_{k - 1}[1] & a[1,0] & \cdots & x_{k - 1}[n - 1] & a[n - 1,0] \text,\\
        b[1] & x_{k}[0] & a[0,1] & x_{k}[1] & a[1,1] & \cdots & x_{k - 1}[n - 1] & a[n - 1,1] \text,\\
        \multicolumn{8}{c}{$\vdots$} \\
        b[n - 1] & x_{k}[0] & a[0,n - 1] & x_{k}[1] & a[1,n - 1] & \cdots & x_{k}[n - 1] & a[n - 1,n - 1] \text,\\
      \end{array}
    \right\}
  \end{equation}}%
is considered, i.e.~the $i$th equation contains the first $i$ elements
of $\vec{x}_k$ as unknowns. The equations are solved for successive
elements of $\vec{x}_k$ from top to bottom.

\begin{algorithm}
  \KwIn{matrix $A \in \RR^{n \times n}$, right vector $\vec{b} \in
    \RR^n$, initial guess $\vec{x} \in \RR^n$, tolerance $\tau > 0$,
    over-relaxation parameter $\omega > 0$}
  \KwOut{approximate solution of $\vec{x}A = \vec{b}$}
  \KwAllocate{$\vec{x}' \in \RR^n$}\;
  Let $A_O$ refer to the off-diagonal part of $A$.\;
  \Repeat{$\epsilon \le \tau$}{
    $\vec{x}' \gets \vec{x} A_O$
    \tcp*{Matrix-vector product}\label{ln:algorithms:jacobi:product}
    $\vec{x}' \gets \vec{x}' + (-1) \cdot \vec{b}$\label{ln:algorithms:jacobi:addition}
    \tcp*{In-place scaled vector addition}
    $\epsilon \gets 0$\;
    \For{$i \gets 0$ \KwTo $n - 1$}{
      \label{ln:algorithms:jacobi:innerloop}
      $y \gets (1 - \omega) x[i] - \omega x'[i] / a[i, i]$\;
      $\epsilon \gets \epsilon + \lvert y - x[i] \rvert$\;
      $x[i] \gets y$\;
    }
  }
  \KwRet{$\vec{x}$}\;
  \caption{Jacobi over-relaxation.}
  \label{alg:algorithms:iterative:jacobi}
\end{algorithm}

Jacobi over-relaxation, a generalized form of Jacobi iteraion, is
realized in \vref{alg:algorithms:iterative:jacobi}. The value $1$ of
the over-relaxation paramter $\omega$ corresponds to ordinary Jacobi
iteration. Values $\omega > 1$ may accelerate convergence, while
$0 < \omega < 1$ may help diverging Jacobi iteration converge.

Jacobi over-relaxation has many parallelization opportunities. The
matrix multiplication in line~\ref{ln:algorithms:jacobi:product} and
the vector addition in line~\ref{ln:algorithms:jacobi:addition} can be
parallelized, as well as the for loop in
line~\ref{ln:algorithms:jacobi:innerloop}. Our implementation takes
advantage of the configurable linear algebra operations framework to
execute lines~\ref{ln:algorithms:jacobi:product} and
\ref{ln:algorithms:jacobi:addition} with possible paralellization
considering the structures of both the vectors $\vec{x}, \vec{x}'$ and
the matrix $A$. However, the inner loop is left sequential to reduce
implementation complexity, as it represents only a small fraction of
execution time compared to the matrix-vector product.

\begin{algorithm}
  \KwIn{matrix $A \in \RR^{n \times n}$, right vector $\vec{b} \in
    \RR^n$, initial guess $\vec{x} \in \RR^n$, tolerance $\tau > 0$,
    over-relaxation parameter $\omega > 0$}
  \KwOut{approximate solution of $\vec{x}A = \vec{b}$}
  \KwAllocate{$\vec{x}' \in \RR^n$}\;
  Let $A_O$ refer to the off-diagonal part of $A$.\;
  \Repeat{$\epsilon \le \tau$}{
    $\epsilon \gets 0$\;
    \For{$i \gets 0$ \KwTo $n - 1$}{
      $\textit{scalarProduct} \gets \vec{x} \cdot \vec{a}_O[\cdot, i]$
      \tcp*{Scalar product with column of matrix}
      $y \gets \omega (b[i] - \textit{scalarProduct}) / a[i, i] + (1 -
      \omega) \cdot x[i]$\;
      $\epsilon \gets \epsilon + \lvert y - x[i] \rvert$\;
      $x[i] \gets y$\;
    }
  }
  \KwRet{$\vec{x}$}\;
  \caption{Gauss--Seidel successive over-relaxatation.}
  \label{alg:algorithms:iterative:gs}
\end{algorithm}

\Vref{alg:algorithms:iterative:gs} shows an implementation of
successive over-relaxation for Gauss--Seidel iteration, where the
notation $\vec{a}_O[\cdot, i]$ refers to the $i$th column of $A_O$.

Gauss--Seidel iteration cannot easily be parallelized, because
calculation of successive elements $x[0], x[1], \ldots$ depend on all
of the prior elements. However, in contrast with Jacobi iteration, no
memory is required in addition to the vectors $\vec{x}$, $\vec{b}$ and
the matrix $X$, which makes the algorithm suitable for very large
vectors and memory-constrained situations. In addition, convergence is
often significantly faster.

The sum of elements $\vec{x} \vec{1}^\T$ does not stay constant during
Jacobi or Gauss--Seidel iteration. Thus, when solving equations of the
form $\vec{x} Q = \vec{0}, \vec{x} \vec{1}^\T = 1$, normalization
cannot be entierly handled by the initial guess. We instead transform
the equation into the form
\begin{equation}
  \label{eq:algorithms:iterative:normalized-linear-system}
  \vec{x} \begin{pmatrix}
    q[0,0] & q[0,1] & \cdots & q[0,n - 2] & 1 \\
    q[1,0] & q[1,1] & \cdots & q[1,n - 2] & 1 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    q[n - 2,0] & q[n - 2,1] & \cdots & q[n - 2,n - 2] & 1 \\
    q[n - 1,0] & q[n - 1,1] & \cdots & q[n - 2,n - 1] & 1 \\
  \end{pmatrix} = \begin{pmatrix}
    0 \\ 0 \\ \vdots \\ 0 \\ 1
  \end{pmatrix} \text,
\end{equation}
where we take advantage of the fact that the infinitesimal generator
matrix is not of full rank, therefore one of the columns is redundant
and can be replaced with the condition $\vec{x} \vec{1}^\T = 1$. While
this transformation may affect the convergence behavior of the
algorithm, it allows uniform handling of homogenous and non-homogenous
linear equations.

\subsubsection{Group iterative methods}
\label{ssec:algorithms:group-jgs}

\emph{Group} or \emph{block} iterative methods%
~\citet[Section~10.4]{stewart2009probability} assume the block
structure for the vectors $\vec{x}$, $\vec{b}$ and the matrix $A$
\begin{equation}
  \vec{x}[i] \in \RR^{n_i}, \vec{b}[j] \in \RR^{n_j}, A[i, j] \in
  \RR^{n_i \times n_j} \text{ for all $i, j \in \{0, 1, \ldots,
    N - 1\}$,}
\end{equation}
Infinitesimal generator matrices in the block Kronecker decomposition
along with appropriately partitioned vectors match this structure (see
\vref{eq:genstor:explicit:block:block}). Each block of $\vec{x}$
corresponds to a group a variables that are simultaneously solved for.

Group Jacobi iteration solves the linear system
{\small\begin{equation}
    \left.
      \begin{array}{r@{${} = {}$}*{2}{r@{\kern1pt}l@{${} + {}$}}c@{${} + {}$}r@{\kern1pt}l}
        \vec{b}[0] & \vec{x}_{k}[0] & A[0,0] & \vec{x}_{k - 1}[1] & A[1,0] & \cdots & \vec{x}_{k - 1}[n - 1] & A[n - 1,0] \text,\\
        \vec{b}[1] & \vec{x}_{k - 1}[0] & A[0,1] & \vec{x}_{k}[1] & A[1,1] & \cdots & \vec{x}_{k - 1}[n - 1] & A[n - 1,1] \text,\\
        \multicolumn{8}{c}{$\vdots$} \\
        \vec{b}[n - 1] & \vec{x}_{k - 1}[0] & A[0,n - 1] & \vec{x}_{k - 1}[1] & A[1,n - 1] & \cdots & \vec{x}_{k}[n - 1] & A[n - 1,n - 1] \text,\\
      \end{array}
    \right\}
  \end{equation}}%
while group Gauss--Seidel considers
{\small\begin{equation}
    \left.
      \begin{array}{r@{${} = {}$}*{2}{r@{\kern1pt}l@{${} + {}$}}c@{${} + {}$}r@{\kern1pt}l}
        \vec{b}[0] & \vec{x}_{k}[0] & A[0,0] & \vec{x}_{k - 1}[1] & A[1,0] & \cdots & \vec{x}_{k - 1}[n - 1] & A[n - 1,0] \text,\\
        \vec{b}[1] & \vec{x}_{k}[0] & A[0,1] & \vec{x}_{k}[1] & A[1,1] & \cdots & \vec{x}_{k - 1}[n - 1] & A[n - 1,1] \text,\\
        \multicolumn{8}{c}{$\vdots$} \\
        \vec{b}[n - 1] & \vec{x}_{k}[0] & A[0,n - 1] & \vec{x}_{k}[1] & A[1,n - 1] & \cdots & \vec{x}_{k}[n - 1] & A[n - 1,n - 1] \text.\\
      \end{array}
    \right\}
  \end{equation}}

\begin{algorithm}
  \KwIn{block matrix $A$, block right vector $\vec{b}$, block initial
    guess $\vec{n}$, tolerance $\tau > 0$, over-relaxation parameter
    $\omega > 0$}
  \KwOut{approximate solution of $\vec{x}A = \vec{b}$ and its residual
    norm}
  \KwAllocate{$\vec{x}'$ and $\vec{c}$ with the same block structure
    as $\vec{x}$ and $\vec{b}$}\;
  Let $A_{OB}$ represent the off-diagonal part of the block matrix $A$
  with the blocks along the diagonal set to zero.\;
  \Repeat{$\epsilon \le \tau$}{
    $\vec{x}' \gets \vec{x}, \vec{c} \gets \vec{b}$\;
    $\vec{c} \gets \vec{c} + (-1) \cdot \vec{x}' A_{OB}$
    \tcp*{Scaled accumulation of vector-matrix product}
    \ParFor(\tcp*[f]{Loop over all blocks}){$i \gets 0$ \KwTo $N - 1$}{
      Solve $\vec{x}[i] A[i,i] = \vec{c}[i]$ for $\vec{x}[i]$\;
    }
    $\epsilon \gets 0$\;
    \For(\tcp*[f]{Loop over all elements}){$k \gets 0$ \KwTo $n - 1$}{
      $y \gets \omega x[k] + (1 - \omega) x'[k]$\;
      $\epsilon \gets \epsilon + \lvert y - x'[k] \rvert$\;
      $x[k] \gets y$\;
    }
  }
  \caption{Group Jacobi over-relaxation.}
  \label{alg:algorithms:iterative:group-jacobi}
\end{algorithm}

\begin{algorithm}
  \KwIn{block matrix $A$, block right vector $\vec{b}$, block initial
    guess $\vec{n}$, tolerance $\tau > 0$, over-relaxation parameter
    $\omega > 0$}
  \KwOut{approximate solution of $\vec{x}A = \vec{b}$ and its residual
    norm}
  \KwAllocate{$\vec{x}'$ and $\vec{c}$ large enough to store a single
    block of $\vec{x}$ and $\vec{b}$.}\;
  \Repeat{$\epsilon \le \tau$}{
    $\epsilon \gets 0$\;
    \For(\tcp*[f]{Loop over all blocks}){$i \gets 0$ \KwTo $N - 1$}{
      $\vec{x}' \gets \vec{x}[i], \vec{c} \gets \vec{b}[i]$\;
      \For{$j \gets 0$ \KwTo $N - 1$}{
        \If(\tcp*[f]{Scaled accumulation of vector-matrix product}){$i \ne j$}{
          $\vec{c} \gets \vec{c} + (-1) \cdot \vec{x}[j] A[i, j]$\;
        }
      }
      Solve $\vec{x}[i] A[i,i] = \vec{c}$ for $\vec{x}[i]$\;
      \For{$k \gets 0$ \KwTo $n_i - 1$}{
        $y \gets \omega x[i][k] + (1 - \omega) x'[k]$\;
        $\epsilon \gets \epsilon + \lvert y - x'[k] \rvert$\;
        $x[i][k] \gets y$\;
      }
    }
  }
  \caption{Group Gauss--Seidel successive over-relaxation.}
  \label{alg:algorithms:iterative:group-gs}
\end{algorithm}

Implementations of group Jacobi over-relaxation and group Gauss--Seidel
successive over-relaxation are shown in
\vref{alg:algorithms:iterative:group-jacobi,alg:algorithms:iterative:group-gs}.
The inner linear equations of the form $\vec{x}[i] A[i,i] = \vec{c}$
may be solved by any algorithm, for example, LU decomposition,
iterative methods, or even block-iterative methods if $A$ has a
two-level block structure. The choice of the inner algorithm may
significantly affect performance and care must be taken to avoid
diverging inner solutions in an iterative solver is used.

In Jacobi over-relaxation, paralellization of both the matrix
multiplication and the inner loop is possible. However, two vectors of
the same size as $\vec{x}$ are required for temporary storage.

Gauss--Seidel successive over-relaxation cannot be parallelized easily,
but it requires only two temporary vectors of size equal to the largest
block of $\vec{x}$, much less than Jacobi over-relaxation. Moreover, it
often requires fewer steps to converge, making it preferable over
Jacobi iteration.

Because the inner solver may be selected by the user and thus its
convergence behaviour varies widely, we do not perform the
transformation for homogenous equations
\eqref{eq:algorithms:iterative:normalized-linear-system}. Instead, the
normalization $\vec{\uppi} = \vec{x} / \vec{x}\vec{1}^\T$ is performed
only after finding any nonzero solution of $\vec{x} Q = \vec{0}$.

For a detailed analysis of the convergence behaviour of group
iterative methods, we refer to
\citet[Chapter~14]{greenbaum1997iterative} and \citet{}

\subsubsection{BiConjugate Gradient Stabilized (\textls[30]{BiCGSTAB})}
\label{ssec:algorithms:bicgstab}

BiConjugate Gradient Stabilized~(\textls{BiCGSTAB})%
~\mkbibbrackets{\nakedcite[Section~7.4.2]{saad2003iterative};~\nakedcite{van1992bi}}
is an iterative algorithm belonging to the class of Krylov subspace
methods, which includes other algorithms such as the Generalized
Minimum Residual~(\textls{GMRES})~\citep{saad1986gmres}, Conjugate Gradient
Squared~(\textls{CGS})~\citep{sonneveld1989cgs} and
IDR$(s)$~\citep{sonneveld2008idr}.

We selected \textls{BiCGSTAB} as the Krylov subspace solver in our
framework because of its good convergence behaviour and low memory
requirements. \textls{BiCGSTAB} only requires the storage of 7
vectors, which makes it suitable even for large state spaces with
large states vectors, unlike e.g.~\textls{GMRES}, which allocates an
additional vector every iteration.

\begin{algorithm}
  \KwIn{matrix $A \in \RR^{n \times n}$, right vector $\vec{b} \in
    \RR^n$, initial guess $\vec{x} \in \RR^n$, tolerance $\tau > 0$}
  \KwOut{approximate solution of $\vec{x}A = \vec{b}$}
  \KwAllocate{$\vec{r}, \vec{r}_0, \vec{v}, \vec{p}, \vec{s}, \vec{t}
    \in \RR^n$}\;
  $\vec{r} \gets \vec{b}$\;
  $\vec{r} \gets \vec{r} + (-1) \cdot \vec{x} A$
  \tcp*{Scaled accumulation of vector-matrix product}
  \If{$\| \vec{r} \| \le \tau$}{
    \KwMessage{initial guess is correct, skipping iteration}\;
    \KwRet{$\vec{x}$}\;
  }
  $\vec{r}_0 \gets \vec{r}, \vec{v} \gets \vec{0}, \vec{p} \gets
  \vec{0}, \rho' \gets 1, \alpha \gets 1, \omega \gets 1$\;
  \While{\KwTrue}{
    $\rho \gets \vec{r_0} \cdot \vec{r}$
    \tcp*{Scalar product}
    \lIf{$\rho \approx 0$}{%
      \KwError{breakdown: $\vec{r} \perp \vec{r}_0$}
    }
    $\beta \gets \rho / \rho' \cdot \alpha / \omega$\;
    $\vec{p} \gets \vec{r} + \beta \cdot \vec{p}$
    \tcp*{Scaled vector addition}
    $\vec{p} \gets \vec{p} + (-\beta \omega) \cdot \vec{v}$
    \tcp*{In-place scaled vector addition}
    $\alpha \gets \rho / (\vec{r}_0 \cdot \vec{v})$
    \tcp*{Scalar product}
    $\vec{r} \gets \vec{s} + (-\alpha) \cdot \vec{s}$
    \tcp*{Scaled vector addition}
    \If{$\| \vec{s} \| < \tau$}{
      $\vec{x} \gets \vec{x} + \alpha \cdot \vec{p}$
      \tcp*{In-place scaled vector addition}
      \KwMessage{early return with vanishing $\vec{s}$}\;
      \KwRet{$\vec{x}$}\;
    }
    $\vec{t} \gets \vec{s} A$
    \tcp*{Vector-matrix multiplication}
    $\textit{tLengthSquared} \gets \vec{t} \cdot \vec{t}$
    \tcp*{Scalar product}
    \lIf{$\textit{tLengthSquared} \approx 0$}{%
      \KwError{breakdown: $\vec{t} \approx \vec{0}$}
    }
    $\omega \gets (\vec{t} \cdot \vec{s}) / \textit{tLengthSquared}$
    \tcp*{Scalar product}
    \lIf{$\omega \approx 0$}{%
      \KwError{breakdown: $\omega \approx 0$}
    }
    $\epsilon \gets 0$\;
    \For{$i \gets 0$ \KwTo $n - 1$}{
      $\textit{change} \gets \alpha p[i] + \omega s[i]$\;
      $\epsilon \gets \epsilon + \lvert \textit{change} \rvert$\;
      $x[i] \gets x[i] + \textit{change}$\;
    }
    \lIf{$\epsilon \le \tau$}{%
      \KwRet{$\vec{x}$}
    }
    $\vec{s} \gets \vec{t} + (-\omega) \cdot \vec{r}$
    \tcp*{Scaled vector addition}
    $\rho' \gets \rho$\;
  }
  \caption{\textls{BiCGSTAB} iteration without preconditioning.}
  \label{alg:algorithms:iterative:bicgstab}
\end{algorithm}

\Vref{alg:algorithms:iterative:bicgstab} shows the pseudocode for
\textls{BiCGSTAB}. Our implementation is based on the \textsc{Matlab}
code\footnote{\url{http://www.netlib.org/templates/matlab//bicgstab.m}}
by \citet{barrett1994templates}.

Solving preconditioned equations in the form
$\vec{x} A M^{-1} = \vec{b} M^{-1}$ could improve convergence, but was
omitted from our current implementation. As the choice is appropriate
preconditioner matrices $M$ is not trivial%
~\citep{DBLP:journals/informs/LangvilleS04}, implementation and sudy
of preconditioners for Markov chains, especially with block Kronecker
decomposition, is in the scope of our future work.

Because six vectors are allocated in addition to $\vec{x}$ and
$\vec{b}$, the amount of available memory may be a significant
bottleneck. 

Similar to \vref{obs:algorithms:iterative:power-keepnorm}, it can be
seen that the sum $\vec{x} \vec{1}^\T$ stays constant throughout
\textls{BiCGSTAB} iteration. Thus, we can find probability vectors
satisfying homogenous equations by the initialization in%
~\vref{eq:algorithms:iterative:power-init}.

\section{Transient analysis}

\subsection{Uniformization}
\label{ssec:algorithms:uniformization}

The \emph{uniformization} or \emph{randomization} method solves the
initial value problem
\begin{gather}
  \frac{\dd \vec{\uppi}(t)}{\dd t} = \vec{\uppi}(t) \, Q, \quad
  \vec{\uppi}(t) = \vec{\uppi}{0}
  \tag{\ref{eq:background:ctmc:diffeq}~revisited}\\
  \shortintertext{by computing}
  \vec{\uppi}(t) = \sum_{k = 0}^{\infty} \vec{\uppi}_0 P^k e^{-\alpha
    t} \frac{(\alpha t)^k}{k!}\text,
  \label{eq:algorithms:transient:uniformization}
\end{gather}
where $P = \alpha^{-1} Q + I$,
$\alpha \ge \max_{i} \lvert a[i, i] \rvert$ and
$e^{-\alpha t} \frac{(\alpha t)^k}{k!}$ is the value of the Poisson
probabilty function with rate $\alpha t$ at $k$.

Integrating both sides of
\vref{eq:algorithms:transient:uniformization} to compute $\vec{L}(t)$
yields~\citep{reibman1989markov}
\begin{align}
  \int_{0}^t \vec{\uppi}(u) \,\dd u = \vec{L}(t)
  &= \sum_{k = 0}^{\infty} \vec{\uppi}_0 P^k \int_{0}^{t} e^{-\alpha
    u} \frac{(\alpha u)^k}{k!} \,\dd u \\
  &= \sum_{k = 0}^{\infty} \vec{\uppi}_0 P^k \frac{1}{\alpha} \sum_{l
    = k + 1}^{\infty} e^{-\alpha t} \frac{(\alpha t)^l}{l!} \\
  &= \frac{1}{\alpha} \sum_{k = 0}^{\infty} \vec{\uppi}_0 P^k \mleft(
    1 - \sum_{l = 0}^{k} e^{-\alpha t} \frac{(\alpha t)^l}{l!}
    \mright) \text.
    \label{eq:algorithms:transient:aggregate-uniformization}
\end{align}

Both \vref{eq:algorithms:transient:uniformization,%
eq:algorithms:transient:aggregate-uniformization} can be realized
as
\begin{equation}
  \vec{x} = \frac{1}{W} \mleft(\; \sum_{k = 0}^{k_{\text{left}} - 1} w_{\text{left}}
  \vec{\uppi}_0 P^k + \sum_{k = k_{\text{left}}}^{k_{\text{right}}}
  w[k - k_{\text{left}}] \vec{\uppi}_0 P^k \!\mright) \text,
  \label{eq:algorithms:transient:uniformization-impl}
\end{equation}
where $\vec{x}$ is either $\vec{\uppi}(t)$ or $\vec{L}(t)$,
$k_{\text{left}}$ and $k_{\text{right}}$ are \emph{trimming constants}
selected based on the required precision, $\vec{w}$ is a vector of
(possibly accumulated) Poisson weights and $W$ is a scaling
factor. The weight before the left cutoff $w_{\text{left}}$ is $1$ if
the accumulated probability vector $\vec{L}(t)$ is calculated, $0$
otherwise.

\begin{algorithm}
  \KwIn{infinitesimal generator $Q \in \RR^{n \times n}$, initial
    probability vector $\vec{\uppi}_0 \in \RR^n$, truncation
    parameters $k_{\text{left}}, k_{\text{right}} \in \NN$, weights
    $w_{\text{left}} \in \RR$,
    $\vec{w} \in \RR^{k_{\text{right}} - k_{\text{left}}}$, scaling
    constant $W \in \RR$, tolerance $\tau > 0$}
  \KwOut{instantenous or accumulated probability vector $\vec{x} \in
    \RR^n$}
  \KwAllocate{$\vec{x}, \vec{p}, \vec{q} \in \RR^n$}\;
  $\alpha^{-1} \gets 1 / \max_{i} \lvert a[i,i] \rvert$
  \label{ln:algorithms:transient:uniformization:alpha}\;
  $\vec{p} \gets \vec{\uppi}_0$\;
  \leIf(\tcp*[f]{Vector scaling})
  {$w_{\text{left}} = 0$}{$\vec{x} \gets \vec{0}$}
  {$\vec{x} \gets w_{\text{left}} \cdot \vec{p}$}
  \For{$k \gets 1$ \KwTo $k_{\text{right}}$}{
    $\vec{q} \gets \vec{p} Q$
    \tcp*{Vector-matrix product}
    $\vec{q} \gets \alpha^{-1} \cdot \vec{q}$
    \tcp*{In-place vector scaling}
    $\vec{q} \gets \vec{q} + \vec{q}$
    \tcp*{In-place vector addition}
    \If{$\| \vec{q} - \vec{p} \| \le \tau$}{
      \label{ln:algorithms:transient:uniformization:steadystate-det}
      $\vec{x} \gets \vec{x} + \mleft( \sum_{l = k}^{k_\text{right}}
      w[l - k_{\text{left}}] \mright) \cdot \vec{q}$
      \tcp*{In-place scaled vector addition}
      \KwSty{break}\;
    }
    \lIf(\tcp*[f]{In-place scaled vector addition})
    {$k < k_{\text{left}} \land w_{\text{left}} \ne 0$}{%
      $\vec{x} \gets \vec{x} + w_{\text{left}} \cdot \vec{q}$
    }
    \lElseIf(\tcp*[f]{In-place scaled vector addition})
    {$k \ge k_{\text{left}}$}{%
      $\vec{x} \gets \vec{x} + w[k - k_{\text{left}}] \cdot \vec{q}$
    }
    Swap the references to $\vec{p}$ and $\vec{q}$
  }
  $\vec{x} \gets W^{-1} \cdot \vec{x}$
  \tcp*{In-place vector scaling}
  \KwRet{$\vec{x}$}\;
  \caption{Uniformization.}
  \label{alg:algorithms:transient:uniformization}
\end{algorithm}

\Vref{eq:algorithms:transient:uniformization-impl} is implemented by
\vref{alg:algorithms:transient:uniformization}. The algorithm performs
\emph{steady-state} detection in line%
~\ref{ln:algorithms:transient:uniformization:steadystate-det} to avoid
unneccessary work once the iteration vector $\vec{p}$ reaches the
steady-state distribution $\vec{\uppi}(\infty)$,
i.e.~$\vec{p} \approx \vec{p} P$. If the initial distribution
$\vec{\uppi}_0$ is not further needed or can be generated efficiently
(as it is the case with a single initial state), the result vector
$\vec{x}$ may share the same storing, resulting in a memory overhead
of only two vectors $\vec{p}$ and $\vec{q}$.

\begin{algorithm}
  \KwIn{Poisson rate $\lambda = \alpha t$, tolerance $\tau >
    10^{-50}$}
  \KwOut{truncation parameters $k_{\text{left}}, k_{\text{right}} \in
    \NN$, weights $\vec{w} \in \RR^{k_{\text{right}} -
      k_{\text{left}}}$, scaling constant $W \in \RR$}
  $M_w \gets 30, M_a \gets 44, M_s \gets 21$\;
  $m \gets \lfloor \lambda \rfloor, \textit{tSize} \gets \lfloor
  M_w \sqrt{\lambda} + M_a \rfloor, \textit{tStart} \gets \max
  \{m + M_s - \lfloor \textit{tSize} / 2 \rfloor, 0 \}$\;
  \KwAllocate{$\textbf{tWeights} \in \RR^{\textit{tSize}}$}\;
  $\textit{tWeights}[m - \textit{tStart}] \gets 2^{176}$\;
  \For{$j \gets m - \textit{tStart}$ \KwDownto $1$}{%
    $\textit{tWeights}[j - 1] = (j + \textit{tStart})\,
    \textit{tWeights}[j] / \lambda$
  }
  \For{$j \gets m - \textit{tStart} + 1$ \KwTo $\textit{tSize}$}{%
    $\textit{tWeights}[j + 1] = \lambda\, \textit{tWeights}[j] / (j +
    \textit{tStart})$
  }
  $W \gets 0$\;
  \For{$j \gets 0$ \KwTo $m - \textit{tStart} - 1$}{%
    $W \gets W + \textit{tWeights}[j]$
  }
  $\textit{sum1} \gets 0$
  \tcp*{Avoid adding small numbers to larger numbers}
  \For{$j \gets \textit{tSize} - 1$ \KwDownto $m -
    \textit{tStart}$}{%
    $\textit{sum1} \gets \textit{sum1} + \textit{tWeights}[j]$
  }
  $W \gets W + \textit{sum1}, \textit{threshold} \gets W \tau / 2,
  \textit{cdf} \gets 0, i \gets 0$\;
  \While{$\textit{cdf} < \textit{threshold}$}{%
    $\textit{cdf} \gets \textit{cdf} + \textit{tWeights}[i]$\;
    $i \gets i + 1$
  }
  $k_{\text{left}} \gets \textit{tStart} + i, \textit{cdf} \gets 0, i
  \gets \textit{tSize} - 1$\;
  \While{$\textit{cdf} < \textit{threshold}$}{%
    $\textit{cdf} \gets \textit{cdf} + \textit{tWeights}[i]$\;
    $i \gets i - 1$
  }
  $k_{\text{right}} \gets \textit{tStart} + i$\;
  \KwAllocate{$\vec{w} \in \RR^{k_{\text{right}} -
      k_{\text{left}}}$}\;
  \For{$j \gets k_{\text{left}}$ \KwTo $k_{\text{right}}$}{%
    $w[j - k_{\text{left}}] \gets \textit{tWeights}[j - \textit{tStart}]$
  }
  \KwRet{$k_{\text{left}}, k_{\text{right}}, \vec{w}, W$}\;
  \caption{\citeauthor{DBLP:journals/corr/Burak14}'s algorithm for
    calculating the Poisson weights.}
  \label{alg:algorithms:transient:poisson}
\end{algorithm}

The weights and trimming constants may be calculated by the famous
algorithm of \citet{DBLP:journals/cacm/FoxG88}. However, their
algorithm is extremely complicated due to the limitations of
single-precision floating-point arithmetic%
~\citep{jansen2011understanding}. We implemented
\citeauthor{DBLP:journals/corr/Burak14}'s significantly simpler
algorithm~\citep{DBLP:journals/corr/Burak14} in double precision
instead (\vref{alg:algorithms:transient:poisson}), which avoids
underflow by a scaling factor $W \gg 1$.

\subsection{\textls*[35]{TR-BDF2}}
\label{ssec:algorithms:trbdf2}

A weakness of the uniformization algorithm is the poor tolerance of
\emph{stiff} Markov chains. The \CTMC\ is called stiff if the
$\lvert \lambda_{\min} \rvert \ll \lvert \lambda_{\max} \rvert$, where
$\lambda_{\min}$ and $\lambda_{\max}$ are the nonzero eigenvalues of
the infinitesimal generator matrix $Q$ of minimum and maximum absolute
value~\citep{DBLP:journals/cor/ReibmanT88}. In other words, stiff
Markov chains have behaviors on drastically different timescales, for
example, clients are served frequently while failures happen
infrequently.

Stiffness leads to very large small rates $\alpha$
in line~\ref{ln:algorithms:transient:uniformization:alpha} of
\vref{alg:algorithms:transient:uniformization}, thus a large right
cutoff $k_{\text{right}}$ is required for computing the transient
solution with sufficient accuracy. Moreover, the slow stabilization
results in taking many iterations before steady-state detection
in line~\ref{ln:algorithms:transient:uniformization:steadystate-det}.

Some methods that can handle stiff \textls{CTMCs} efficiently are
stochastic complementation~\citep{meyer1989stochastic}, which
decouples the slow and fast behaviors of the system, and adaptive
uniformization~\citep{van1994adaptive}, which varies the uniformization
rate $\alpha$. Alternatively, an $L$-stable differential equation
solver may be used to solve \vref{eq:background:ctmc:diffeq}, such as
\mbox{\textls{TR-BDF2}}~\citep{DBLP:journals/cor/ReibmanT88,%
DBLP:journals/tcad/BankCFGRS85}.

\textls{TR-BDF2} is an implicit integrator with alternating trapezoid
rule (\textls{TR}) steps
\begin{equation}
  \vec{\uppi}_{k + \gamma} (2I + \gamma h_k Q) = 2 \vec{\uppi}_{k} +
  \gamma h_k \vec{\uppi}_k Q
\end{equation}
and second order backward difference steps
\begin{equation}
  \vec{\uppi}_{k + 1} [(2 - \gamma) I - (1 - \gamma) h_k Q] =
  \frac{1}{\gamma} \vec{\uppi}_{k + \gamma} - \frac{(1 -
    \gamma)^2}{\gamma} \vec{\uppi}_k \text,
\end{equation}
which advance the time together by a step of size $h_k$. The constant
$0 < \gamma < 1$ sets the breakpoint between the two steps. We set it
to $\gamma = 2 - \sqrt{2} \approx 0.59$ following the recommendation
of \citet{DBLP:journals/tcad/BankCFGRS85}.

As a guess for the initial step size $h_0$, we chose the
uniformization rate of $Q$. The $k$th step size $h_k > 0$, including
the $0$th one, is selected such that the local error estimate
\begin{equation}
  \label{eq:algorithms:transient:trbdf-lte}
  \textit{\textls{LTE}}_{k + 1} = \mleft\| 2 \frac{-3\gamma^4 + 4\gamma - 2}{24 -
    12\gamma} h_k \mleft[ -\frac{1}{\gamma} \vec{\uppi}_k +
  \frac{1}{\gamma (1 - \gamma)} \vec{\uppi}_{k + \gamma} - \frac{1}{1
    - \gamma} \vec{\uppi}_{k + 1} \mright] \mright\|
\end{equation}
is bounded by the local error tolerance
\begin{equation}
  \textit{\textls{LTE}}_{k + 1} \le \mleft( \frac{ \tau - \sum_{i = 0}^k
    \textit{\textls{LTE}}_{i} }{ t - \sum_{i = 0}^k k_i } \mright) h_{k + 1}
  \text.
\end{equation}
This Local Error per Unit Step (\textls{LEPUS}) error control
\enquote{produces excellent results for many problems}, but is usually
costly~\citep{DBLP:journals/cor/ReibmanT88}. Moreover, the accumulated
error at the end of integration may be larger than the prescribed
tolerance $\tau$, since \vref{eq:algorithms:transient:trbdf-lte} is
only an approximation of the true error.

\begin{algorithm}
  \KwIn{infinitesimal generator $Q \in \RR^{n \times n}$, initial
    distribution $\vec{\uppi}_0$, mission time $t > 0$, tolerance
    $\tau > 0$}
  \KwOut{transient distribution $\vec{\uppi}(t)$}
  \KwAllocate{$\vec{\uppi}_k, \vec{\uppi}_{k + \gamma}, \vec{\uppi}_{k +
      1}, \vec{d}_k, \vec{d}_{k + 1}, \vec{y} \in \RR^n$}\;
  $\textit{maxIncrease} \gets 10, \textit{leastDecrease} \gets 0.9$\;
  $\textit{timeLeft} \gets t, h \gets 1 / \max_{i} \lvert a[i,i]
  \rvert, \gamma \gets 2 - \sqrt{2}, C \gets \mleft\lvert \frac{-3\gamma^4 +
    4\gamma - 2}{24 - 12\gamma} \mright\rvert, \textit{errorSum} \gets 0$\;
  $\vec{\uppi}_k \gets \vec{\pi}_0$\;
  $\vec{d}_k \gets \vec{\uppi}_k Q$
  \tcp*{Vector-matrix product}
  \While{$\textit{timeLeft} > 0$}{%
    $\textit{stepFailed} \gets \KwFalse, h \gets \min \{h,
    \textit{timeLeft}\}$\;
    \While{$\KwTrue$}{
      \tcc{\textls{TR} step}
      $\vec{y} \gets 2 \cdot \vec{\uppi}_k$
      \tcp*{Vector scaling}
      $\vec{y} \gets \vec{y} + \gamma h \cdot \vec{d}_k$
      \tcp*{In-place vector addition}
      Solve $\vec{\uppi}_{k + \gamma} (2I + -\gamma h Q) = \vec{y}$
      for $\vec{\uppi}_{k + \gamma}$ with initial guess
      $\vec{\uppi}_k$\label{ln:algorithms:transient:trbdf2:solve1}\;
      \tcc{\textls{BDF2} step}
      $\vec{y} \gets -\frac{(1 - \gamma)^2}{\gamma} \cdot
      \vec{\uppi}_k$
      \tcp*{Vector scaling}
      $\vec{y} \gets \frac{1}{\gamma} \cdot \vec{\uppi}_{k + \gamma}$
      \tcp*{In-place scaled vector addition}
      Solve $\vec{\uppi}_{k + 1} ( (2 - \gamma) I + (\gamma - 1) h Q ) =
      \vec{y}$ for $\vec{\uppi}_{k + 1}$ with initial guess
      $\vec{\uppi}_{k + \gamma}$\label{ln:algorithms:transient:trbdf2:solve2}\;
      \tcc{Error control and step size estimation}
      $\vec{y} \gets -\frac{1}{\gamma} \vec{d}_k$
      \tcp*{Vector scaling}
      $\vec{y} \gets \vec{y} + \frac{1}{\gamma (1 - \gamma)}
      \vec{\uppi}_{k + \gamma} Q$
      \tcp*{In-place scaled addition of vector-matrix product}
      $\vec{d}_{k + 1} \gets \vec{\uppi}_{k + 1} Q$
      \tcp*{Vector-matrix product}
      $\vec{y} \gets \vec{y} + \mleft(-\frac{1}{1 - \gamma}\mright)
      \vec{d}_{k + 1}$
      \tcp*{In-place scaled vector addition}
      $\textit{\textls{LTE}} \gets 2 C h \| \vec{y} \|, \textit{localTol} \gets
      (\tau - \textit{errorSum}) / \textit{timeLeft} \cdot h$\;
      \If(\tcp*[f]{Successful step}){$\textit{\textls{LTE}} < \textit{localTol}$}{%
        $\textit{timeLeft} \gets \textit{timeLeft} - h,
        \textit{errorSum} \gets \textit{errorSum} +
        \textit{\textls{LTE}}$\;
        \tcp{Do not try to increase $h$ after a failed step}
        \lIf{$\neg \textit{stepFailed}$}{%
          $h \gets h \cdot \min\{ \textit{maxIncrease},
          \sqrt[3]{\textit{localTol} / \textit{\textls{LTE}}} \}$
        }
        \KwSty{break}\;
      }
      $\textit{stepFailed} \gets \KwTrue, h \gets h \cdot \min\{
      \textit{leastDecrease}, \sqrt[3]{\textit{localTol} /
        \textit{\textls{LTE}}} \}$\;
    }
    Swap the references to $\vec{\uppi}_k, \vec{\uppi}_{k + 1}$ and
    $\vec{d}_k, \vec{d_{k + 1}}$\;
  }
  \KwRet{$\vec{\uppi}_k$}\;
  \caption{\textls{TR-BDF2} for transient analysis.}
  \label{alg:algorithms:transient:trbdf2}
\end{algorithm}

An implementation of \textls{TR-BDF2} based on the pseudocode of
\citet{DBLP:journals/cor/ReibmanT88} is shown in
\vref{alg:algorithms:transient:trbdf2}.

In lines~\ref{ln:algorithms:transient:trbdf2:solve1} and~%
\ref{ln:algorithms:transient:trbdf2:solve2} any linear equation solver
from \vref{sec:algorithms:solvers} may be used except power iteration,
since the matrices, in general, do not have strictly negative
diagonals. Due to the way the matrices, which are linear combinations
of $I$ and $Q$, are passed to the inner solvers, our \textls{TR-BDF2}
integrator is currently limited to $Q$ matrices which are not in block
form.

The vectors $\vec{\uppi}_0, \vec{\uppi}_k$ and
$\vec{\uppi}_{k + \gamma}, \vec{d}_{k + 1}$ may share storage,
respectively, therefore only 4 state-space sized vectors are required
in addition to the initial distribution $\vec{\uppi}_0$.

The most computationally intensive part is the solution of two linear
equation per every attempted step, which may make \textls{TR-BDF2}
extremely slow. However, its performance does \emph{not} depend on the
stiffness of the Markov chain, which may make it better suited to stiff
\textls{CTMCs} than uniformization%
~\citep{DBLP:journals/cor/ReibmanT88}.

\section{Mean time to first failure}

In \textls{MTFF} calculation (\vref{ssec:background:mtff}), quantities
of the forms
\begin{equation}
  \MTFF = - \underbrace{\vec{\uppi}_U Q_{UU}^{-1}}_{\vec{\upgamma}} \vec{1}^\T, \quad
  \Pr(X(\TFF_{+ 0}) = y) = - \underbrace{\vec{\uppi}_U Q_{UU}^{-1}}_{\vec{\upgamma}} \vec{q}_{UD'}^\T
  \tag{\ref{eq:background:mtff}, \ref{eq:background:failure-mode}
    revisited}
\end{equation}
are computed, where $U, D, D'$ are the set of operations states,
failure states and a specific failure mode $D' \subsetneq D$,
respectively.

The vector $\vec{\upgamma} \in \RR^{\lvert U \rvert}$ is the solution
of the linear equation
\begin{equation}
  \label{eq:algorithms:mtff:gamma}
  \vec{\upgamma} Q_{UU} = \vec{\uppi}_{U}
\end{equation}
and may be obtained by any linear equation solver.

The sets $U, D = D_1 \cup D_2 \cup \cdots$ are constructed by the
evaluation of \CTL\ expressions. If the failure mode $D_i$ is
described by $\varphi_i$, then the sets $D$ and $U$ are described by
\CTL\ formulas
$\varphi_D = \neg \ctlAX \KwTrue \lor \varphi_1 \lor \varphi_2 \lor
\cdots$
and $\varphi_U = \neg \varphi_D$, where the deadlock condition
$\neg \ctlAX \KwTrue$ is added to make \eqref{eq:algorithms:mtff:gamma}
irreducible.

After the set $U$ is generated symbolically, the matrix $Q_{UU}$ may be
decomposed in the same way as the whole state space $S$. Thus, the
vector-matrix operations required for solving
\eqref{eq:algorithms:mtff:gamma} can be executed as in steady-state
analysis.

\section{Efficient vector-matrix products}
\label{sec:algorithms:vector-matrix}

\begin{algorithm}
  \KwIn{block vector $\vec{b} \in \RR^{n_0 + n_1 + \cdots + n_{k -
        1}}$,\\block matrix $A \in \RR^{(n_0 + n_1 + \cdots + n_{k -
        1}) \times (m_0 + m_1 + \cdots + m_{l - 1})}$}
  \KwOut{$\vec{c} = \vec{b} A \in \RR^{m_0 + m_1 + \cdots + m_{l -
        1}}$}
  \KwAllocate{$\vec{c}\in \RR^{m_0 + m_1 + \cdots + m_{l - 1}}$}\;
  \ParFor{$j \gets 0$ \KwTo $l - 1$}{
    $\vec{c}[j] \gets \vec{0}$\;
    \For{$i \gets 0$ \KwTo $k - 1$}{
      $\vec{c}[j] \gets \vec{c}[j] + \vec{b}[i] A[i, j]$
      \tcp*{Scaled addition of vector-matrix product}
    }
  }
  \caption{Parallel block vector-matrix product.}
  \label{alg:algorithms:matmul:block}
\end{algorithm}

\begin{algorithm}
  \KwIn{$\vec{b} \in \RR^n$, $A = \nu_0 A_0 + \nu_1 A_1 + \cdots +
    \nu_{k - 1} A_{k - 1}$, where $A_h \in \RR^{n \times m}$}
  \KwOut{$\vec{c} = \vec{b} A \in \RR^m$}
  \KwAllocate{$\vec{c} \in \RR^m$ if no target buffer is provided}\;
  $\vec{c} \gets \vec{0}$\;
  \For{$h \gets 0$ \KwTo $k - 1$}{
    $\vec{c} \gets \nu_h \cdot \vec{b} A_h$
    \tcp*{In-place scaled addition of vector-matrix product}
  }
  \KwRet{$\vec{c}$}\;
  \caption{Product of a vector with a linear combination matrix.}
  \label{alg:algorithms:matmul:lincomb}
\end{algorithm}

\begin{algorithm}
  \KwIn{$\vec{b} \in \RR^{n_0 n_1 \cdots n_{k - 1}}$, $A = \loc{A}{0}
    \krtimes \loc{A}{1} \krtimes \cdots \krtimes \loc{A}{k - 1}$,
    where $\loc{A}{h} \in \RR^{n_h \times m_h}$}
  \KwOut{$\vec{c} = \vec{b} A \in \RR^{m_0 m_1 \cdots m_{k - 1}}$}
  $n \gets n_0 n_1 \cdots n_{k - 1},\quad m \gets m_0 m_1 \cdots m_{k - 1}$\;
  $\textit{tempLength} \gets \max_{h = -1, 0, 1, \ldots, k - 1} \prod_{f =
    0}^h m_f \prod_{f = h + 1}^{k - 1} n_f$\;
  \KwAllocate{$\vec{x}, \vec{x}'$ with at least $\textit{tempLength}$
    elements}\;
  $\vec{x}[0{:}1{:}n] \gets \vec{b},\quad
  i_{\text{left}} \gets 1,\quad
  i_{\text{right}} \gets \prod_{h = 1}^{k - 1} n_h$\;
  \For{$h \gets 0$ \KwTo $k - 1$}{
    \If{$\loc{A}{h}$ is not an identity matrix}{
      $i_{\text{base}} \gets 0, j_{\text{base}} \gets 0$\;
      \For{$\textit{il} \gets 0$ \KwTo $i_{\text{left}} - 1$}{
        \label{ln:algorithms:matmul:shuffle:innerloop}
        \For{$\textit{ir} \gets 0$ \KwTo $i_{\text{right}} - 1$}{
          $\vec{x}'[j_{\text{base}}{:}m_h{:}i_{\text{right}}] \gets
          \vec{x}[i_{\text{base}}{:}n_h{:}i_{\text{right}}] \loc{A}{h}$\;
          $i_{\text{base}} \gets i_{\text{base}} + n_h
          i_{\text{right}},\quad
          j_{\text{base}} \gets j_{\text{base}} + m_h
          i_{\text{right}}$\;
        }
      }
      Swap the references to $\vec{x}$ and $\vec{x}'$\;
    }
    $i_{\text{left}} \gets i_{\text{left}} \cdot m_h$\;
    \lIf{$h \ne k - 1$}
    {$i_{\text{right}} \gets i_{\text{right}} / n_{h + 1}$}
  }
  \KwRet{$\vec{c} = \vec{x}[0{:}1{:}m]$}\;
  \caption{The \textsc{Shuffle} algorithm for vector-matrix
    multiplication.}
  \label{alg:algorithms:matmul:shuffle}
\end{algorithm}

Iterative linear equation and transient distribution solvers require
several vector-matrix products per iteration. Therefore, efficient
vector-matrix multiplication algorithms are required for the various
matrix storage methods (i.e.~dense, sparse and block Kronecker
matrices) to support configurable stochastic analysis.

Our data structure supports run-time reconfiguration of operations,
for example, to switch between parallel and sequential matrix
multiplication implementations for different parts of an
algorithm, depending on the characteristics of the model and the
hardware which runs the analysis.

Implemented matrix multiplication for the data structure
(see~\vref{fig:genstor:kronecker:datastructure}) routines are
\begin{itemize}
\item Multiplication of vectors with dense and sparse matrices. Sparse
  matrix multiplication may be parallelized by splitting the columns
  of the matrix into chunck and submitting each chunk to the executor
  thread pool.

  Operations with vectors and sparse matrices are implemented in an
  \texttt{unsafe}%
  \footnote{\url{https://msdn.microsoft.com/en-us/library/chfa2zb8.aspx}}
  context. The elements of the data structures are not under the
  influence of the Garbage Collector runtime, but stored in natively
  allocated memory. This allows the handling of large matrices without
  adversely impacting the performance of other parts of the program,
  albeit the cost of allocations in increased.
\item Multiplication with block matrices by delegation to the
  constituent blocks of the matrix%
  ~(\vref{alg:algorithms:matmul:block}). The input and output vectors
  are converted to block vectors before multiplication. If parallel
  execution is required, each block of the output vector can be
  computed in a different task, since it is independent from the
  others.
\item Multiplication by a linear combination of matrices is delegated
  to the constituent matrices
  (\vref{alg:algorithms:matmul:lincomb}). An in-place scaled addition
  of vector-matrix product to a vector operation is required for this
  delegation. To facilitate this, each vector-matrix multiplication
  algorithm is implemented also as an in-place addition and in-place
  scaled addition of vector-matrix product, and the appropriate
  implementation is selected based on the function call aruments.
\item Multiplications $\vec{b} \cdot \diag\{\vec{a}\}$ by diagonal
  matrices are executed as elementwise product
  $\vec{b} \eltimes \vec{a}$. The special case of multiplication by an
  identity matrix is equivalent to a vector copy.
\item Multiplications by Kronecker products is performed by the
  \textsc{Shuffle} algorithm~%
  \citep{DBLP:journals/informs/BuchholzCDK00,benoit2001memory} as
  shown in \vref{alg:algorithms:matmul:shuffle}.

  The algorithm requires access to slices of a vector, denoted as
  $\vec{x}[i_0{:}s{:}l]$, which refers to the elements
  $x[i], x[i + s], x[i + 2s], \ldots, x[i + (l - 1)s]$. Thus, slices
  were integrated into the operations framework as first-class
  elements, and multiplication algorithms are implemented with support
  for vector slice indexing.

  \textsc{Shuffle} rewrites the Kronecker products as
  \begin{equation}
    \bigkrtimes_{h = 0}^{k - 1} \loc{A}{h} = \prod_{h = 0}^{k - 1}
    I_{\prod_{f = 0}^{h - 1} n_f \times \prod_{f = 0}^{h - 1} n_f} \krtimes
    \loc{A}{h} \krtimes I_{\prod_{f = h + 1}^{k - 1} m_f \times
      \prod_{f = h + 1}^{k - 1} m_f} \text,
  \end{equation}
  where $I_{a \times a}$ denotes an $a \times a$ identity matrix.
  Multiplications by terms of the form
  $I_{N \times N} \otimes \loc{A}{h} \otimes I_{M \times M}$ are
  carried out in the loop at line%
  ~\ref{ln:algorithms:matmul:shuffle:innerloop} of
  \cref{alg:algorithms:matmul:shuffle}.

  The temporary vectors $\vec{x}, \vec{x}'$ are large enough store the
  results of the successive matrix multiplications. They are cached
  for every worker thread to avoid repeated allocations.

  Other algorithms for vector-Kronecker product multiplication are the
  \textsc{Slice}~\citep{fernandes2005alternative} and \textsc{Split}~%
  \citep{DBLP:conf/springsim/CzeksterRFLW10} algorithms, which are
  more amenable to parallel execution than \textsc{Shuffle}. Their
  implementation is in the scope of our future work.
\end{itemize}
