\chapter{Overview of the approach}
\label{chap:overview}

\section{General workflow}

\begin{figure}
  \centering
  \begin{tikzpicture}[
    start chain,
    every node/.style={
      on chain,join,
      text width=2.1cm,minimum height=1.2cm,align=center,
      draw,tdk highlight,rounded corners=0.25cm},
    node distance=1cm,
    every join/.style={-{Latex},draw}]

    \node {State space exploration};
    \node {Descriptor generation};
    \node {Numerical solution};
    \node {Reward calculation};
  \end{tikzpicture}
  \caption{The general stochastic analysis workflow.}
  \label{fig:overview:general:workflow}
\end{figure}

The tasks performed by stochastic analysis tools that operate on
higher level formalisms can be often structured as follows
(\vref{fig:overview:general:workflow}):
\begin{enumerate}
\item \emph{State space exploration.} The reachable state space of the
  higher level model, for example stochastic automata network or
  stochastic Petri net is explored to enumerate the possible behaviors
  of the model $S$. If the model is partitioned into a hierarchy of
  components, this step includes the exploration of the local state
  spaces of the component as well as the possible global combinations
  of states.

  If the set of reachable states is infinite, only special algorithms,
  e.g.~matrix geometric methods~\citep{haverkort1995matrix} may be
  employed later in the workflow. In this work, we restrict our
  attention to finite cases.
\item \emph{Descriptor generation.} The infinitesimal generator matrix
  $Q$ of the Markov chain $X(t)$ defined over $S$ is created. If the
  analyzed formalism is a Markov chain, $Q$ is readily
  given. Otherwise, this matrix contains the transition rates between
  reachable states, which are obtained by evaluating rate expressions
  given in the model.
\item \emph{Numerical solution.} Numerical algorithms are ran on the
  matrix $Q$ for steady-state solutions $\vec{\uppi}$, transient
  solutions $\vec{\uppi}(t)$, $\vec{L}(t)$ or \textls{MTFF} measures.
\item \emph{Reward calculations.} The studied performance measures are
  calculated from the output of the previous step. This includes
  calculation of steady-state and transient rewards and sensitivities
  of the rewards. Additional algebraic manipulations (for example, the
  calculation of the ratio of an instantenous and accumulated reward)
  may be provided to the modeler as convenience.

\end{enumerate}

In stochastic model checking, where the desired system behaviors are
expressed in stochastic temporal logics%
~\citep{bianco1995model,baier1999approximative}, these analytic steps
are called as subrouties to evaluate propositions. In the sythesis and
optimization of stochastic models%
~\citep{DBLP:journals/jacm/ChatterjeeHJ015}, the workflow is executed
as part of the fitness functions.

\subsection{Challenges}

The implementation of the stochastic analysis workflow poses several
challenges.

Handling of large models is difficult due to the phenomenon of
\enquote{state space explosion}. As the size of the model grows,
including the number of components, the number of reachable spaces can
grow exponentially.

Methods such as the \emph{saturation} algorithm~\citep{Ciardo:2006}
were developed to efficiently explore and represent large state
spaces. However, in stochastic analysis, the generator matrix $Q$ and
several vectors of real numbers with lengths equal to the state space
size must be stored in addition to the state space. This neccessitates
the use of further decomposition techniques for data storage.

The convergence of the numerical methods depends on the structure of
the model and the matrix decomposition in use. In addition, the memory
requirements of the algorithms may constrain the methods that can be
employed. As several numerical algorithms for stochastic analysis
tasks are known with different characteristics, it is important to
allow the modeler to select the algorithm suitable for the properties
of the model, as well as the decomposition method and executing
hardware in use.

The vector operations and vector-matrix products that are preformed by
the numerical algorithms can also be executed in multiple ways. For
example, multiplications with matrices can be implemented either
sequentially or on multiple threads. Large matrices benefit from
parallelization, while for small matrices, the overhead of managing
multiple tasks is usually excessive. Distributed or \textls{GPU}
impelemtations are also possible, albeit they are missing from the
current version of our framework.

\section{Our workflow}

\begin{figure}
  \centering
  \begin{tikzpicture}[
    data/.style={
      text width=2.1cm,minimum height=1.2cm,align=center,
      draw,tdk highlight
    },
    activity/.style={rounded corners=0.25cm}
    ]
    \matrix [every node/.append style={data},column sep=1cm,row sep=0.65cm] {
      \node (model) {Stochastic Petri Net};
      & \node [activity] (sse) {State space exploration};
      & \node [activity] (gen) {Generator construction};
      & \node (ds) {Data structure}; \\
      & \node (sspn) {\textls{SSPN} partition};
      & & \node [activity] (numeric) {Numerical algorithms}; \\
      & & \coordinate (calc-input);
      & \node [activity] (calc) {Reward calculation}; \\
    };
    \node [yshift=0.925cm,data] (reward) at (calc-input)
    {Reward config.};
    \node [yshift=-0.925cm,data] (sensitivity) at (calc-input)
    {Sensitivity parameters};

    \draw [-{Latex}] (model) edge (sse) (sse) edge (gen)
    (sspn) edge [dashed] (sse) (gen) edge (ds)
    (ds) edge [transform canvas={xshift=0.325cm}] (numeric)
    (numeric) edge [transform canvas={xshift=-0.325cm}] (ds)
    (numeric) edge (calc) (reward) edge (calc)
    (sensitivity) edge (calc);
    \draw ($(ds.south)+(-0.7cm,-0.325cm)$) edge
    ($(ds.south)+(0.7cm,-0.325cm)$) node [left]
    {Configurable operations};
  \end{tikzpicture}
  \caption{Configurable stochastic analysis workflow.}
  \label{fig:overview:our:workflow}
\end{figure}

Our implementation of the general stochastic analysis workflow is
illustrated in \cref{fig:overview:our:workflow}.

The workflow is fully \emph{configurable}, meaning components can be
freely selected by the modeler to suit the application.

\begin{table}
  \caption{Linear equation solvers supported by our framework.}
  \centering
  \begin{tabular}{@{}llcccc@{}}
    \toprule
    & & memory & parallel & uses inner & block \\[-0.5ex]
    & \multicolumn{1}{c}{see} & usage & impl. & solver & matrix \\
    \midrule
    \textls{LU} decomposition & p.~\pageref{ssec:algorithms:lu} & very high & -- & -- & -- \\
    Power method & p.~\pageref{ssec:algorithms:power} & moderate & $\checkmark$ & -- & $\checkmark$ \\
    Jacobi over-relaxation & p.~\pageref{ssec:algorithms:jgs} & moderate & $\checkmark$ & -- & $\checkmark$ \\
    Gauss--Seidel over-relaxation & p.~\pageref{ssec:algorithms:jgs} & very low & -- & -- & $\checkmark$ \\
    \textls{BiCGSTAB} & p.~\pageref{ssec:algorithms:bicgstab} & high & $\checkmark$ & -- & $\checkmark$ \\
    Group Jacobi & p.~\pageref{ssec:algorithms:group-jgs} & moderate & $\checkmark$ & $\checkmark$ & required \\
    Group Gauss--Seidel & p.~\pageref{ssec:algorithms:group-jgs} & low & -- & $\checkmark$ & required \\
    \bottomrule
  \end{tabular}
  \label{tab:overview:our:linear}
\end{table}

\begin{table}
  \caption{Transient solvers supported by our framework.}
  \centering
  \begin{tabular}{@{}llcccc@{}}
    \toprule
    & & instantenous & accumulated & uses inner & block \\[-0.5ex]
    & \multicolumn{1}{c}{see} & distribution & distribution & solver & matrix \\
    \midrule
    Uniformization & p.~\pageref{ssec:algorithms:uniformization} & $\checkmark$ & $\checkmark$ & -- & $\checkmark$ \\
    \textls{TR-BDF2} & p.~\pageref{ssec:algorithms:trbdf2} & $\checkmark$ & not impl. & $\checkmark$ & not impl. \\
    \bottomrule
  \end{tabular}
  \label{tab:overview:our:transient}
\end{table}

\begin{itemize}
\item The model state space may be explored either by an explicitly
  enumeration of every reachable space, or by symbolic
  saturation~\citep{Ciardo:2006}. As symbolic methods are usually much
  faster and use significantly less memory than explicit enumeration,
  they are the recommended approach for stochastic analysis. However,
  the explicit algorithms are provided for benchmarking and software
  redundancy reason.

  For state space exploration algorithms operating on a superposed
  \textls{SPN}, the component partitions may be provided by the user
  as part of the model or generated on the fly.
\item The generator matrix may be stored in sparse matrix or block
  Kronecker decomposition form%
  ~\citep{DBLP:journals/sigmetrics/BuchholzK98}. The matrix can be
  build from both explicitly or symbolically stored state spaces.

  To facilitate block Kronecker matrix generation, we propose a purely
  symbolic algorithm, which avoids any overheads of explicit state
  space operations.
\item The resulting matrices, in possibly decomposed form, are part of
  specialized data structure. Extremely large matrices may be stored
  as results of linear algebraic operations (e.g.~linear combinations,
  Kronecker products, contatenations into block structures). The data
  structure defines generic vector and matrics operations, as well as
  more specific manipulations performed by stochastic analysis
  algorithms.

  State space exploration and generator matrix decomposition methods
  are presented in \cref{chap:genstor}, including our algorithmic
  contribution for block Kronecker decomposition.
\item The execution of the operations of the data structure can be
  modified at runtime. This allows the use of different
  implementations at the different stages of the workflow, or when
  different algorithms are employed to calculate multiple performance
  measures. Whenever possible, both sequential and parallel
  implementations of the most common operations are available for the
  supported datatypes.
\item Several numerical algorithms are provided for steady-state and
  transient analysis of Markov chains. The user can select the
  algorithm most suitable for the model under study. For solvers that
  called nother as a subroutine, any other solver can be passed as an
  inner algorithm.

  Important considerations in selecting the solver are its convergence
  behavior and its memory requirements. While matrix decompositions
  can reduce the storage space occupied by the $Q$ matrix by orders of
  magnitudes, we store all elements of probability vectors
  explicitly. Therefore, one should pay close attention to the number
  of temporary vectors used in the algorithm in order to avoid
  excessive memory consumption.

  Numerical algorithms supported by our framework are discussed in
  \cref{chap:algorithms}. Linear equations solvers for steady-state
  \CTMC\ analysis are shown in \cref{tab:overview:our:linear}, while
  linear solver are shown in \cref{tab:overview:our:transient}.
\end{itemize}

\subsection{Supported formalisms}

Our stochastic analysis framework targets models in the Stochastic
Petri Net with inhibitor arcs formalism (see
\vref{dfn:background:spn}). Structured models are handled as
Superposed Stochastic Petri Nets (see
\vref{dfn:background:sspn}). However, any modeling formalism can be
processed by integrating the appropriate state space exploration
algorithms with the workflow.

Transition rates in the \textls{SPNs} can be arbitrary algebraic
expressions containing references to \emph{sensitivity
  variables}. These variables correspond to the parameter vector
$\vec{\theta}$ in Markov chain sensitivity analysis. However, the rate
expression may not depend on the marking of the net.

Rewards structures are defined as Stochastic Reward Nets (see
\vref{dfn:background:srn}). An \textls{SRN} reward structure may be
specified by composing any \emph{reward expressions} of the forms
\begin{enumerate}
\item $(p, w)$, where $p \in P$ is a place and $w$ is a \emph{reward
    weight expression}. This reward expression is equivalent to
  a rate reward $\rateReward(M) = M(p) \cdot w$, i.e.~the value of $w$
  is multiplied by the numer of tokens on $p$.
\item $(t, w)$, where $t \in T$ is a transition and $w$ is a reward
  weight expression. This is equivalent to an impulse reward
  $\impulseReward(t, M) = w$ gained upon the firing of $t$.
\item $\varphi \rightarrow w$, where $\varphi$ is a Computational Tree
  Logic (\textls{CTL}) expression and $w$ is a reward weight
  expression. This is equivalent to the rate reward $\rateReward(M) =
  w$ if $\varphi$ holds in $M$, $0$ otherwise.
\end{enumerate}

A reward weight expression is an algebraic expression that may refer
to places an transition rates in the net. Refrences to places are
replaced with the number of tokens upon evaluation. For example, the
reward expression $(p, w)$ may be written as
$\KwTrue \rightarrow p \cdot w$ or $p > 0 \rightarrow p \cdot w$ using
\textls{CTL}.

Reward expressions with \textls{CTL} are only allowed when symbolic
state spaces representation is in use, as \textls{CTL} evaluation%
\footnote{The symbolic state space exploration and \textls{CTL}
  evluation component is currently provided by the
  \textsc{Petridotnet}~\citep{Petridotnet} tool.} is performed
symbolically~\citep{TDK2010_Darvas}.

\begin{runningExample}
  Consider the reward structures defined over the
  \emph{Shared\-Resource} Petri net from \vref{ex:background:srn}.

  The utilization of the shared resource can be described by the
  reward expressions
  \begin{equation}
    \textit{resourceUtilization} = \{ (p_{S_1}, 1), (p_{S_2}, 1)\} \text,
  \end{equation}
  which are equivalent to the \textls{SRN} reward structure
  \begin{equation}
    \rateReward_1(M) = M(p_{S_1}) + M(p_{S_2}), \quad
    \impulseReward_1(t, M) \equiv 0 \text.
    \tag{\ref{eq:background:srn:example-1} revisited}
  \end{equation}
  This can be also written as
  \begin{equation}
    \textit{resourceUtilization} = \{p_{S_1} > 0 \lor p_{S_2} > 0
    \rightarrow 1\}
  \end{equation}
  using \textls{CTL}, because the places $S_1$ and $S_2$ are
  $1$-bounded in the \emph{Shared\-Resource} model.

  Completed calculations are described by
  \begin{equation}
    \textit{completedCalculations} = \{ (t_{s_1}, 1), (t_{r_2}, 1)\} \text,
  \end{equation}
  which is equivalent to the reward structure
  \begin{equation}
    \rateReward_2(M) \equiv 0, \quad \impulseReward_2(t, M) = \begin{cases}
      1 & \text{if $t \in \{t_{r_1}, t_{r_2}\}$,} \\
      0 & \text{otherwise.}
    \end{cases}
    \tag{\ref{eq:background:srn:example-2} revisited}
  \end{equation}
\end{runningExample}

\subsection{Supported analysis types}

Our framework supports calcuations of
\begin{itemize*}
\item expected steady-state reward rates $\Ex R$ for any reward
  structure defined by reward expressions,
\item expected transient reward rates $\Ex R(t)$ and accumulated
  rewards $\Ex Y(t)$,
\item \emph{complex rewards}, which are algebraic expressions of mean
  reward rates and accumulated rewards
  (e.g~$1 + \Ex R(t) / \Ex Y(t)$),
\item sensitivity of mean steady-state reward rates and complex
  rewards involving steady-state rates,
\item mean-time to failure $\MTFF$ and associated failure mode
  probabilities.
\end{itemize*}
A selection fo multiple algorithms is available for each analysis
type.

\subsection{Reward and sensitivity calculation}

Transition and reward rates are stored as algebraic expression trees
in the input \textls{SPN} models. Symbolic operations, such as partial
differentiation may be performed exactly on the trees using algebraic
laws, as the evaluation of the expressions can be delayed.

In reward and \textls{MTFF} calculations, rate expressions are
evaluated by replacing sensitivity parameters with their values before
the matrix $Q$ is composed. Thus, the elements of matrix are not
expression trees, but floating point numbers and matrix generation
has to be performed only when sensitivity parameters are changed.

Reward weight expressions may refer to the token counts on places,
therefore they must be evaluated for every marking individually. If a
\textls{CTL} reward expression $\varphi \rightarrow w$ is used,
evaluation is skipped in markings where $\varphi$ is false.

\begin{figure}
  \centering
  \begin{tikzpicture}[
    data/.style={
      text width=2.1cm,minimum height=1.2cm,align=center,
      draw,tdk highlight
    },
    activity/.style={data,rounded corners=0.25cm},
    node distance=0pt,inner sep=0pt
    ]
    \matrix [every node/.append style={activity},column sep=1cm,
    row sep=0.65cm] (m) {
      & \node (partial) {$\displaystyle \frac{\partial}{\partial
          \theta [i]}$}; \\
      \node (genq) {$Q$ matrix generation};
      & \node (genv) {$V_i$ matrix generation}; \\
      \node (solveq) {Solve for $\vec{p}$};
      & \node (solvev) {Solve for $\vec{s}_i$}; \\
      \node (preward) {$\displaystyle \vec{p}
        \frac{\partial \vec{r}^\T}{\partial \theta[i]}$};
      & \node (sreward) {Calculate $\vec{s}_i \vec{r}^\T$}; \\
    };
    \node [above=0.65cm of m,data] (rates) {Rate expressions};
    \node [below=0.65cm of m,activity] (sum) {$\displaystyle \sum$};
    \node [below=0.65cm of sum,data] (result) {Sensitivity value};
    \node [left=0.65cm of genq,data,yshift=0.925cm] (param)
    {Parameter bindings};
    \node [left=0.65cm of preward,data,yshift=0.925cm] (reward)
    {Reward expressions};

    \begin{scope}[every path/.append style={-{Latex}}]
      \draw (rates.south) -- ++(0,-0.325cm) -| (genq);
      \draw (rates.south) ++(0,-0.325cm) -| (partial);
      \draw (partial) edge (genv) (genv) edge (solvev)
      (solvev) edge (sreward) (genq) edge (solveq)
      (solveq) edge (preward) (sum) edge (result);
      \draw (preward.south) -- ++(0,-0.325cm) -| (sum);
      \draw [-] (sreward) |- ($(sum.north)+(0,0.325cm)$);
      \draw (param) -| ($(genv.north)+(-0.5cm,0)$);
      \draw ($(genq.north)+(-0.5cm,0.325cm)$) --
      ($(genq.north)+(-0.5cm,0)$);
      \draw (reward) -| ($(sreward.north)+(-0.5cm,0)$);
      \draw ($(preward.north)+(-0.5cm,0.325cm)$) --
      ($(preward.north)+(-0.5cm,0)$);
      \draw [dashed] (param) -- (reward);
    \end{scope}
  \end{tikzpicture}
  \caption{Reward and sensitivity calculation from expression tree
    inputs.}
  \label{fig:overview:sensitivity}
\end{figure}

Steady-state sensitivity calculation, shown in
\cref{fig:overview:sensitivity}, is the most complicated
post-processing in the workflow. Partial derivatives of the transition
rate expressions and reward weight expressions are taken to calculate
$\partial \Ex R / \partial \theta[i]$ using
\cref{eq:background:ctmc:sensitvity:s,%
eq:background:ctmc:sensitivity:productrule}.