\chapter{Overview of the approach}
\label{chap:overview}

\section{General workflow}

\begin{figure}
  \centering
  \begin{tikzpicture}[
    start chain,
    every node/.style={
      on chain,join,
      text width=2.1cm,minimum height=1.2cm,align=center,
      draw,tdk highlight,rounded corners=0.25cm},
    node distance=1cm,
    every join/.style={-{Latex},draw}]

    \node {State space exploration};
    \node {Descriptor generation};
    \node {Numerical solution};
    \node {Reward calculation};
  \end{tikzpicture}
  \caption{The general stochastic analysis workflow.}
  \label{fig:overview:general:workflow}
\end{figure}

The tasks performed by stochastic analysis tools that operate on
higher level formalisms can be often structured as follows
(\vref{fig:overview:general:workflow}):
\begin{enumerate}
\item \emph{State space exploration.} The reachable state space of the
  higher level model, for example stochastic automata network or
  stochastic Petri net is explored to enumerate the possible behaviors of the model $S$. If the model is hierarchically partitioned, this step includes the exploration of the local state
  spaces of the component as well as the possible global combinations
  of states.

  If the set of reachable states is infinite, only special algorithms,
  e.g.~matrix geometric methods~\citep{haverkort1995matrix} may be
  employed later in the workflow. In this work, we restrict our
  attention to finite cases.
\item \emph{Descriptor generation.} The infinitesimal generator matrix
  $Q$ of the Markov chain $X(t)$ defined over $S$ is built. If the
  analyzed formalism is a Markov chain, $Q$ is readily
  given. Otherwise, this matrix contains the transition rates between
  reachable states, which are obtained by evaluating rate expressions
  given in the model.
\item \emph{Numerical solution.} Numerical algorithms are ran on the
  matrix $Q$ for steady-state solutions $\vec{\uppi}$, transient
  solutions $\vec{\uppi}(t)$, $\vec{L}(t)$ or \textls{MTFF} measures.
\item \emph{Reward calculations.} The studied performance measures are
  calculated from the output of the previous step. This includes
  calculation of steady-state and transient rewards and sensitivities
  of the rewards. Additional algebraic manipulations (for example, the
  calculation of the ratio of an instantenous and accumulated reward)
  may be provided to the modeler for convenience.

\end{enumerate}

In stochastic model checking, where the desired system behaviors are
expressed in stochastic temporal logics%
~\citep{bianco1995model,baier1999approximative}, these analytic steps
are called as subrouties to evaluate propositions. In the sythesis and
optimization of stochastic models%
~\citep{DBLP:journals/jacm/ChatterjeeHJ015}, the workflow is executed
as part of the fitness functions.

\subsection{Challenges}

The implementation of the stochastic analysis workflow poses several
challenges.

Handling of large models is difficult due to the phenomenon of
\enquote{state space explosion}. As the size of the model grows,
including the number of components, the number of reachable spaces can
grow exponentially.

Methods such as the \emph{saturation} algorithm~\citep{Ciardo:2006}
were developed to efficiently explore and represent large state
spaces. However, in stochastic analysis, the generator matrix $Q$ and
several vectors of real numbers with lengths equal to the state space
size must be stored in addition to the state space. This neccessitates
the use of further decomposition techniques for data storage.

The convergence of the numerical methods depends on the structure of
the model and the applied matrix decomposition. In addition, the memory
requirements of the algorithms may constrain the methods that can be
employed. As various numerical algorithms for stochastic analysis
tasks are known with different characteristics, it is important to
allow the modeler to select the algorithm suitable for the properties
of the model, as well as the decomposition method and hardware environment.

The vector operations and vector-matrix products that are preformed by
the numerical algorithms can also be performed in multiple ways. For
example, multiplications with matrices can be implemented either
sequentially or in parallel. Large matrices benefit from
parallelization, while for small matrices managing
multiple tasks yields overhead. Distributed or \textls{GPU}
impelemtations are also possible, albeit they are missing from the
current version of our framework.

\section{Our workflow}
  \label{chap:overview:sec:our-workflow}

\begin{figure}
  \centering
  \begin{tikzpicture}[
    data/.style={
      text width=2.1cm,minimum height=1.2cm,align=center,
      draw,tdk highlight
    },
    activity/.style={rounded corners=0.25cm}
    ]
    \matrix [every node/.append style={data},column sep=1cm,row sep=0.65cm] {
      \node (model) {Stochastic Petri Net};
      & \node [activity] (sse) {State space exploration};
      & \node [activity] (gen) {Generator construction};
      & \node (ds) {Data structure}; \\
      & \node (sspn) {\textls{SSPN} partition};
      & & \node [activity] (numeric) {Numerical algorithms}; \\
      & & \coordinate (calc-input);
      & \node [activity] (calc) {Reward calculation}; \\
    };
    \node [yshift=0.925cm,data] (reward) at (calc-input)
    {Reward config.};
    \node [yshift=-0.925cm,data] (sensitivity) at (calc-input)
    {Sensitivity parameters};

    \draw [-{Latex}] (model) edge (sse) (sse) edge (gen)
    (sspn) edge [dashed] (sse) (gen) edge (ds)
    (ds) edge [transform canvas={xshift=0.325cm}] (numeric)
    (numeric) edge [transform canvas={xshift=-0.325cm}] (ds)
    (numeric) edge (calc) (reward) edge (calc)
    (sensitivity) edge (calc);

    \draw ($(ds.north west)+(-0.325cm,0.325cm)$)
    rectangle ($(numeric.south east)+(0.325cm,-0.325cm)$);
    \node [above=0.4cm of ds] {Configurable operations};

    %\draw ($(ds.south)+(-0.7cm,-0.325cm)$) edge
    %($(ds.south)+(0.7cm,-0.325cm)$) node [left]
    %{Configurable operations};
  \end{tikzpicture}
  \caption{Configurable stochastic analysis workflow.}
  \label{fig:overview:our:workflow}
\end{figure}

\begin{figure}
  \centering
  \begin{tikzpicture}[
    architecture layer/.style={
      tdk highlight,draw,
      minimum width=0.98\linewidth,
      minimum height=2.5cm
    },
    layer label/.style={
      anchor=north west,font=\strut
    },
    component/.style={
      text width=2.1cm,minimum height=1.2cm,align=center,
      draw,tdk highlight
    },
    small component/.style={
      component,text width=1.5cm, minimum height=0.9cm,
      font=\footnotesize
    },
    components/.style={
      column sep=0.65cm,
      yshift=-4ex,
      anchor=north
    },
    two components/.style={
      components,
      column sep=2cm
    },
    three components/.style={
      components,
      column sep=1.5cm
    },
    small components/.style={
      components,
      column sep=0.325cm,
      row sep=0.325cm
    }
    ]
    \node [architecture layer] (l-state-exploration) {};
    \node [layer label] at (l-state-exploration.north west)
    {State space exploration};
    \node [architecture layer,below=of l-state-exploration] (l-state-storage) {};
    \node [layer label] at (l-state-storage.north west)
    {State space storage};
    \node [architecture layer,below=of l-state-storage] (l-matrix-decomposition) {};
    \node [layer label] at (l-matrix-decomposition.north west)
    {Matrix representation};
    \node [architecture layer,below=of l-matrix-decomposition,
    rectangle split,rectangle split horizontal,rectangle split
    parts=2,text width=0.49\linewidth,inner sep=0pt,minimum height=4.75cm] (l-numerical)
    {\nodepart{one}\hfil\nodepart{two}\hfil};
    \node [layer label] at (l-numerical.north west)
    {Steady-state solution algorithms};
    \node [layer label] at (l-numerical.north)
    {Transient solution algorithms};
    \node [architecture layer,below=of l-numerical] (l-reward) {};
    \node [layer label] at (l-reward.north west)
    {Performance measure calculation};

    \matrix [two components] at (l-state-exploration.north) {
      \node [component] (se-explicit) {Explicit};
      & \node [component] (se-symbolic) {Symbolic}; \\
    };
    \matrix [two components] at (l-state-storage.north) {
      \node [component] (ss-explicit) {Explicit};
      & \node [component] (ss-symbolic) {Decision diagram}; \\
    };
    \matrix [three components] at (l-matrix-decomposition.north) {
      \node [component] (md-dense) {Dense matrix};
      & \node [component] (md-sparse) {Sparse matrix};
      & \node [component] (md-kronecker) {Block Kronecker}; \\
    };
    \matrix [small components] at (l-numerical.one north) {
      \node [small component] (st-lu) {\textls{LU} decomposition};
      & \node [small component] (st-power) {Power method};
      & \node [small component] (st-jacobi) {Jacobi}; \\
      \node [small component] (st-gs) {Gauss--Seidel};
      & \node [small component] (st-groupjacobi) {Group Jacobi};
      & \node [small component] (st-groupgs) {Group G--S}; \\
      & \node [small component] (st-bicgstab) {\textls{BiCGSTAB}}; & \\
    };
    \matrix [components] at (l-numerical.two north) {
      \node [component] (tr-unif) {Uni\-for\-mi\-za\-tion};
      & \node [component] (tr-trbdf2) {\textls{TR-BDF2}}; \\
    };
    \matrix [three components] at (l-reward.north) {
      \node [component] (r-reward) {Reward};
      & \node [component] (r-sensitivty) {Sensitivity};
      & \node [component] (r-mtff) {\textls{MTFF}}; \\
    };

    \draw [{Latex}-{Latex}] (se-explicit) edge (ss-explicit)
    (se-symbolic) edge (ss-symbolic);

    \newcommand*{\commArrows}[2]{
      \draw [-{Latex},transform canvas={xshift=-1cm}] (#1) -- (#2);
      \draw [-{Latex},transform canvas={xshift=1cm}] (#2) -- (#1);
    }
    \commArrows{l-state-storage}{l-matrix-decomposition}
    \commArrows{l-matrix-decomposition}{l-numerical}
    \commArrows{l-numerical}{l-reward}
  \end{tikzpicture}
  \caption{Architecture of the configurable stochastic analysis
    framework.}
  \label{fig:overview:our:architecture}
\end{figure}

Our implementation of the general stochastic analysis workflow is
illustrated in \cref{fig:overview:our:workflow}.

The workflow is fully \emph{configurable}, which means that the
modeler may combine the available algorithms for the analysis steps
arbitrarily. This is achieved by a layered architecture as shown in
\cref{fig:overview:our:architecture}.

\begin{table}
  \caption{Linear equation solvers supported by our framework.}
  \centering
  \begin{tabular}{@{}llcccc@{}}
    \toprule
    & & memory & parallel & uses inner & block \\[-0.5ex]
    & \multicolumn{1}{c}{see} & usage & impl. & solver & matrix \\
    \midrule
    \textls{LU} decomposition & p.~\pageref{ssec:algorithms:lu} & very high & -- & -- & -- \\
    Power method & p.~\pageref{ssec:algorithms:power} & moderate & $\checkmark$ & -- & $\checkmark$ \\
    Jacobi over-relaxation & p.~\pageref{ssec:algorithms:jgs} & moderate & $\checkmark$ & -- & $\checkmark$ \\
    Gauss--Seidel over-relaxation & p.~\pageref{ssec:algorithms:jgs} & very low & -- & -- & $\checkmark$ \\
    \textls{BiCGSTAB} & p.~\pageref{ssec:algorithms:bicgstab} & high & $\checkmark$ & -- & $\checkmark$ \\
    Group Jacobi & p.~\pageref{ssec:algorithms:group-jgs} & moderate & $\checkmark$ & $\checkmark$ & required \\
    Group Gauss--Seidel & p.~\pageref{ssec:algorithms:group-jgs} & low & -- & $\checkmark$ & required \\
    \bottomrule
  \end{tabular}
  \label{tab:overview:our:linear}
\end{table}

\begin{table}
  \caption{Transient solvers supported by our framework.}
  \centering
  \begin{tabular}{@{}llcccc@{}}
    \toprule
    & & instantenous & accumulated & uses inner & block \\[-0.5ex]
    & \multicolumn{1}{c}{see} & distribution & distribution & solver & matrix \\
    \midrule
    Uniformization & p.~\pageref{ssec:algorithms:uniformization} & $\checkmark$ & $\checkmark$ & -- & $\checkmark$ \\
    \textls{TR-BDF2} & p.~\pageref{ssec:algorithms:trbdf2} & $\checkmark$ & not impl. & $\checkmark$ & not impl. \\
    \bottomrule
  \end{tabular}
  \label{tab:overview:our:transient}
\end{table}

\begin{itemize}
\item The model state space may be explored either by an explicit 
  state space traversal, or by symbolic
  saturation~\citep{Ciardo:2006}. As symbolic methods are usually much
  faster and use significantly less memory than explicit enumeration,
  they are the recommended approach for stochastic analysis. However the explicit algorithms are not sensitive to the structure of the model, they provide a robust solution as long as the state space fits into memory. In addition they are provided for benchmarking and software redundancy reasons too.

  The algorithms operating on a superposed
  \textls{SPN} receive the model and a decomposition as an input. Partitions needed for the decomposition may be provided by the user
  as part of the model or generated on the fly.
\item The generator matrix may be stored in sparse matrix representation or decomposed into block
  Kronecker form%
  ~\citep{DBLP:journals/sigmetrics/BuchholzK98}. The matrix can be
  build from both explicitly or symbolically stored state spaces.

  To facilitate block Kronecker matrix generation, we propose a purely
  symbolic algorithm. The developed solution avoids any overheads of explicit state
  space operations.
\item The resulting matrices, in a possibly decomposed form, are part of a
  specialized data structure. Extremely large matrices may be stored
  with the developed decomposition algorithms (e.g.~linear combinations,
  Kronecker products, contatenations into block structures). The data
  structure defines generic vector and matrix operations, as well as
  more specific manipulations performed by stochastic analysis
  algorithms.

  State space exploration and generator matrix decomposition methods
  are presented in \cref{chap:genstor}, including our theoretical and algorithmic
  contribution for block Kronecker decomposition.
\item The execution of the operations on the data structures can be
  set at runtime. This allows the use of different
  implementations at the different stages of the workflow, or when
  different algorithms are employed to calculate multiple performance
  measures. Whenever possible, both sequential and parallel
  implementations of the most common operations are available for the
  supported datatypes.
\item Several numerical algorithms are provided for steady-state and
  transient analysis of Markov chains. The user can select the
  algorithm most suitable for the model under study. The algorithm library supports the combination of the algorithms and data structures at different levels of computations. This allows us to fine tune the numerical solution and solve every component with the most suitable algorithm.

  Important considerations in solver selection are convergence
  properties and memory requirements. Matrix decompositions
  can reduce the storage space needed by the matrix $Q$ by orders of
  magnitudes. We store all elements of probability vectors
  explicitly. Therefore, one should pay close attention to the number
  of temporary vectors used in the algorithm in order to avoid
  excessive memory consumption.

  Numerical algorithms supported by our framework are discussed in
  \cref{chap:algorithms}. Linear equations solvers for steady-state
  \CTMC\ analysis are shown in \cref{tab:overview:our:linear}, while
  linear solver are shown in \cref{tab:overview:our:transient}.
\end{itemize}

\subsection{Formalisms}

Our stochastic analysis framework supports models in the Stochastic
Petri Net with inhibitor arcs formalism (see
\vref{dfn:background:spn}). Structured models are handled as
Superposed Stochastic Petri Nets (see
\vref{dfn:background:sspn}). However, any modeling formalism can be
processed by integrating the appropriate state space exploration
algorithms with the workflow.

Transition rates in the \textls{SPNs} can be arbitrary algebraic
expressions containing references to \emph{sensitivity
  variables}. These variables correspond to the parameter vector
$\vec{\theta}$ of the Markov chain sensitivity analysis. However, the rate
expression may not depend on the marking of the net.

Reward structures are defined as Stochastic Reward Nets (see
\vref{dfn:background:srn}). An \textls{SRN} reward structure may be
specified by composing any \emph{reward expressions} of the forms
\begin{enumerate}
\item $(p, w)$, where $p \in P$ is a place and $w$ is a \emph{reward
    weight expression}. This reward expression is equivalent to
  a rate reward $\rateReward(M) = M(p) \cdot w$, i.e.~the value of $w$
  is multiplied by the numer of tokens on $p$.
\item $(t, w)$, where $t \in T$ is a transition and $w$ is a reward
  weight expression. This is equivalent to an impulse reward
  $\impulseReward(t, M) = w$ gained upon the firing of $t$.
\item $\varphi \rightarrow w$, where $\varphi$ is a Computational Tree
  Logic (\textls{CTL}) expression and $w$ is a reward weight
  expression. This is equivalent to the rate reward $\rateReward(M) =
  w$ if $\varphi$ holds in $M$, $0$ otherwise.
\end{enumerate}

A reward weight expression is an algebraic expression that may refer
to places and transition rates in the net. Refrences to places are
replaced by the number of tokens upon evaluation. For example, the
reward expression $(p, w)$ may be written as
$\KwTrue \rightarrow p \cdot w$ or $p > 0 \rightarrow p \cdot w$ using
\textls{CTL}.

Reward expressions with \textls{CTL} are only allowed when symbolic
state spaces representation is used, as \textls{CTL} evaluation%
\footnote{The symbolic state space exploration and \textls{CTL}
  evaluation component is currently provided by the
  \textsc{PetriDotNet}~\citep{Petridotnet} tool.} is performed
symbolically~\citep{TDK2010_Darvas}.

\begin{runningExample}
  Consider the reward structures defined over the
  \emph{Shared\-Resource} Petri net from \vref{ex:background:srn}.

  The utilization of the shared resource can be described by the
  reward expression
  \begin{equation}
    \textit{resourceUtilization} = \{ (p_{S_1}, 1), (p_{S_2}, 1)\} \text,
  \end{equation}
  which is equivalent to the \textls{SRN} reward structure
  \begin{equation}
    \rateReward_1(M) = M(p_{S_1}) + M(p_{S_2}), \quad
    \impulseReward_1(t, M) \equiv 0 \text.
    \tag{\ref{eq:background:srn:example-1} revisited}
  \end{equation}
  This can be also written as
  \begin{equation}
    \textit{resourceUtilization} = \{p_{S_1} > 0 \lor p_{S_2} > 0
    \rightarrow 1\}
  \end{equation}
  using \textls{CTL}, because the places $S_1$ and $S_2$ are
  $1$-bounded in the \emph{Shared\-Resource} model.

  Completed calculations are described by
  \begin{equation}
    \textit{completedCalculations} = \{ (t_{s_1}, 1), (t_{r_2}, 1)\} \text,
  \end{equation}
  which is equivalent to the reward structure
  \begin{equation}
    \rateReward_2(M) \equiv 0, \quad \impulseReward_2(t, M) = \begin{cases}
      1 & \text{if $t \in \{t_{r_1}, t_{r_2}\}$,} \\
      0 & \text{otherwise.}
    \end{cases}
    \tag{\ref{eq:background:srn:example-2} revisited}
  \end{equation}
\end{runningExample}

\subsection{Analysis}

The framework introducted in this paper supports the configurable stochastic analysis of the following problems:
\begin{itemize*}
\item expected steady-state reward rates $\Ex R$ for any reward
  structure defined by reward expressions,
\item expected transient reward rates $\Ex R(t)$ and accumulated
  rewards $\Ex Y(t)$,
\item \emph{complex rewards}, which are algebraic expressions of mean
  reward rates and accumulated rewards
  (e.g~$1 + \Ex R(t) / \Ex Y(t)$),
\item sensitivity of mean steady-state reward rates and complex
  rewards involving steady-state rates,
\item mean-time to failure $\MTFF$ and associated failure mode
  probabilities.
\end{itemize*}
Configurable stochastic analysis provides the combination of multiple solver and representation algorithms for the efficient computation of the introduced properties.

\subsection{Reward and sensitivity computation}

Transition and reward rates are stored as algebraic expression trees
in the input \textls{SPN} models. Symbolic operations, such as partial
differentiation may be performed exactly on the trees using algebraic
laws, as the evaluation of the expressions can be delayed.

In reward and \textls{MTFF} calculations, rate expressions are
evaluated by replacing sensitivity parameters with their values before
the matrix $Q$ is composed. Thus, the elements of a matrix are not
expression trees, but floating point numbers and matrix generation
has to be performed only when sensitivity parameters are changed.

Reward weight expressions may refer to the token counts on places,
therefore they must be evaluated for every marking individually. If a
\textls{CTL} reward expression $\varphi \rightarrow w$ is used,
evaluation is skipped in markings where $\varphi$ is false.

\begin{figure}
  \centering
  \begin{tikzpicture}[
    data/.style={
      text width=2.1cm,minimum height=1.2cm,align=center,
      draw,tdk highlight
    },
    activity/.style={data,rounded corners=0.25cm},
    node distance=0pt,inner sep=0pt
    ]
    \matrix [every node/.append style={activity},column sep=1cm,
    row sep=0.65cm] (m) {
      & \node (partial) {$\displaystyle \frac{\partial}{\partial
          \theta [i]}$}; \\
      \node (genq) {$Q$ matrix generation};
      & \node (genv) {$V_i$ matrix generation}; \\
      \node (solveq) {Solve for $\vec{p}$};
      & \node (solvev) {Solve for $\vec{s}_i$}; \\
      \node (preward) {$\displaystyle \vec{p}
        \frac{\partial \vec{r}^\T}{\partial \theta[i]}$};
      & \node (sreward) {Calculate $\vec{s}_i \vec{r}^\T$}; \\
    };
    \node [above=0.65cm of m,data] (rates) {Rate expressions};
    \node [below=0.65cm of m,activity] (sum) {$\displaystyle \sum$};
    \node [below=0.65cm of sum,data] (result) {Sensitivity value};
    \node [left=0.65cm of genq,data,yshift=0.925cm] (param)
    {Parameter bindings};
    \node [left=0.65cm of preward,data,yshift=0.925cm] (reward)
    {Reward expressions};

    \begin{scope}[every path/.append style={-{Latex}}]
      \draw (rates.south) -- ++(0,-0.325cm) -| (genq);
      \draw (rates.south) ++(0,-0.325cm) -| (partial);
      \draw (partial) edge (genv) (genv) edge (solvev)
      (solvev) edge (sreward) (genq) edge (solveq)
      (solveq) edge (preward) (sum) edge (result);
      \draw (preward.south) |- ($(sum.north)+(0,0.325cm)$) -- (sum);
      \draw [-] (sreward) |- ($(sum.north)+(0,0.325cm)$);
      \draw (param) -| ($(genv.north)+(-0.5cm,0)$);
      \draw ($(genq.north)+(-0.5cm,0.325cm)$) --
      ($(genq.north)+(-0.5cm,0)$);
      \draw (reward) -| ($(sreward.north)+(-0.5cm,0)$);
      \draw ($(preward.north)+(-0.5cm,0.325cm)$) --
      ($(preward.north)+(-0.5cm,0)$);
      \draw [dashed] (param) -- (reward);
    \end{scope}
  \end{tikzpicture}
  \caption{Reward and sensitivity calculation from expression tree
    inputs.}
  \label{fig:overview:sensitivity}
\end{figure}

Steady-state sensitivity calculation, shown in
\cref{fig:overview:sensitivity}, is the most complicated
post-processing in the workflow. Partial derivatives of the transition
rate expressions and reward weight expressions are taken to calculate
$\partial \Ex R / \partial \theta[i]$ using
\cref{eq:background:ctmc:sensitvity:s,%
eq:background:ctmc:sensitivity:productrule}.