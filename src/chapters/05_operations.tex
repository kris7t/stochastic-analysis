\chapter{Configurable data structure and operations}
\label{chap:operations}

In this chapter, we present the linear algebra library that was
developed as a foundation for configurable stochastic analysis.

The library is composed of a data structure and its related
operations. The \emph{data structure} provides abstraction for the
numerical solution algorithms over the used matrix and vectors storage
formats. Matrices stored as dense or sparse arrays, and even complex
expression involving sums and Kronecker products that arise from
matrix decompositions can be handled in a general way.

While direct read write access to elements is supported for most
matrices and vectors, the majority of manipulations, such as
vector--matrix products or vector additions, structure are performed
as \emph{operations}. Instead of being impelemented as methods of the
data structure classes, operations are decoupled into separata
entities. This allows operation execution with multiple dispatch,
selecting optimized implementations accoringd to dynamic types of all
operation arguments and other runtime properties, e.g.~the numer of
elements in the vector.

Another advantage of the decoupled operations framework is runtime
configurability. The dispatch logic may be replaced between the
execution of algorithms in the stochastic analysis workflow, therefore
low level linear algebra operations may customized to suit the
algorithm and the matrix decomposition in use, as well as the
hardware. For example, paralell and sequential execution may be
switched as neccessary.

Existing linear algebra and matrix libraries, such as%
~\citep{mathdotnet,bluebit,extremeopt,eigen,sanderson2010armadillo},
usually have unsatisfactory support for operations required in
stochastic analysis algorithms with decomposed matrices, such as
multiplications with Kronecker and block Kronecker
matrices. Therefore, we have decided to develop out linear algebra
framework from scratch in C\#.NET specifically for stochastic
algorithms as a basis of our stochastic analysis framework.

\section{Data structure}

The data structure library contains matrix and vector classes for
stochastic analysis.

Client code interacts with the data strucutre through interfaces only,
no classes are exposed on the public API. The main interfaces are
\texttt{IVector} and \texttt{IMatrix} for vectors and matrices,
respectively. The instances are created through an exposed static
factory.

The interfaces are generic in the element type. For
example~\texttt{IVector<\textbf{double}>} and
\texttt{IMatrix<\textbf{double}>} are used to work with
double-precision floating point arithmetic. Due to language
limitations, some classes must be implemented without genericity. In
these cases, only \texttt{\textbf{double}} is currently supported,
although re-implementation for single-precision floating point or
other numeric types is trivial. The static factory handles selection
of the appropriate non-generic type to instantiate if generic behavior
is impossible.

There also exist \emph{block} versions of these interfaces,
IBlockVector and IBlockMatrix. A block object is conceptually a
container of objects with scalar elements. For example, if
$\vec{v} \in \RR^{n_0 + n_1 + \cdots + n_{k - 1}}$ is a block vectors
with $k$ blocks, $\vec{v}[i] \in \RR^{n_i}$ ($0 \le i < k$) is a
vector of real numbers with $n_i$ elements, while $v[i][j]$ is the
$j$th element of the $i$th block of $\vec{v}$. Hovewer, block
interfaces do not extend from \texttt{IVector<IVector<T>>} and
\texttt{IMatrix<IMatrix<T>>}, but a facility separate from ordinary
indexing is provided for block access. This allows passing
\texttt{IBlockVector<\textbf{double}>} and
\texttt{IBlockMatrix<\textbf{double}>} objects to procedures consuming
ordinary \texttt{IVector<\textbf{double}>} and
\texttt{IMatrix<\textbf{double}>}.

\subsection{Partials, splitting and composition}

Manipulations of subsequences of vector and mactrix elements, as well
as conversion between flat and block object are performed by partial
object wrappers.

\begin{dfn}
  A \emph{partial vector} $\partialVec{v}{s}{t}{m}$ of a vector
  $\vec{v} \in \RR^n$ is
  \begin{equation}
    \partialVec{v}{s}{t}{m} \in \RR^m : \partialElement{v}{s}{t}{m}[i] =
    v[s + t \cdot i] \text{ for $i = 0, 1, \ldots, m$,}
  \end{equation}
  where $0 \le s \le n, 1 \le t, s + t (m - 1) \le n$.
\end{dfn}
The index $s$ is the start of partial, $t$ is the stride and $m$ is
the partial length. Matrix partials
$\partialMat{A}{s_1}{t_1}{m_1}{s_2}{t_2}{m_2} \in \RR^{m_1 \times
  m_2}$ are defined analogously.

The method \texttt{GetPartial} forms partial matrices and vectors. The
returned object is always a wrapper which passes through and read and
write indices to the original object after index
manipulation. However, partial manipulation of large vectors, which
was found to be a performance bottleneck upon profiling, is
implemented with pointer arithmetic instead.

The \texttt{GetPartial} method itself is also passed through. This
means forming a partial of partial
($\partialElement{\partialVec{v}{s_1}{t_1}{m_1}}{s_2}{t_2}{m_2}$) does not
result in a chain of wrappers being created, but only a single wrapper
object is places around the original after the neccassary index
manipulations.

Block vectors and matrices may be be formed by splitting flat objects
into blocks with partials, or by composition from unrelated objects.

\begin{dfn}
  A \emph{split} of a vector $\vec{v} \in \RR^n$ at $(n_0, n_1,
  \ldots, n_{k - 1})$ is a block vector
  \begin{equation}
    \label{eq:operations:ds:vector-split}
    \vec{v}_S \in \RR^{n_0 + n_1 + \cdots n_{k - 1}} : \vec{v}_S[i]
    = \partialVec{v}{N_i}{1}{n_i}, \quad N_i = \sum_{j = 0}^{i - 1} n_j \text,
  \end{equation}
  where $N_0 = 0$ and $n = N_{k + 1} = n_0 + n_1 + \cdots n_{k - 1}$.
\end{dfn}
Split matrices are defined analogously.

\begin{dfn}
  \label{dfn:operations:ds:vector-compose}
  If $\vec{v}_0, \vec{v}_1, \ldots, \vec{v}_{k - 1}$ are real vectors
  of length $n_0, n_1, \ldots, n_{k - 1}$, respectively, their
  \emph{composition}
  $\vec{v}_C \in \RR^{n_0 + n_1 + \cdots + n_{k - 1}}$ is block
  vector $\vec{v}_C[i] = \vec{v}_i$.
\end{dfn}

\begin{dfn}
  \label{dfn:operations:ds:matrix-compose}
  If
  $A_{0, 0}, A_{0, 1}, \ldots, A_{0, l - 1}, A_{1, 0}, \ldots A_{k - 1,
    l - 1}$
  are matrices such that $A_{i, j} \in \RR^{n_i, m_j}$, their
  \emph{composition}
  $A_C \in \RR^{(n_0 + n_1 + \cdots + n_{k - 1}) \times (m_0 +
    m_1 + \cdots m_{l - 1})}$ is block
  matrix $A_C[i, j] = A_{i, j}$.
\end{dfn}

The \texttt{Split} method builds block vectors and matrices from flat
objects for blockwise access. Objects formed by \texttt{Split} can be
split again arbitrarily, where the split command is forwarded to the
original flat object.

In contrast, \texttt{Split} can only be applied to a composite object
if it does not result in the creation of new partials. That is, a
composite vector $\vec{v} \in \RR^{n_0 + n_1 + \cdots + n_{k - 1}}$
may only be split at $(m_0, m_1, \ldots, m_{l - 1})$ if $k = l$ and
$n_i = m_i$ for all $0, 1, \ldots, k - 1$. Because composite objects
are usually very large and are used in performance critical parts of
algorithms, we decided to throw an exception instead of splitting even
though arbitrary splitting of composite vectors and matrices would
have been implemented easily.

\subsection{Vectors}

\begin{figure}
  \centering
  \begin{tikzpicture}[
    edge from parent/.style={uml inheritance},
    level 1/.style={
      edge from parent fork down,sibling distance=18em},
    level 2/.style={
      grow via three points={one child at (2em,-3em) and
      two children at (2em,-3em) and (2em,-6em)},
      edge from parent path={(\tikzparentnode.south) |-
        (\tikzchildnode.west)},
      anchor=west
    }
    ]
    \node [uml class] (Abstract) {\emph{AbstractVector}}
    child { coordinate [edge from parent/.style={draw}]
      child { node [uml class] {ArrayVector} }
      child { node [uml class] (Native) {NativeVector} }
      child { node [uml class] {ConstantVector} }
      child { node [uml class] {IdentityVector} }
      child { node [uml class] (Partial) {PartialVectorWrapper}
        child { [edge from parent/.style={uml inheritance}]
          node [uml class] {NativePartialVectorWrapper} }
      }
      child { node [uml class,yshift=-4em] (FlexiblePartial) {FlexiblePartialVectorWrapper}
        child { [edge from parent/.style={uml inheritance}]
          node [uml class] {FlexibleNativePartialVectorWrapper} }
      }
      child { node [uml class,yshift=-7em] {MatrixRowVector} }
      child { node [uml class,yshift=-7em] {MatrixColumnVector} }
      child { node [uml class,yshift=-7em] {MatrixDiagonalVector} }
    }
    child {
      node (AbstractBlock) [uml class] {\emph{AbstractBlockVector}}
      child { node [uml class] {VectorSplitWrapper} }
      child { node [uml class] {BlockVector} }
    };
    \umlLollipop{Abstract}{IVector}
    \umlLollipop{AbstractBlock}{IBlockVector}
    \umlLollipop{Native}{IDisposeable}
    \umlLollipop[5em,2em]{FlexiblePartial}{IFlexiblePartialVector}
  \end{tikzpicture}
  \caption{Inheritance hierarchy of vectors.}
  \label{fig:operations:ds:vectors-uml}
\end{figure}

Vector data structures are used to store probability distributions of
Markovian modells, as well as intermediate results of numerical
algorithms.

The class hiearchy of vectors in our library is shown in
\cref{fig:operations:ds:vectors-uml}.

The abstract base classes \texttt{AbstractVector},
\texttt{AbstractBlockVector} are at the root of the inheritance
hierarchies. The data structure may be extended by inheriting from
these classes, or by implementing the publicly exposed interfaces
directly.

Vector datatypes are available for the storage of general vectors, as
well as for some special cases.

\paragraph{ArrayVector}

The basic vector datatype provided is \texttt{ArrayVector}, which
stores vector elements in a Common Language Runtime (\textls{CLR})
array. This class is completely generic, i.e.~any \textls{CLR} value or
reference type may be used as an element.

The Microsoft \textls{.NET} implementation of the \textls{CLR} allows
arrays of size up to 2~GiB even on 60-bit platforms. While on
\textls{.NET}~4.5, this limitation may be lifted with the
\texttt{gcAllowVeryLargeObjects} configuration directive%
\footnote{\url{https://msdn.microsoft.com/en-us/library/hh285054(v=vs.110).aspx}},
this setting is cumbersome to use. Therefore, no vectors larger than
2~GiB should be stored as array vectors.

\paragraph{NativeVector}

To work around the 2~GiB memory limitation on \textls{CLR} arrays, we
implemented \texttt{NativeVector} which stores vector elements on the
unmanaged heap. We also found unmanaged allocation reduce the pressure
on the garbage collector, therefore provide the benefit of faster
allocations.

Native vectors utilize the \texttt{unsafe} facilities%
\footnote{\url{https://msdn.microsoft.com/en-us/library/chfa2zb8.aspx}}
provided by the C\# language, including the access to memory through
pointers and direct memory management through \texttt{AllocHGlobal}
and \texttt{FreeHGlobal}. Therefore, the linear algebra library must
be compiled with unsafe language features enabled. As an alternative,
\texttt{NativeVector} may be disabled with conditional compilation
directive and replaced by a wrapper around \texttt{ArrayVector},
forgoing the benefits of unmanaged allocation.

Due to language limitations, \texttt{NativeVector} must be implemented
for any primitive type desired to be used as vector
elements. Currently, only \texttt{\textbf{double}} is supported.

The use of unmanaged memory requires manual deallocation to avoid
memory leaks. Because the C\# language does not provide deterministic
destructors, the \texttt{IDisposeable} pattern%
\footnote{\url{https://msdn.microsoft.com/en-us/library/system.idisposable(v=vs.110).aspx}}
must be used.

\begin{lstlisting}[float,caption={Manual memory management for
    \texttt{NativeVector}.},label=lst:operations:ds:dispose-vector]
// Create and dispose a NativeVector of length 100.
using (var vector = Vectors.NewDisposeableVector<double>(100))
{
	vector[0] = 1.0;
}

// Dispose using IBufferProvider.
var factory = new DisposingBufferProviderFactory();
using (var bufferProvider = factory.Make())
{
	var v1 = bufferProvider.GetVector<double>(100);
	var v2 = bufferProvider.GetVector<double>(100);
}
// Both v1 and v2 are disposed here.
\end{lstlisting}

As an alternative means of memory management, an interface
\texttt{IBufferProvider} may be used to allocate and track multiple
vectors. A \texttt{IBufferProvider} itself implements
\texttt{IDisposeable}, a single C\# \texttt{using} block may free
several vectors in the same scope, easing the burden of manual
disposal. This approach is illustrated in
\cref{lst:operations:ds:dispose-vector}.

\paragraph{ConstantVector}

A constant vector is a vector with equal elements.

Two important special vector may be realized as
\texttt{ConstantVector} instances in stochastic analysis, the vector
of all zeroes $\vec{0}$, and the vector of all ones $\vec{1}$,
\begin{align}
  \texttt{Vectors.Constant<\textbf{double}>($n$, 0)}
  &= \vec{0} \in \RR^n \text, \\
  \texttt{Vectors.Constant<\textbf{double}>($n$, 1)}
  &= \vec{1} \in \RR^n \text.
\end{align}

Because constant vectors require only $O(1)$ storage space instead of
$O(n)$, this is an important optimization in equations involving
$\vec{0}$, $\vec{1}$ and its scalar multiples.

\paragraph{IdentityVector}

An identity vector is vector with all but one zero elements and a
single $1$ element. Formally,
\begin{equation}
  \texttt{Vectors.Identity<\textbf{double}>($n$, $i$)} = \vec{e}_i \in
  \RR^n, \quad e_i[j] = \delta_{i, j} = \begin{cases}
    1 & \text{if $i = j$,} \\
    0 & \text{if $i \ne j$.}
  \end{cases}
\end{equation}

\texttt{IdentityVector} is an $O(1)$ space optimization for storing
special vectors, similar to \texttt{ConstantVector}.

\paragraph{PartialVectorWrapper and FlexiblePartialVectorWrapper}

Taking a partial vector of a vector results in the creation of
\texttt{PartialVectorWrapper} object, which passes through read and
write actions to the underlying vector after the neccessary index
manipulations. Hence a new object is allocated at every call to
\texttt{GetPartial}.

Long delegation chains are eliminated by \emph{collapsing} partial
vectors. When the method \texttt{GetPartial} is invoked on
\texttt{PartialVectorWrapper}, it performs index manipulations and
passes delegates to the \texttt{GetPartial} method of the original
vector. Thus, further partials do not have reference to the partial
vector they were created from, but only to the underlying vector.

\begin{lstlisting}[float,caption={Ambigous use of
    \texttt{FlexiblePartialVectorWrapper}.},label=lst:operations:ds:flexible-vector]
var vector = Vectors.NewArray<double>(100);

var part = vector.GetPartial(5, 2, 20);
var subPart = part.GetPartial(2, 1, 5);
// Unambigous, subPart = vector[5:2:20][2:1:5].

var flexible = vector.GetFlexiblepartial(5, 2, 20);
var subFlexible = vector.GetPartial(2, 1, 5);
flexible.SetPartial(6, 2, 20);
// Ambigous, is subFlexible = vector[5:2:20][2:1:5] or vector[6:2:20][2:1:5]?
\end{lstlisting}

\texttt{FlexiblePartialVectorWrapper} alleviates allocation costs in
inner loops by providing a partial vector whose indices can be changed
after construction. For example, if a flexible partial vector
$\partialVec{v}{s}{t}{m}$ is available, it can be changed to
$\partialVec{v}{s'}{t'}{m'}$ without allocating a new instance
whenever the need arises. The functionality is exposed to consumers
through an interface.

Flexible partials cannot have their partials taken, because the
collapse of the delegation chain makes the propagation of index
changes impossible. This problem is illustrated in
\cref{lst:operations:ds:flexible-vector}.

Another set of partial wrappers handle partials of
\texttt{NativeVector} instances. In these cases, a
$(\textit{base pointer}, \textit{stride}, \textit{length})$ triple amy
be queried for use in low-level operations. Hence indexing logic may
be skipped in favor of direct access.

To create a unified interface, the same triple may be queried from
\texttt{NativeVector} instances themselves, where $\textit{base
  pointer}$ is the pointer to the allocated buffer, $\textit{stride} =
1$ and $\textit{length}$ is the length of the vector itself.

\paragraph{Matrix vector wrappers}

To facilitate common manipulations of matrices, our library provides
wrappers to for read and write access of parts of matrices as
vectors.

\texttt{MatrixRowVector} and \texttt{MatrixColumnVector} accesses a
row or a column of matrix, respectively. If $A \in \RR^{n \times m}$,
\begin{align}
  \texttt{$A$.GetRow($i$)} = \mathrlap{\vec{r}}\phantom{\vec{c}} \in \RR^m, \quad
  &r[j] = a[i, j] \text,\\
  \texttt{$A$.GetColumn($j$)} = \vec{c} \in \RR^{\mathrlap{n}\phantom{m}}, \quad
  &\phantom{r[j]}\mathllap{c[i]} = a[i, j]
\end{align}
for $0 \le i < n$, $0 \le j < m$.

If $n = m$, i.e.~$A$ is square, \texttt{MatrixDiagonalVector} may
provide access to the diagonal of the matrix,
\begin{equation}
  \texttt{$A$.GetDiagonal()} = \vec{d} \in \RR^n = \RR^m, \quad
  d[i] = a[i, j] \text.
\end{equation}

\paragraph{Block vectors}

\texttt{VectorSplitWrapper} reifies vector splitting accoring to
\vref{eq:operations:ds:vector-split}. The split vector is backed by a
composition of partial vectors, thus it acts as a composite
vector. However, when the split wrapper is used as an instance of
\texttt{IVector}, commands, including re-splitting, are delegated to
the underlying vector instead.

Composition of vectors according to
\vref{dfn:operations:ds:vector-compose} is represented by
\texttt{BlockVector}. The constructor of \texttt{BlockVector} is
passed a sequence of vectors which will constitute the block
vector. Because \texttt{BlockVector} implements \texttt{IVector}, the
composite vector may be used as a normal vector, however, splitting is
limited to avoid performance penalties associated 

\subsection{Matrices}

\begin{figure}
  \centering
  \begin{tikzpicture}[
    edge from parent/.style={draw,{Triangle[open,fill=white,angle=90:8pt]}-{}},
    level 1/.style={
      edge from parent fork down,sibling distance=18em},
    level 2/.style={
      grow via three points={one child at (2em,-3em) and
      two children at (2em,-3em) and (2em,-6em)},
      edge from parent path={(\tikzparentnode.south) |-
        (\tikzchildnode.west)},
      anchor=west
    }
    ]
    \node [uml class] (Abstract) {\emph{AbstractMatrix}}
    child { coordinate [edge from parent/.style={draw}]
      child { node [uml class] {ArrayMatrix} }
      child { node [uml class] (Sparse) {SparseMatrix} }
      child { node [uml class] (Native) {NativeSparseMatrix} }
      child { node [uml class] (Diagonal) {DiagonalMatrix} }
      child { node [uml class] {NullMatrix} }
      child { node [uml class] {IdentityMatrix} }
      child { node [uml class] {PartialMatrixWrapper} }
      child { node [uml class] {PartialMaskWrapper} }
      child { node [uml class] {NormalizedLinearSystemWrapper} }
      child { node [uml class,yshift=-1em] (Linear) {LinearCombinationMatrix} }
      child { node [uml class,yshift=-2em] (Kronecker) {KroneckerMatrix} }
    }
    child {
      node (AbstractBlock) [uml class] {\emph{AbstractBlockMatrix}}
      child { node [uml class] {MatrixSplitWrapper} }
      child { node [uml class] (Block) {BlockVector} }
    };
    \umlLollipop{Abstract}{IMatrix}
    \umlLollipop{AbstractBlock}{IBlockMatrix}
    \umlLollipop{Sparse}{ISparseMatrix}
    \umlLollipop{Native}{ISparseMatrix}
    \umlLollipop[7em,0.5em]{Native}{IDisposeable}
    \umlLollipop{Linear}{ILinearCombinationMatrix}
    \umlLollipop{Kronecker}{IKroneckerMatrix}

    \path let \p1 = (Linear) in let \p2 = (Block.west) in
    node [anchor=west,uml class] (Weighted) at (\x2,\y1)
    {\texttt{WeightedMatrix}};
    \draw [{Diamond[length=10pt]}-] (Linear) edge node [at
    end,anchor=south east] {*} (Weighted);
  \end{tikzpicture}
  \caption{Inheritance hierarchy of matrices.}
  \label{fig:operations:ds:matrices-uml}
\end{figure}

The class hiearchy of vectors in our library is shown in
\cref{fig:operations:ds:matrices-uml}. The abstract base classes
\texttt{AbstractVector}, \texttt{AbstractBlockVector} are at the root
of the inheritance hierarchies.

\paragraph{ArrayMatrix}

For smaller dense arrays with items of any value or reference type,
\texttt{ArrayMatrix} allows storage in two-dimensional \textls{CLR}
array.

The matrix may not be larger than 2~GiB, however, this is not a
serious limitation in practice, because processing large dense arrays
could take extreme amounts of time.

\paragraph{SparseMatrix and NativeSparseMatrix}

\begin{figure}
  \centering
  \begin{minipage}{0.5\linewidth}
    \begin{equation}
      A = \
      \begin{pmatrix}
        1 & 0 & 0 & 2.5 \\
        3 & 1 & 0 & 0 \\
        4 & 0 & 0 & 1 \\
        5 & 0 & 0 & 0
      \end{pmatrix}
    \end{equation}
  \end{minipage}%
  \begin{minipage}{0.5\linewidth}
    \begin{align}
      A = \{&\{(1,0), (3,1), (4,2), (5,3)\}, \\
      &\{(1,1)\}, \\
      &\{\}, \\
      &\{(2.5,0), (1,2)\}\}
    \end{align}
  \end{minipage}
  \caption{Compressed Column Storage of a matrix.}
  \label{fig:genstor:matrix:ccs}
\end{figure}

Sparse matrices are stored in Compressed Column Storage (\textls{CCS})
format, i.e.~an array or values and row indices are stored for each
column of the matrix (\cref{fig:genstor:matrix:ccs}), in order to
effectively perform multiplications by vectors from left.

While other sparse matrix formats, such as sliced \textls{LAPACK} are
more amenable to parallel and \textls{SIMD} processing
\citet{DBLP:journals/corr/KreutzerHWFB13}, \textls{CCS} was selected
due to implementation simplicity and the small number of nonzero
entries in each column of the matrix, which reduces the potential
benefits of \textls{SIMD} implementations.

The class \texttt{SparseMatrix} implements \textls{CCS} sparse
matrices backed by \textls{CLR} arrays.

Due to state space explosition, extremely large sparse matrices may be
needed when block Kronecker decomposition is not use use. In our
experiments in \cref{chap:evaluation}, sparse matrices up to 20~GiB
were tested. Therefore, sparse matrices backed by unmanaged
allocations were also implemented in the class
\texttt{NativeSparseMatrix}.

\texttt{NativeSparseMatrix} is used for all sparse matrices, including
Kronecker factor matrices in block Kronecker decompositions. Memory
may be freed by the \texttt{IDisposeable} pattern or
\texttt{IBufferProvider} (see
\vref{lst:operations:ds:dispose-vector}).

\paragraph{DiagonalMatrix}

For $O(n)$ storage of matrices that that have nonzero elements only
along their diagonal, \texttt{DiagonalMatrix} provides a wrapper
around any \texttt{IVector} containing the diagonal
elements.

Let $\vec{v} \in \RR^n$. Then we have
\begin{equation}
  \texttt{Matrices.Diagonal($\vec{v}$)} = \diag \{ \vec{v} \} = D \in
  \RR^{n \times n}, \quad d[i, j] = \begin{cases}
    v[i] & \text{if $i = j$,} \\
    0 & \text{if $i \ne j$.}
  \end{cases}
\end{equation}

\paragraph{NullMatrix and IdentityMatrix}

Two special cases were implemented with $O(1)$ storage, zero matrices
and identity matrices, i.e.
\begin{gather}
  \texttt{Matrices.Null<\textbf{double}>($n$, $m$)} = A \in \RR^{n
    \times m}, \quad a[i, j] = 0, \\
  \texttt{Matrices.Identity<\textbf{double}>($n$)}
  = B \in \RR^{n \times n}, \quad b[i, j] = \delta_{i, j} =
  \begin{cases}
    1 & \text{if $i = j$,} \\
    0 & \text{if $i \ne j$.}
  \end{cases}  
\end{gather}

\paragraph{PartialMatrixWrapper}

Partials of matrices written as
$\partialMat{A}{s_1}{t_1}{m_1}{s_2}{t_2}{m_2}$ are represented as
instances of \texttt{PartialMatrixWrapper}. The wrapper passes through
any access after the neccessary index manipulations to the underlying
matrix.

There is no support for flexible partial matrices, that is, unlike
vectors, the partial indexing of a matrix cannot be updated without
creating a new instance of \texttt{IPartialMatrixWrapper}.

\paragraph{PartialMaskWrapper}

The \texttt{PartialMaskWrapper} class may be used to access the
strictly upper and strictly lower triangular and diagonal parts of a
matrix. In addition, the mask flags may be combined arbitrarily to
yield upper and lower triangular parts of matrices including the
diagonal and also the off-diagonal part.

\paragraph{NormalizedLinearSystemWrapper}

The normalized linear system wrapper replaces the last column of a
matrix with the vector of all ones $\vec{1}$. If $A \in \RR^{n \times
  m}$,
\begin{equation}
  \texttt{$A$.ToNormalizedLinearSystem()} = \widehat{A} \in \RR^{n \times
    m}, \quad \hat{a}[i, j] =
  \begin{cases}
    1 & \text{if $j = m - 1$,} \\
    a[i, j] & \text{if $j \ne m - 1$.}
  \end{cases}
\end{equation}

If $A$ is an $n \times n$ matrix of rank $n  - 1$ (i.e.~its nullity is
$1$) the system of linear equations
\begin{equation}
  \vec{x} A = \vec{1}, \quad \vec{x} \vec{1}^\T = 1
\end{equation}
may be replaced with
\begin{equation}
  \vec{x} \widehat{A} = \vec{e}_{n - 1},
\end{equation}
because $A$ contains a redundant column due to its rank
deficiency. The vector $\vec{e}_{n - 1}$ may be realized as an
\texttt{IdentityVector}.

\paragraph{LinearCombinationMatrix}

Linear combinations of matrices may be stored as an instance of
\texttt{LinearCombinationsMatrix} that contains a sequence of
\texttt{WeightedMatrix} instances, which are pairs of \texttt{IMatrix}
objects a double-precision floating point scaling factors. In our
current implementation, only linear combinations of
\texttt{IMatrix<\textbf{double}>} are supported.

If $A_0, A_1, \ldots, A_{k - 1} \in \RR^{n \times m}$ and $w_0, w_1,
\ldots, w_{k - 1} \in \RR$,
\begin{equation}
  \begin{multlined}[12cm]
    \texttt{Matrices.LinearCombination($A_0$.WithWeight($w_0$),} \\[1ex]
    \texttt{$A_1$.WithWeight($w_1$), $\ldots$, $A_{k - 1}$.WithWeight($w_{k -
      1}$))} = \\
  w_0 A_0 + w_1 A_1 + \cdots + w_{k - 1} A_{k - 1} \in \RR^{n \times
    m} \text.
  \end{multlined}
\end{equation}

As an optimization, a term of type \texttt{DiagonalMatrix} in the
linear combination may be designated as the \emph{diagonal} if no
other term contains diagonal elements. In this case, methods to access
the diagonal of the linear combination matrix will return the
designated term instead of \texttt{MatrixDiagonalVector} and
\texttt{PartialMaskWrapper} wrapper objects.

\paragraph{KroneckerMatrix}

\texttt{KroneckerMatrix} allows the representations of Kronecker
products of matrices as expressions similar to
\texttt{LinearCombinationMatrix} for linear combinations.
More concretely,
\begin{equation}
  \texttt{Matrices.KroneckerProduct($A_0$, $A_1$, $\ldots$, $A_{J -
      1}$)} = A_0 \krtimes A_1 \krtimes \cdots \krtimes A_{J - 1}
  \text,
\end{equation}
where $A_0, A_1, \ldots, A_{J - 1}$ are matrices of possibly different
sizes.

As the Kronecker product is not explicitly computed, very large
matrices may be expressed with this method in relatively little space.

\paragraph{Block matrices}

The classes \texttt{MatrixSplitWrapper} and \texttt{BlockMatrix} may
be used for blockwise storage and access of matrices in ways similar
to \texttt{VectorSplitWrapper} and \texttt{BlockVector}.

Diagonals and triangular parts of block matrices in $\RR^{(n_0 + n_1 +
  \cdots + n_{k - 1}) \times (m_0 + m_1 + \cdots + m_{l - 1})}$ may
only be requested if $k = l$ and $n_i = m_i$ for all $i = 0, 1,
\ldots, k - 1$, i.e.~when the rows and columns of the matrix have the
same blocking partition. This always occurs if the matrices describe
transitions between states of a Markov chain, therefore, it poses no
practical limitation.

\subsection{Expression trees}

\begin{figure}
  \centering
  \begin{tikzpicture}
    \matrix [every node/.append style={
      text width=2.1cm,minimum height=1.2cm,align=center,
      draw,tdk highlight
    },column sep=1cm,row sep=0.3cm] {
      \node (block) {Block\\Matrix}; & \node (lin) {Linear\\Combination};
      & \node (kr) {Kronecker\\Matrix}; & \node (sparse) {Sparse\\Matrix}; \\
      & & & \node (id) {Identity\\Matrix}; \\
      & & \node (diag) {Diagonal Matrix}; & \node (vec) {Vector}; \\
    };
    \draw [{Diamond[length=10pt]}-,
    every node/.append style={at end,above,anchor=south east}]
    (block) edge node {$\ast$} (lin) (lin) edge node {$\ast$} (kr)
    (kr) edge node {$\ast$} (sparse)
    (diag) edge node [below,anchor=north east,yshift=-1] {1} (vec);
    \draw [every node/.append style={at end,below,anchor=north east,yshift=-1pt}]
    ($(lin.east)+(0.5cm,0)$) |- node {0..1} (diag)
    ($(kr.east)+(0.5cm,0)$) |- node {$\ast$} (id);
  \end{tikzpicture}
  \caption{Data structure for block Kronecker matrices.}
  \label{fig:genstor:kronecker:datastructure}
\end{figure}

Decomposed Kronecker and block Kronecker matrices are stored as
algebraic expression trees as shown in
\cref{fig:genstor:kronecker:datastructure}. Expression may contain
Kronecker products (\texttt{KroneckerMatrix}), linear combinations
(\texttt{LinearCombinationMatrix}) and block compositions
(\texttt{BlockMatrix}).

The expression tree approach allows the use of arbitrary matrix
decompositions that can be expressed with block matrices, linear
combinations and Kronecker products. The implementation of additional
opeartional primitives is also straightforward. The data structure
forms a flexible basis for the development of stochastic analysis
algorithms with decomposed matrix representations.

\section{Operations}

The operation framework achieves decoupling between the data structure
and operations by reifying low-level linear algebra manipulations as
\emph{operation} objects.

In contrast to numerical solution algorithms, these operations are
specific to their argument types. For example, the multiplication of a
\texttt{NativeVector} with a matrix stored as a Kronecker product
might be served by a different operation object that the
multiplication by a \texttt{SparseMatrix}. In addition, dispatch logic
may take other runtime properties into account, such as selecting a
parallel implementation for multiplication only if the vector is long
enough.

\subsection{Operation declarations}

Operations have a \emph{base} (B) argument, and optionally one or
both of \emph{operand} (O) and \emph{target} (T)
arguments. Syntactically, operation calls are represented as methods
of the base object.

The target of the operation is the vector or matrix that gets modified
by the operation. \emph{In-place} execution of operations refers to
the usage when the base and the target are equal. It is also possible
that the operations has no other argument that its base, for example,
the clearing of a vector by filling it with zeroes uses only a
base. Then the base of the operation will be modified, similar to
in-place use.

Extra (E) operation arguments may include a \texttt{\textbf{double}}
scaling factor $\lambda$ and various indices for operations
manipulating parts of vectors and matrices. Operations that return a
value are also possible.

\begin{lstlisting}[float,caption={\textls{DSL} for operation
    declarations (excerpt).},label=lst:operations:op:operationname]
public enum OperationName
{
    // Operation with base, operand, target and scale factor.
    [OperationAlias("Add")]
    [OperationArguments(typeof(IVector<>), typeof(IVector<>), typeof(IVector<>), ExtraOperationArguments.ScaleFactor)]
    VectorAdd,

    // Operation with only base and extra argument.
    [OperationAlias("Clear")]
    [OperationArguments(typeof(IVector<>), Extra = ExtraOperationArguments.Length)]
    VectorClearFirstN,

    // Operation that returns a value of the vector element type.
    [OperationAlias("Sum")]
    [OperationArguments(typeof(IVector<>), Return = typeof(TheGenericArgument))]
    VectorSumElements,
}
\end{lstlisting}

Operations are specified using an Domain Specific Language
(\textls{DSL}) embededd into the C\# programming language. An excerpt
of the operation declarations is shown in
\cref{lst:operations:op:operationname}.

\begin{lstlisting}[float,caption={An example interface generated by
    the \texttt{T4} template.},label=lst:operations:op:interface]
public interface IVectorAddOperation<T>
{
    void Invoke(PDN.Analysis.Common.Math.Vector.IVector<T> @base, PDN.Analysis.Common.Math.Vector.IVector<T> operand, PDN.Analysis.Common.Math.Vector.IVector<T> target, double scaleFactor);
}
\end{lstlisting}

The operation declarations are processed with a Microsoft Text
Template Transformation Toolkit (\textls{T4}) template~\citep{T4}. The
template genrates interfaces to be implemented by the operation
classes (\cref{lst:operations:op:interface}). In addition, a dispatch
logic stub is created in the \texttt{OperationContext}, which is the
class that dispatches operation invocations.

\begin{table}
  \caption{Linear algebra operations supported by our framework with
    their base (B), operand (O), target (T) and extra (E)
    arguments. Operations marked with a star (*) are syntactic sugars
    implemented in terms of other operations.}
  \centering
  \begin{tabular}{@{}lc@{\,}c@{\,}c@{\,}cl@{}}
    \toprule
    Category and operation & B & O & T & E & Description \\
    \midrule
    \OperationPrefix{Vector}
    \OperationDesc{Add}{$\vec{b}$}{$\vec{o}$}{$\vec{t}$}{$\lambda$}{%
    $\vec{t} \gets \vec{b} + \lambda \vec{o}$}
    \OperationDesc{Scale}{$\vec{b}$}{$-$}{$\vec{t}$}{$\lambda$}{%
    $\vec{t} \gets \lambda \vec{b}$}
    \OperationDesc{Accumulate}{$\vec{b}$}{$-$}{$\vec{t}$}{$\lambda$}{%
    $\vec{t} \gets \vec{t} + \lambda \vec{b}$}
    \OperationSugarDesc{Set}{$\vec{b}$}{$\vec{o}$}{$-$}{$-$}{%
    $\vec{b} \gets \vec{o}$}
    \OperationDesc{ElementwiseMultiply}{$\vec{b}$}{$\vec{o}$}{$\vec{t}$}{$-$}{%
    $t[i] \gets b[i] \cdot o[i]$}
    \OperationDesc{AccumulateElementwiseMultiply}{$\vec{b}$}{$\vec{o}$}{$\vec{t}$}{$\lambda$}{%
    $t[i] \gets t[i] + \lambda \cdot b[i] \cdot o[i]$}
    \OperationDesc{Clear}{$\vec{b}$}{$-$}{$-$}{$-$}{%
    $\vec{b} \gets \vec{0}$}
    \OperationDesc{ClearFirstN}{$\vec{b}$}{$-$}{$-$}{$n$}{%
    $\partialVec{b}{0}{1}{n} \gets \vec{0}$}
    \OperationDesc{ScalarProduct}{$\vec{b}$}{$\vec{o}$}{$-$}{$-$}{%
    return $\vec{b} \cdot \vec{o}$}
    \OperationDesc{Sum}{$\vec{b}$}{$-$}{$-$}{$-$}{%
    return $\sum_{i = 0}^{n} b[i] = \vec{b} \cdot \vec{1}$}
    \OperationDesc{L1Norm}{$\vec{b}$}{$-$}{$-$}{$-$}{%
    return $\sum_{i = 0}^{n} \lvert b[i] \rvert = \| \vec{b} \|_1$}
    \OperationSugarDesc{L2Norm}{$\vec{b}$}{$-$}{$-$}{$-$}{%
    return $\sqrt{\vec{b} \cdot \vec{b}} = \| \vec{b} \|_2$}
    \OperationPrefix{Matrix}
    \OperationDesc{Add}{$B$}{$O$}{$T$}{$\lambda$}{%
    $T \gets B + \lambda O$}
    \OperationDesc{Scale}{$B$}{$-$}{$T$}{$\lambda$}{%
    $T \gets \lambda B$}
    \OperationDesc{Accumulate}{$B$}{$-$}{$T$}{$\lambda$}{%
    $T \gets T + \lambda B$}
    \OperationSugarDesc{Set}{$B$}{$O$}{$-$}{$-$}{%
    $B \gets O$}
    \OperationDesc{Clear}{$B$}{$-$}{$-$}{$-$}{%
    $B \gets \text{the zero matrix}$}
    \OperationPrefix{VectorMatrix}
    \OperationDesc{MultiplyFromLeft}{$B$}{$\vec{o}$}{$\vec{t}$}{$-$}{%
    $\vec{t} \gets \vec{o} B$}
    \OperationDesc{AccumulateMultiplyFromLeft}{$B$}{$\vec{o}$}{$\vec{t}$}{$\lambda$}{%
    $\vec{t} \gets \vec{t} + \lambda \vec{o} B$}
    \OperationDesc{MultiplyFromRight}{$B$}{$\vec{o}$}{$\vec{t}$}{$-$}{%
    $\vec{t} \gets B \vec{o}$}
    \OperationDesc{AccumulateMultiplyFromRight}{$B$}{$\vec{o}$}{$\vec{t}$}{$\lambda$}{%
    $\vec{t} \gets \vec{t} + \lambda B \vec{o}$}
    \OperationDesc{ScalarProductWithColumn}{$B$}{$\vec{o}$}{$-$}{$j$}{%
    return $\vec{b}[\cdot, j] \cdot \vec{o}$}
    \bottomrule
  \end{tabular}
  \label{tab:operations:operations}
\end{table}

The declared operations are summarised in
\cref{tab:operations:operations}.

Some operations are special cases of others, for example,
\operationname[Vector]{ElementwiseMultiply} may be implemented in terms of
\operationname[Vector]{Clear} and
\operationname[Vector]{AccumulateElementwiseMultiply}. However, a
non-orthogonal set operations was defined to handle special cases such
as in-place execution. In addition, the non-orthogonality allows
more easier overloading in specific bottleneck scenarios identified by
profiling.

\subsection{Binding of operations to the data structure}

Operations are declared on the interface of the base object as
methods. Therefore, the call to an operation is syntactically
equivalent to a method call. This gives the opportunity to use a
friendlier naming and parameter order on the interfaces than the
scrict base-operand-target conventions used by the operation
declrations.

The interface is augmented with a \emph{contract class} which
describes the pre- and postconditions of the operation. The Microsoft
Code Contracts~\citep{CodeContracts} runtime verification engine
inserts assertions into the output of the compiler if the library is
compiled in Debug mode. However, no assertions are inserted in Release
mode, thus performance penalties are averted in production.

The base classes \texttt{AbstractVector} and \texttt{AbstractMatrix}
contain a reference to their \texttt{OperationContext}. Operations are
executed by delegation to the context, which contains the dispatch
logic, partly generated by the T4 template from the operation
declaration \textls{DSL}, partly configured at runtime.

\begin{lstlisting}[float,caption={Delegation of operations to the
    context (excerpt).},label=lst:operations:op:delegation]
[ContractClass(typeof(VectorContract<>))]
public interface IVector<T> : IEnumerable<T>
{
    // Method declaration on the interface with friendlier naming.
    IVector<T> Add(IVector<T> toAdd, IVector<T> resultTarget, double scaleFactor = 1);
}

[ContractClassFor(typeof(IVector<>))]
abstract class VectorContract<T> : IVector<T>
{
    // Declaration of preconditions.
    IVector<T> IVector<T>.Add(IVector<T> toAdd, IVector<T> resultTarget, double scaleFactor)
    {
        Contract.Requires(toAdd != null);
        Contract.Requires(resultTarget != null);
        Contract.Requires(toAdd.Length == ((IVector<T>)this).Length);
        Contract.Requires(resultTarget.Length >= ((IVector<T>)this).Length);
        throw new NotImplementedException();
    }
}

abstract class AbstractVector<T> : IVector<T>
{
    OperationContext OperationContext { get; set; }

    // Delegation to the OperationContext.
    public virtual IVector<T> Add(IVector<T> toAdd, IVector<T> resultTarget, double scaleFactor = 1)
    {
        OperationContext.Add(this, toAdd, resultTarget, scaleFactor);
        return resultTarget;
    }
}
\end{lstlisting}

The contract and delegation process is illustrated in
\cref{lst:operations:op:delegation}.

Another domain specific language describe the dispatch logic of
operations, which is referred to as the \emph{operation
  configuration}. The calls are dispateched to objects implementing
the interfaces generated from the operations declarations, such as the
one shown in \cref{lst:operations:op:interface}.

The dispatch logic is compiled into .\textls{NET} \textls{IL} bytecode
for fast execution. The dispatch logic may be changed at runtime
between executions of higher level algorithms.

Current, two operation configurations are exposed readily on the
public interface of the library.
\begin{itemize}
\item The \emph{Parallel} operation configuration uses the thread pool
  to utilize multiple \textls{CPU} cores.
\item The \emph{Sequential} operation configuration does not use
  multiple thread, therefore it is suitable for use with algorithms
  that handle multithreaded execution via other means.
\end{itemize}

Additional operations configurations may be developed with the public
\textls{API} of the library. Thus, the characteristics of the model
and the executing harware may be considered on the linear algebra
operation level in addition to the numerical algorithm level in
advanced stochastic analysis scenarios.

The flexible dispatch logic allows the identification of calculation
hotspots via profiling, such that a specific operation implementation
may be created and used to improve performance. If the specific
implementation degrades performance of some algorithms, it can be
switched off by replacing the operation configuration.

\subsection{Efficient vector-matrix products}
\label{sec:algorithms:vector-matrix}

\begin{algorithm}
  \KwIn{block vector $\vec{b} \in \RR^{n_0 + n_1 + \cdots + n_{k -
        1}}$,\\block matrix $A \in \RR^{(n_0 + n_1 + \cdots + n_{k -
        1}) \times (m_0 + m_1 + \cdots + m_{l - 1})}$}
  \KwOut{$\vec{c} = \vec{b} A \in \RR^{m_0 + m_1 + \cdots + m_{l -
        1}}$}
  \KwAllocate{$\vec{c}\in \RR^{m_0 + m_1 + \cdots + m_{l - 1}}$}\;
  \ParFor{$j \gets 0$ \KwTo $l - 1$}{
    $\vec{c}[j] \gets \vec{0}$
    \Operation*{VectorClear}
    \For{$i \gets 0$ \KwTo $k - 1$}{
      $\vec{c}[j] \gets \vec{c}[j] + \vec{b}[i] A[i, j]$
      \Operation*{VectorMatrixAccumulateMultiplyFromLeft}
    }
  }
  \caption{Parallel block vector-matrix product.}
  \label{alg:algorithms:matmul:block}
\end{algorithm}

\begin{algorithm}
  \KwIn{$\vec{b} \in \RR^n$, $A = \nu_0 A_0 + \nu_1 A_1 + \cdots +
    \nu_{k - 1} A_{k - 1}$, where $A_h \in \RR^{n \times m}$}
  \KwOut{$\vec{c} = \vec{b} A \in \RR^m$}
  $\vec{c} \gets \vec{0}$
  \Operation*{VectorClear}
  \For{$h \gets 0$ \KwTo $k - 1$}{
    $\vec{c} \gets \nu_h \cdot \vec{b} A_h$
    \Operation*{VectorMatrixAccumulateMultiplyFromLeft}
  }
  \KwRet{$\vec{c}$}\;
  \caption{Product of a vector with a linear combination matrix.}
  \label{alg:algorithms:matmul:lincomb}
\end{algorithm}

\begin{algorithm}
  \KwIn{$\vec{b} \in \RR^{n_0 n_1 \cdots n_{k - 1}}$, $A = \loc{A}{0}
    \krtimes \loc{A}{1} \krtimes \cdots \krtimes \loc{A}{k - 1}$,
    where $\loc{A}{h} \in \RR^{n_h \times m_h}$}
  \KwOut{$\vec{c} = \vec{b} A \in \RR^{m_0 m_1 \cdots m_{k - 1}}$}
  $n \gets n_0 n_1 \cdots n_{k - 1},\quad m \gets m_0 m_1 \cdots m_{k - 1}$\;
  $\textit{tempLength} \gets \max_{h = -1, 0, 1, \ldots, k - 1} \prod_{f =
    0}^h m_f \prod_{f = h + 1}^{k - 1} n_f$\;
  \KwAllocate{$\vec{x}, \vec{x}'$ with at least $\textit{tempLength}$
    elements}\;
  $\vec{x}[0{:}1{:}n] \gets \vec{b},
  i_{\text{left}} \gets 1,
  i_{\text{right}} \gets \prod_{h = 1}^{k - 1} n_h$
  \Operation*{VectorSet}
  \For{$h \gets 0$ \KwTo $k - 1$}{
    \If{$\loc{A}{h}$ is not an identity matrix}{
      $i_{\text{base}} \gets 0, j_{\text{base}} \gets 0$\;
      \For{$\textit{il} \gets 0$ \KwTo $i_{\text{left}} - 1$}{
        \label{ln:algorithms:matmul:shuffle:innerloop}
        \For{$\textit{ir} \gets 0$ \KwTo $i_{\text{right}} - 1$}{
          \Operation{VectorMatrixMultiplyFromLeft}
          $\vec{x}'[j_{\text{base}}{:}m_h{:}i_{\text{right}}] \gets
          \vec{x}[i_{\text{base}}{:}n_h{:}i_{\text{right}}] \, \loc{A}{h}$\;
          $i_{\text{base}} \gets i_{\text{base}} + n_h
          i_{\text{right}},
          j_{\text{base}} \gets j_{\text{base}} + m_h
          i_{\text{right}}$\;
        }
      }
      Swap the references to $\vec{x}$ and $\vec{x}'$\;
    }
    $i_{\text{left}} \gets i_{\text{left}} \cdot m_h$\;
    \lIf{$h \ne k - 1$}
    {$i_{\text{right}} \gets i_{\text{right}} / n_{h + 1}$}
  }
  \KwRet{$\vec{c} = \vec{x}[0{:}1{:}m]$}\;
  \caption{The \textsc{Shuffle} algorithm for vector-matrix
    multiplication.}
  \label{alg:algorithms:matmul:shuffle}
\end{algorithm}

Iterative linear equation and transient distribution solvers require
several vector-matrix products per iteration. Therefore, efficient
vector-matrix multiplication algorithms are required for the various
matrix storage methods (i.e.~dense, sparse and block Kronecker
matrices) to support configurable stochastic analysis.

In this section, we present the operations developed in our framework
for vector-matrix multiplication. In addition, an example of the
complex dispatch logic made possible by the operation context
mechanism is described.

Implemented matrix multiplication routines for the data structure
(see~\vref{fig:genstor:kronecker:datastructure}) with a base and a
target vector include
\begin{itemize}
\item Multiplication of vectors with dense and sparse matrices with
  and without paralellization.

  If parallel execution is desired, vectors are partitioned into
  chunks of length equal to a \emph{blocking factor}. Multiplications
  involving each chunk are executed on the thread pool provided by the
  .\textls{NET} Common Language Runtime.
\item If one of the vectors is a \texttt{VectorSplitWrapper} but the
  matrix is not a block matrix, the vector must be \emph{unwrapped}
  first and the dispatch should be repeated.
\item Multiplication with block matrices by delegation to the
  constituent blocks of the matrix%
  ~(\vref{alg:algorithms:matmul:block}). The input and output vectors
  are converted to block vectors before multiplication. If parallel
  execution is required, each block of the output vector can be
  computed in a different task, since it is independent from the
  others. If the operand and target vectors are not block vectors, a
  \texttt{VectorSplitWrapper} must be created first.
\item Multiplication by a linear combination of matrices is delegated
  to the constituent matrices (\vref{alg:algorithms:matmul:lincomb}).
\item Multiplications $\vec{b} \cdot \diag\{\vec{a}\}$ by diagonal
  matrices are executed as elementwise theproduct
  $\vec{b} \eltimes \vec{a}$. The special case of multiplication by an
  identity matrix is equivalent to a vector copy.
\item Multiplications by Kronecker products is performed by the
  \textsc{Shuffle} algorithm~%
  \citep{DBLP:journals/informs/BuchholzCDK00,benoit2001memory} as
  shown in \vref{alg:algorithms:matmul:shuffle}.

  The algorithm requires access to partial slices of a vector
  $\partialVec{x}{s}{t}{m}$. As a sliding window of partial vectors is
  used, the multiplication uses flexible partial vectors to avoid
  repeated object construction in the inner loop. If both vectors are
  \texttt{NativeVector} and the Kronecker product only contains
  identity matrices and \texttt{NativeSparseMatrix} instances, a
  specialized subroutine is used which performs pointer arithmetic
  directly without the use of partial vectors. This is an example of
  an optimization that was added after profiling the computation of
  vector-matrix products.

  \textsc{Shuffle} rewrites the Kronecker products as
  \begin{equation}
    \bigkrtimes_{h = 0}^{k - 1} \loc{A}{h} = \prod_{h = 0}^{k - 1}
    I_{\prod_{f = 0}^{h - 1} n_f \times \prod_{f = 0}^{h - 1} n_f} \krtimes
    \loc{A}{h} \krtimes I_{\prod_{f = h + 1}^{k - 1} m_f \times
      \prod_{f = h + 1}^{k - 1} m_f} \text,
  \end{equation}
  where $I_{a \times a}$ denotes an $a \times a$ identity matrix.
  Multiplications by terms of the form
  $I_{N \times N} \otimes \loc{A}{h} \otimes I_{M \times M}$ are
  carried out in the loop at line%
  ~\ref{ln:algorithms:matmul:shuffle:innerloop} of
  \cref{alg:algorithms:matmul:shuffle}.

  The temporary vectors $\vec{x}, \vec{x}'$ are large enough store the
  results of the successive matrix multiplications. They are cached
  for every worker thread to avoid repeated allocations.

  Other algorithms for vector-Kronecker product multiplication are the
  \textsc{Slice}~\citep{fernandes2005alternative} and \textsc{Split}~%
  \citep{DBLP:conf/springsim/CzeksterRFLW10} algorithms, which are
  more amenable to parallel execution than \textsc{Shuffle}. Their
  implementation is in the scope of our future work.
\end{itemize}
Multiplication of a matrix with a vector from the right is implemented
similarly.