\documentclass[a4paper,10pt,twoside,openright]{memoir}
\semiisopage
\checkandfixthelayout

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}

\usepackage{array,booktabs,mdwlist}
\renewcommand*{\arraystretch}{1.2}

\usepackage{amsmath,amssymb,relsize,stackengine}
\usepackage[ntheorem]{empheq}
\usepackage{upgreek}

\newcommand*{\PN}{\textnormal{\emph{PN}}}
\newcommand*{\LN}{\textnormal{\emph{LN}}}
\newcommand*{\HN}{\textnormal{\emph{HN}}}
\newcommand*{\EN}{\textnormal{\emph{EN}}}
\newcommand*{\PI}{\textnormal{\emph{PI}}}
\newcommand*{\xto}{}\let\xto\xrightarrow
\newcommand*{\vecarrow}{}\let\vecarrow\vec
\renewcommand*{\vec}[1]{\boldsymbol{\mathrm{#1}}}
\newcommand*{\EffC}[1]{\mathop{\textnormal{EC}\mathnormal{(#1)}}}
\DeclareMathOperator{\EME}{EME}
\DeclareMathOperator{\SME}{SME}
\DeclareMathOperator{\PriME}{\Pi ME}
\DeclareMathOperator{\HME}{HME}
\DeclareMathOperator{\MME}{MME}
\newcommand*{\MInv}[1]{\tau\vec{#1}}
\DeclareMathOperator{\StrC}{SC}
\DeclareMathOperator{\CasC}{CC}
\DeclareMathOperator{\SCC}{SCC}
\newcommand*{\CCS}{\textnormal{\emph{CCS}}}
\newcommand*{\ECS}{\textnormal{\emph{ECS}}}
\newcommand*{\TRS}{\textnormal{\emph{TRS}}}
\newcommand*{\PS}{\textnormal{\emph{PS}}}
\newcommand*{\macroS}{}\let\macroS\tilde
\newcommand*{\macroStates}{}\let\macroStates\widetilde
\newcommand*{\macroTRS}{\macroStates{\TRS}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\level}{level}
\DeclareMathOperator{\Env}{Env}
\DeclareMathOperator{\Above}{Above}

\usepackage{xparse}

\usepackage{microtype}

\usepackage{tikz}
\usetikzlibrary{arrows,calc,positioning,shapes,matrix}
\usepackage[compactlambda]{petriTikz}

\pgfdeclarelayer{bg}
\pgfdeclarelayer{connections}
\pgfsetlayers{bg,connections,main}

\tikzset{>=latex'}

\newsubfloat{figure}

\usepackage[
  hyperfootnotes=false,
  breaklinks=true,
  hypertexnames=true,
  plainpages=false,
  pdfpagelabels=true,
  hidelinks
]{hyperref}

\usepackage[amsthm,thmmarks,thref,hyperref]{ntheorem}
\newtheorem{thm}{Theorem}[chapter]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{dfn}[thm]{Definition}

\usepackage[
  style=authoryear-comp,
  maxnames=2,
  maxbibnames=99,
  uniquename=init,
  uniquelist=false,
  backref=false,
  doi=false,
  isbn=false,
  eprint=false,
  natbib=true,
  hyperref=true,
  autolang=hyphen,
  backend=biber
]{biblatex}
\bibliography{markov}
\AtEveryBibitem{% Clean up the bibtex rather than editing it
  \clearlist{address}
  \clearfield{date}
  \clearfield{issn}
  \clearlist{location}
  \clearfield{month}
  \clearfield{series}
  \ifentrytype{book}{}{% Remove publisher and editor except for books
    \ifentrytype{inbook}{}{
      \clearlist{publisher}
      \clearname{editor}
    }
  }
  \ifentrytype{article}{
    \clearfield{url}}{}
  \ifentrytype{inproceedings}{
    \clearfield{url}}{}
}
\DefineBibliographyStrings{english}{%
  bibliography = {References}
}

\begin{document}

\mainmatter

\chapter{Example Petri Net}

We will now consider the Generalized Stochastic Petri Net $\PN$ in
Figure~\ref{fig:example:pn}~(a). The net contains seven places, two
immediate transitions $t_1$ and $t_3$, which form an effective
conflict set, and six timed transitions.

\begin{figure}
  \hspace*{\fill}
  \parbox[c]{0.55\linewidth}{\centering \input{example_net}}
  \hfill
  \parbox[c]{0.4\linewidth}{\centering \input{example_net_hl}}
  \hspace*{\fill}
  \par\vspace{0.5\onelineskip}
  \hspace*{\fill}
  \parbox[t]{0.55\linewidth}{\subcaption{%
      Partition consisting of regions $\LN_1$ and $\LN_2$}}
  \hfill
  \parbox[t]{0.4\linewidth}{\subcaption{High level structure}}
  \hspace*{\fill}
  \caption{Example Generalized Stochastic Petri Net $\PN$ with its
    initial marking $M_0$}
  \label{fig:example:pn}
\end{figure}

\section{Decomposition into High Level and Low Level Nets}

\subsection{Partitioning Into Minimal Regions}

To facilitate Kronecker analysis we partition the net into
\emph{minimal regions} following
\citet{DBLP:journals/sigmetrics/BuchholzK98}.

A set of transitions $T_r \subset T$ defines a region if
$({}^{\bullet, \circ}T_r)^{\bullet, \circ} = T_r$, where
${}^{\bullet,\circ}T_r = {}^{\bullet}T_r \cup {}^{\circ}T_r$ is set of
places connected to transitions in $T_r$ by input or inhibitor edges,
and $P_r^{\bullet, \circ} = P_r^{\bullet} \cup P_r^{\circ}$ is the set
of transitions connected to places in $P_r$ by input or inhibitor
edges. In other words, the minimum regions partition the set of
transitions $T$ such that no place is shared by two transitions for
different regions as input on inhibitor.

We first partition $T$ into singleton sets $(\{p_i\})_{i = 1}^7$, then
iterate over the places. In the $i$th iteration, all the sets which
share $p_i$ as input (or inhibitor) are merged. We only need to merge
regions in iterations when $p_i$ has at least two adjacent input (or
inhibitor) edges.


\subsection{Merging Minimal Regions into Subnets}

The minimal region $\{t_1, t_2, t_3, t_4\}, \{t_5\}$ has immediate
output transitions $t_1$ and $t_2$ which must be turned into internal
transitions by region merging. Moreover, minimal regions $\{t_7\}$ and
$\{t_8\}$ contain no internal (timed) behaviour, therefore, they also
need to be merged. We merge them with each other in order to divide
$\PN$ into $J = 2$ low-level nets and $k = 3$ output transitions
between them.

Partitioning of the net is illustrated in
Table~\ref{tbl:example:partition}.

\begin{table}
  {\centering
    \begin{tabular}{@{}lll@{}}
      \toprule
      & Action & Regions \\
      \midrule
      & Initialize & $\{t_1\}, \{t_2\}, \{t_3\}, \{t_4\}, \{t_5\},
                     \{t_6\}, \{t_7\}, \{t_8\}$ \\[0.5em]
      $p_1$ & Merge $t_1, t_2, t_3, t_4$ & $\{t_1, t_2, t_3, t_4\}, \{t_5\},
                     \{t_6\}, \{t_7\}, \{t_8\}$ \\
      $p_2$ & & $\{t_1, t_2, t_3, t_4\}, \{t_5\},
                     \{t_6\}, \{t_7\}, \{t_8\}$ \\
      $p_3$ & & $\{t_1, t_2, t_3, t_4\}, \{t_5\},
                     \{t_6\}, \{t_7\}, \{t_8\}$ \\
      $p_4$ & $t_1, t_3$ ok & $\{t_1, t_2, t_3, t_4\}, \{t_5\},
                     \{t_6\}, \{t_7\}, \{t_8\}$ \\
      $p_5$ & $t_2, t_4$ ok & $\{t_1, t_2, t_3, t_4\}, \{t_5\},
                     \{t_6\}, \{t_7\}, \{t_8\}$ \\
      $p_6$ & & $\{t_1, t_2, t_3, t_4\}, \{t_5\},
                     \{t_6\}, \{t_7\}, \{t_8\}$ \\
      $p_7$ & & $\{t_1, t_2, t_3, t_4\}, \{t_5\},
                     \{t_6\}, \{t_7\}, \{t_8\}$ \\[0.5em]
      $t_1$ & Merge $t_1, t_5$ & $\{t_1, t_2, t_3, t_4, t_5\},
                     \{t_6\}, \{t_7\}, \{t_8\}$ \\
      $t_3$ & Merge $t_3, t_6$ & $\{t_1, t_2, t_3, t_4, t_5, t_6\},
                     \{t_7\}, \{t_8\}$ \\[0.5em]
      $t_7$ & Merge $t_7, t_8$ & $\LN_1 = \{t_1, t_2, t_3, t_4, t_5, t_6\},
                     \LN_2 = \{t_7, t_8\}$ \\
      \bottomrule
    \end{tabular}
    \par}
  \caption{Partitioning the set of transitions into regions}
  \label{tbl:example:partition}
\end{table}

\subsection{Partial Place Invariants and MSIPs}

We first compute the $P$-invariants of $\PN$:
\begin{itemize*}
\item $\{p_2, p_3, p_4, p_5\}$,
\item $\{p_1, p_2, p_3, p_6, p_7\}$.
\end{itemize*}
The $P$-invariants of $\LN_1$ with the output transitions removed:
\begin{itemize*}
\item $\PI_1 = \{p_1, p_2, p_3\}$,
\item $\{p_2, p_3, p_4, p_5\}$.
\end{itemize*}
The $P$-invariant of $\LN_2$ with the output transition removed:
\begin{itemize*}
\item $\PI_2 = \{p_6, p_7\}$.
\end{itemize*}

\emph{Partial} $P$-invariants are invariants of the low-level nets
which are not invariants of the full net. Therefore, both $\LN_1$ and
$\LN_2$ have only one partial invariants, $\PI_1$ and $\PI_2$,
respectively. These invariants correspond to MSIPs in the high level
net $\HN$ an the extended nets $\EN_1$ and $\EN_2$. The high-level net
is shown in Figure~\ref{fig:example:pn}~(b).

\section{High-level and Low-level TRS exploration}

Because both the high-level and low-level nets are $1$-bounded, we
will denote a marking by the set of places which have a token,
e.g.~$M_0 = \{p_1, p_5\}$ in the original net $\PN$ and $M_0 =
\{\PI_1\}$ in $\HN$.

The set of reachable high-level states (macro-markings) are
\begin{equation*}
  \TRS(\HN) = \{s^H_1 = \{\PI_1\}, s^H_2 = \{\PI_2\}\} \text.
\end{equation*}
When exploring the tangible state-spaces of the low-level nets, we
also note which high-level marking a given state corresponds to:
\begin{align*}
  \TRS(\LN_1) &= S^{(1)}_1 \cup S^{(1)}_2 \text, \\
  S^{(1)}_1 &= \{s^{(1)}_{1} = \{p_1, p_5\}, s^{(1)}_{2} = \{p_2\},
              s^{(1)}_{3} = \{p_3\}\} \text, \\
  S^{(2)}_2 &= \{s^{(1)}_{4} = \{p_4\}, s^{(1)}_{5} = \{p_5\}\} \text,
  \\
  \TRS(\LN_2) &= S^{(2)}_1 \cup S^{(2)}_2 \text, \\
  S^{(2)}_1 &= \{s^{(2)}_{1} = \emptyset\} \text, \\
  S^{(2)}_2 &= \{s^{(2)}_{2} = \{p_6\}, s^{(2)}_{3} = \{p_7\}\} \text.
\end{align*}

A na\"\i ve approximation of the state space of $\PN$ is $\TRS(\LN_1)
\times \TRS(\LN_2)$. However, this set of states contains many
markings unreachable from $M_0$.

We will analyse the behaviour of $\PN$ as a CTMC over the state space
$S' = S'_1 \cup S'_2 = S^{(1)}_1 \times S^{(2)}_1 \cup S^{(1)}_2
\times S^{(2)}_2\}$.
Observe that the the global state $(s^{(1)}_{5}, s^{(2)}_{2})$ is
unreachable, because in order have a token in $p_5$, transition $t_6$
must be fired, however, in order to have a token in $p_6$, transition
$t_5$ must be fired. The true state space of $\PN$ is
$S = S' \setminus \{(s^{(1)}_{5}, s^{(2)}_{2})\}$.

While the introduction of low-level state-space partitioning by MSIPs
reduced the size of $S'$ considerably, the resulting Markov chain is
still not irreducible; we have to work around unreachable marking. In
addition, the reduction of state-space size---which reduces storage
requirements of probability vectors---increased the complexity of the
rate matrix, as we will see in the next Section.

\section{Rate Matrix Generation}

We will find the rate matrix as a block matrix of $\lvert S^H \rvert
\times \lvert S^H \rvert$ block, plus a diagonal correction term:
\begin{gather}
  Q_O = \left( \begin{array}{@{}c|c@{}}
                 Q_O[1, 1] & Q_O[1, 2] \\
                 \hline Q_O[2, 1] & Q_O[2, 2]
       \end{array} \right) \text, \\
  Q_D = \diag(\vec{e}^T Q_O) \text, \\
  Q = Q_O + Q_D \text,
  \intertext{where the $\lvert S'_{x} \rvert \times \lvert S'_{y}
    \rvert$ matrix $Q_O[x, y]$ describes the rate of transitions from
    macro-marking $s^H_{x}$ to $s^H_{y}$ and is of the form:}
  Q_O[x, y] = \begin{cases}
    \displaystyle \bigoplus_{j = 1}^J Q^{(j)}_L[x, x] +
    \sum_{i = 1}^{k_{x, x}} \bigotimes_{j = 1}^{J} Q^{j}_k[x, x] & \text{if
      $x = y$,} \\
    \displaystyle
    \sum_{i = 1}^{k_{x, y}} \bigotimes_{j = 1}^{J} Q^{j}_k[x, y] & \text{if
      $x \ne y$.}
  \end{cases} \label{eq:example:QO}
\end{gather}

\emph{Is it true in the general case that $k_{x,x} = 0$? An output
  transition adds at least one token to a place in a \emph{foreign}
  region, therefore increases the value of at least one partial
  P-invariant of that region. Therefore, no output transition may
  leave the MSIPs unchanged.}

The $\lvert S^{(j)}_{x} \rvert \times \lvert S^{(j)}_{x} \rvert$
matrix $Q^{(j)}_L[x, x]$ describes the effects of inner transitions in
$\LN_j$ when the current macro-marking is $s^H_x$. The
$\lvert S^{(j)}_{x} \rvert \times \lvert S^{(j)}_{y} \rvert$ matrix
$Q^{(j)}_i[x, x]$ describes the effects in $\LN_j$ of the $i$th
transition (out of $k_{x, x}$) which takes $\HN$ from $s^H_x$ to
$s^H_y$.

The local transition matrices are
\begin{align}
  Q^{(1)}_L[1, 1] &= \begin{pmatrix}
    0 & 2 & 1 \\
    0 & 0 & 0 \\
    0 & 0 & 0
  \end{pmatrix} \text,
  & Q^{(2)}_L[1, 1] &= \begin{pmatrix}
    0
  \end{pmatrix} \text, \\
  Q^{(1)}_L[2, 2] &= \begin{pmatrix}
    0 & 0 \\
    0 & 0
  \end{pmatrix} \text, &
  Q^{(2)}_L[2, 2] &= \begin{pmatrix}
    0 & 1 \\
    0 & 0
  \end{pmatrix} \text.
\end{align}

There are no output transitions that leave the macro-marking
unchanged, therefore $k_{1, 1} = k_{2, 2} = 0$. The transitions $t_5$
and $t_6$ take $s^H_1$ to $s^H_2$, while transition $t_8$ takes
$s^H_2$ to $s^H_1$, therefore $k_{1, 2} = 2$ and $k_{2, 1} = 1$.

First we construct the matrices corresponding to output transitions
$t_5$. Note that we write $\lambda(t_5)$ ad $\lambda(t_6)$ in the
appropriate cells of $Q^{(1)}_1[1, 2]$ and $Q^{(1)}_2[1, 2]$ instead
of $1$ in order to avoid multiplication by $\lambda$ in
\eqref{eq:example:QO}.
\begin{align}
  Q^{(1)}_1[1, 2] &= \begin{pmatrix}
    0 & 0 \\
    0.1 & 0 \\
    0 & 0
  \end{pmatrix} \text, &
  Q^{(2)}_1[1, 2] &= \begin{pmatrix}
    1 & 0
  \end{pmatrix} \text, \\
  Q^{(1)}_2[1, 2] &= \begin{pmatrix}
    0 & 0 \\
    0 & 0 \\
    0 & 0.1
  \end{pmatrix} \text, &
  Q^{(2)}_2[1, 2] &= \begin{pmatrix}
    0 & 1
  \end{pmatrix} \text. \\
\end{align}

To construct the matrices corresponding to $t_8$, we must model the
immediate random behaviour of $\LN_1$ due to $t_1$ and $t_3$ in
$Q^{(1)}_1[2, 1]$:
\begin{align}
  Q^{(1)}_1[2, 1] &= \begin{pmatrix}
    0 & \frac{1}{3} & \frac{2}{3} \\
    1 & 0 & 0
  \end{pmatrix} \text, &
  Q^{(2)}_1[2, 1] &= \begin{pmatrix}
    0 \\
    0.5
  \end{pmatrix} \text.
\end{align}

The full $Q$ matrix is
\begin{equation}
  Q = \left( \begin{array}{@{}ccc|cccc@{}}
               * & 2 & 1 & 0 & 0 & 0 & 0 \\
               0 & * & 0 & 0.1 & 0 & 0 & 0 \\
               0 & 0 & * & 0 & 0 & 0 & 0.1 \\
               \hline
               0 & 0 & 0 & * & 1 & 0 & 0 \\
               0 & \frac{1}{6} & \frac{1}{3} & 0 & * & 0 & 0 \\
               0 & 0 & 0 & 0 & 0 & * & 1 \\
               0.5 & 0 & 0 & 0 & 0 & 0 & * \\
             \end{array} \right) \text.
\end{equation}
Observe that the second-to-last column contains no positive entries,
therefore, the corresponding $(s^{(1)}_5, s^{(2)}_2)$ state is indeed
unreachable and the CMTC is not irreducible.

\section{Kronecker Analysis with Decomposition}

\chapter{Kronecker Analysis}

\begin{figure}
  {\centering
    \input{flowchart.tex}
    \par}
  \caption{The exact decompositional algorithm}
  \label{fig:kronecker:flowchart}
\end{figure}

Rough outline for figuring out the exact decomposition algorithm
\citep{bao2008decompositional} in Figure~\ref{fig:kronecker:flowchart}:

\section{Construction of Initial Probability Vector}

Due to the reducibility of $Q$, the initial choice of $\vec{\uppi}$ will
determine the steady-state solution. Therefore, we need to choose
$\vec{\uppi}$ such that it is only nonzero at the states reachable from
$M_0$,
\begin{equation}
  \pi_i = \begin{cases}
    1 / \lvert \TRS(\PN) \rvert & \text{if $s_i \in \TRS(\PN)$,} \\
    0 & \text{if $s_i \notin \TRS(\PN)$.} \\
  \end{cases}
\end{equation}
This requires discovery of $\TRS(\PN)$.

\section{Solution of the Reducible Subproblem}

Due to the block structure of $Q^{(j)}$, the ``projection'' of $Q$ to
$\LN_j$ which is obtained by removing the Kronecker products from
\eqref{eq:example:QO}, it is never irreducible. State changes which
would allow $Q^{(j)}$ to move between different macro-markings would violate
the partial $P$-invariants of $\LN_j$ which are used to describe the
macro-markings.

Therefore, we have to disaggregate and aggregate the synchronized part
of $Q$ in the decompositional algorithm. This reduces the
decompositional algorithm of \citet{bao2008decompositional} to the
approximate decompositional algorithm of
\citet{DBLP:journals/sigmetrics/BuchholzK98}, plus a Gauss--Seidel
correction step.

\section{Disaggregation of the Probability Vector}

After solving the local equations with (dis)aggregated synchronized
transitions, we obtain local probability vectors $\pi^{(j)}$ for each
local net $\LN_j$. Due to the reducibility of $Q$, the
vector Kronecker product
\begin{equation}
  \vec{\uppi}' = \bigotimes_{j = 1}^J \vec{\uppi}^{(j)}
\end{equation}
cannot be used as input for the GS correction step, because it can
have support at states unreachable from $M_0$.

\begin{prop}
  Let $S = \{1, 2, \ldots, n\}$ and $S^{(j)} = \{1, 2, \ldots, n_j\}$
  for all $j = 1, 2, \ldots, J$ be state spaces, $\TRS \subseteq S$,
  $f^{(j)}: S \to S^{(j)}$ be surjective mappings and
  $\vec{\uprho}^{(j)}$ be positive probability vectors (that is,
  $\sum_{s^{(j)} \in S^{(j)}} \rho^{(j)}(s^{(j)}) = 1$ and
  $\rho^{(j)}(s^{(j)}) > 0$ for all
  $s^{(j)} \in S^{(j)}$). The system of equations
  \begin{empheq}[left=\empheqlbrace]{gather}
    \sum_{s \in S} \pi(s) = 1 \text, \qquad
    \pi(s) \ge 0 \text{ for all $s \in S$,}
    \label{eq:kronecker:disaggregate-cond1} \\
    \sum_{f^{(j)}(s) = s^{(j)}} \pi(s) = \rho^{(j)}(s^{(j)}) \text{
      for all $j = 1, 2, \ldots, j$ and $s^{(j)} \in S^{(j)}$,}
    \label{eq:kronecker:disaggregate-cond2} \\
    \pi(s) = 0 \text{ for all $s \notin \TRS$}
    \label{eq:kronecker:disaggregate-cond3}
  \end{empheq}
  has a solution of the form
  \begin{align}
    \pi(s) = a \prod_{j = 1}^{J} p^{(j)}(f^{(j)}(s))
    \text, \label{eq:kronecker:disaggregated-pi}
  \end{align}
  where $\vec{p}^{(j)}$ are nonnegative vectors and a is a positive
  constant. Solutions of this form have maximum entropy
  \begin{align}
    H(\vec{\uppi}) = \sum_{s \in \TRS} - \pi(s) \log \pi(s) \text.
  \end{align}

  Conversely, any probability vector $\vec{\uppi}$ of the form
  \eqref{eq:kronecker:disaggregated-pi} that satisfies
  \eqref{eq:kronecker:disaggregate-cond2} and
  \eqref{eq:kronecker:disaggregate-cond3} also satisfies
  \eqref{eq:kronecker:disaggregate-cond1} and has maximum entropy the
  entropy $H(\vec{\uppi})$.
\end{prop}

\begin{proof}
  Consider the Lagrange function associated with the optimization
  problem $\max_{\vec{\uppi}} H(\vec{\uppi})$ subject to
  \eqref{eq:kronecker:disaggregate-cond1}--%
  \eqref{eq:kronecker:disaggregate-cond3} by introducing Lagrange
  multiplicators $\alpha, \vec{\upbeta}, \vec{\upgamma}^{(j)}$ and
  $\vec{\updelta}$,
  \begin{equation}
    \begin{aligned}
      & \Lambda(\vec{\pi}, \alpha, \vec{\upbeta},
      \vec{\upgamma}^{(1)}, \vec{\upgamma}^{(2)}, \ldots,
      \vec{\upgamma}^{(J)},
      \vec{\updelta}) =\\
      &\qquad - \sum_{s \in \TRS} - \pi(s) \log \pi(s) + \alpha
      \Biggl( \sum_{s \in S} \pi(s) - 1 \Biggr) + \sum_{s \in S}
      \beta(s) \pi(s) \\
      &\qquad + \sum_{j = 1}^{J} \, \sum_{s' \in S^{(j)}}
      \gamma^{(j)}(s') \Biggl( \sum_{s \in F^{(j)}(s')} \pi(s) -
      \rho^{(j)}(s') \Biggr) + \sum_{s \notin \TRS} \delta(s) \pi(s)
      \text,
    \end{aligned}
  \end{equation}
  where we have written $F^{(j)}(s') = \{ s \in S \mid f^{(j)}(s) = s'
  \}$ for brevity.

  Suppose that the optimal solution has $\pi(s) > 0$ for all
  $s \in \TRS$. Hence, the gradient $\nabla \Lambda$ exists and the
  optimal solution satisfies the Karush--Kuhn--Tucker conditions
  \begin{gather}
    \frac{\partial \Lambda}{\partial \pi(s)}
    = \log \pi(s) + 1 + \alpha + \beta(s) + \sum_{j = 1}^{J}
    \gamma^{(j)}(f^{(j)}(s)) = 0 \text{ for all $s \in \TRS$,} \\
    \beta(s) \ge 0 \text{ and } \beta(s) \pi(s) = 0 \text{ for all $s
      \in S$.}
  \end{gather}
  We can conclude from $\pi(\TRS) > 0$ that $\beta(s) = 0$ for all
  $s \in \TRS$ and
  \begin{equation}
    \pi(s) = \exp (-1 - \alpha) \prod_{j = 1}^J \exp
    [ -\gamma^{(j)}(f^{(j)}(s)) ]
    = a \prod_{j = 1}^J p^{(j)}(f^{(j)}(s)) \text.
  \end{equation}
\end{proof}

\section{Correction Vector Calculation}

(Block) Gauss--Seidel, as presented in the book, only works on sums of
Kronecker products, not block matrices where each block is a sum of
Kronecker products. Therefore, another method is needed to calculate
the correction vector $\vec{y}$.

Could (Block) Jacobi or (Block) power iteration \citep[Section
3.2]{dayar2012analyzing} be used here? Another possibility is a
``hybrid'' Jacobi--Gauss--Seidel approach, where we use Gauss--Seidel
backward substitution is used for the diagonal block, but the
off-diagonal blocks are zero in the matrix we need to ``invert'' in
matrix splitting.

\citet[Equation~(16)]{DBLP:journals/questa/Buchholz94} gives a
modified form of block Gauss--Seidel iteration for block structured
matrices. Does it respect the unreachable global states?

\section{Exact Hierarchical Decomposition}

\subsection{Basic Algorithm}

Let $\LN_1, \LN_2, \ldots, \LN_J$ be a decomposition of $\PN$ into
subnets, i.e.~$P = P_1 \coprod P_2 \coprod \cdots \coprod P_j$ a
disjoint partition of the places $P$ in $\PN$ into subsets $P_j$,
which are the places of $\LN_j$, respectively.

Let $\TRS_j$ denote the tangible reachability set of $\LN_j$. The set
$\TRS$ of tangible states of $\PN$ is a subset of the \emph{potential}
reachability set $\PS$,
\begin{equation}
  \TRS \subseteq \PS = \TRS_1 \times \TRS_2 \times \cdots \times
  \TRS_J\text.
\end{equation}

Let us define the equivalence relation $\sim_j$ over $\TRS_j$ as
\begin{equation}\label{eq:kronecker:state-equivalence}
  s^{(j)} \sim_j s'^{(j)} \Longleftrightarrow \{ s \in \TRS : s[j] =
  s^{(j)} \} = \{ s' \in \TRS : s'[j] = s'^{(j)} \} \text,
\end{equation}
where $s[j] \in \TRS_j$ denotes the local state of $\LN_j$ in the
global state $s \in \TRS$, i.e.~the $j$th component of $s$. The set of
macro-states $\macroTRS_j$ is the set of equivalence classes $\TRS_j /
{\sim_j}$.

The set of reachable macro-state combinations
$\macroTRS \subset \macroStates{\PS} = \macroTRS_1 \times \macroTRS_2
\times \cdots \times \macroTRS_j$
is used as a set os block indices to the block-Kronecker infinitesimal
generatior matrix:
\begin{gather}
  Q = Q_O - Q_D \text, \\
  Q_O[\macroS{s}, \macroS{s}'] = \begin{cases} \displaystyle
    \bigoplus_{j = 1}^J Q_L^{(j)}[\macroS{s}, \macroS{s}] + \sum_{i =
      1}^{k_{\macroS{s}, \macroS{s}}} \bigotimes_{j = 1}^J
    Q_{i}^{j}[\macroS{s},
    \macroS{s}] & \text{if $\macroS{s} = \macroS{s}'$,} \\
    \displaystyle \sum_{i = 1}^{k_{\macroS{s}, \macroS{s}'}}
    \bigotimes_{j = 1}^J Q_{i}^{j}[\macroS{s}, \macroS{s}'] & \text{if
      $\macroS{s} = \macroS{s}'$,}
  \end{cases} \\
  Q_D = \diag (Q_O \vec{e}^T) \text,
\end{gather}
where $Q_L^{(j)}[\macroS{s}, \macroS{s}]$ corresponds to the local
transitions in $\LN_j$ and the macro-state $\macroS{s}$, while
$Q_{i}^{j}[\macroS{s}, \macroS{s}']$ corresponds to the effects of
$i$th synchronized transition that brings the system from global
macro-state $\macroS{s}$ to $\macroS{s}'$ in $\LN_j$.

In the original algorithm of \citet{DBLP:journals/tse/Buchholz99},
$\TRS$ is represented with a bit vector of length $\lvert \PS
\rvert$.
After the generation of macro-states $\macroTRS_1$, only a bit vector
of length
$\lvert \macroTRS_1 \times \TRS_2 \times \TRS_3 \times \cdots \times
\TRS_J \rvert$
is needed, because states in the macro-states of $\LN_1$ have
identical external behaviour. Likewise, the first $j - 1$ subnets may
be represented by their macro-states when we calculate $\macroTRS_j$.

\subsection{Reducing Storage Requirements by Preprocessing}

TODO Section~5 in \citet{DBLP:journals/tse/Buchholz99}.

\subsection{Efficient Macro-state Generation with MDDs}

Consider the quasi-reduced ordered MDD that describes $\TRS$ and
suppose that nodes on the $j$th level correspond to the subnet
$\LN_j$. Let
\begin{equation}
  \Env_j(s^{(j)}) = \bigcup_{\level(a) = j} \Above(a) \times a[s^{(j)}]
\end{equation}
be the \emph{environment} of the local state $s^{(j)} \in S^{(j)}$,
where $\Above(a)$ is the MDD of the upper $J - j$ levels of $r$
leading to $a$.

Local macro states consist of local states with the same
(quasi-reduced) environment, that is,
\begin{gather}
  s^{(j)}_1 \sim_j s^{(j)}_2 \Longleftrightarrow \Env_j(s^{(j)}_1) =
  \Env_j(s^{(j)}) \text, \\
  \macroS{S}^{(j)} = S^{(j)} / {\sim}_j \text.
\end{gather}

We can obtain the MDD representation of $\macroTRS$ by replacing the
local states with the corresponding local macro-states in $r$.

\subsection{Block iteration}

The block iteration technique \citep{DBLP:journals/tse/Buchholz99}
attempts to find the steady-state solution by iterating
\begin{gather}
  \vec{p}_k[\macroS{x}] Q[\macroS{x}, \macroS{x}] =
  \vec{b}_k[\macroS{x}] \text, \label{eq:kronecker:block:solve} \\
  \vec{b}_k = \sum_{\macroS{y} < \macroS{x}} \vec{p}_k[\macroS{y}]
  Q[\macroS{y}, \macroS{x}] + \sum_{\macroS{y} > \macroS{x}}
  \vec{p}_{k - 1}[\macroS{y}] Q[\macroS{y}, \macroS{x}] \text,
\end{gather}
which corresponds to \emph{grouped GS iteration} from 

The main complication comes from the fact that, even in the case of an
irreducible $Q[\macroS{x}, \macroS{x}]$,
\eqref{eq:kronecker:block:solve} may have infinitely many solutions.

\begin{dfn}[\citet{young1971iterative}, Chapter~2, Definition~6.1.]
  The $n \times n$ matrix $A$ has \emph{Property A} if there exists a
  permutation matrix $P$ such that
  \begin{align}
    P^{-1} A P = \begin{pmatrix}
      D_1 & H \\ K & D_2
    \end{pmatrix} \text,
  \end{align}
  where $D_1$ and $D_2$ are square diagonal matrices.
\end{dfn}

\begin{dfn}[\citet{young1971iterative}, Chapter~5, Definition~3.1.]
  Given a matrix $A = (a_{ij})$, the integers $i$ and $j$ are
  associated with respect to $A$ if $a_{ij} \ne 0$ or $a_{ji} \ne 0$.
\end{dfn}

\begin{dfn}[\citet{young1971iterative}, Chapter~5, Definition~3.2.]
  The $n \times n$ matrix $A$ is \emph{consistently ordered} if there
  exists a partition
  $S_1 \coprod S_2 \coprod \cdots \coprod S_t = \{1, 2, \ldots, n\}$
  such that if $i$ and $j$ are associated with respect to $A$ and
  $i \in S_k$, then $j \in S_{k + 1}$ if $j > i$ and $j \in S_{k - 1}$
  if $j < i$.
\end{dfn}

\begin{thm}[\citet{young1971iterative}, Chapter~5]
  If $A$ is consistently ordered, then $A$ has Property A. Moreover,
  if $A$ has Property A, there exists a permutation matrix $P$ such
  that $P^{-1} A P$ is consistently ordered.
\end{thm}

\section{TODO}

\begin{itemize}
\item Add citations to \emph{everything} including the activity
  diagram
\item TODO articles and workflow
\item Navigable activity diagram
\item More examples\ldots
\end{itemize}

\begin{gather}
  Q[\macroS{x}, \macroS{y}] = \sum_{t \in T(\macroS{x}, \macroS{y})}
  w(t) \bigotimes_{j = 1}^J Q_{t}^{(j)}[\macroS{x}^{(j)},
  \macroS{y}^{(j)}]
  - \delta(\macroS{x}, \macroS{y}) \sum_{\macroS{z} \in \macroS{S}} \sum_{t \in T(\macroS{x}, \macroS{z})}
  w(t) \bigotimes_{j = 1}^J D_{t}^{(j)}[\macroS{x}^{(j)},
  \macroS{z}^{(j)}] \text, \\
  D_{t}^{(j)}[\macroS{x}^{(j)}, \macroS{z}^{(j)}] = \diag
  Q_{t}^{(j)}[\macroS{x}^{(j)}, \macroS{z}^{(j)}] \vec{e}^T \text.
\end{gather}

\backmatter

\printbibliography

\end{document}